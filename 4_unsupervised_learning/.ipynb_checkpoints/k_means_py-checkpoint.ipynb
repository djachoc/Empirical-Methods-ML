{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# *K*-Means Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "$K$-means clustering is a simple and elegant approach for partitioning a data set into $K$ distinct, non-overlapping clusters. To perform $K$-means clustering, we must first specify the desired number of clusters $K$; then the $K$-means algorithm will assign each observation to exactly one of the $K$ clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_stata('http://fmwww.bc.edu/ec-p/data/wooldridge/openness.dta')\n",
    "\n",
    "##  Obs:   114\n",
    "\n",
    "## 1. open                     imports as % GDP, '73-\n",
    "## 2. inf                      avg. annual inflation, '73-\n",
    "## 3. pcinc                    1980 per capita inc., U.S. \n",
    "## 4. land                     land area, square miles\n",
    "## 5. oil                      =1 if major oil producer\n",
    "## 6. good                     =1 if 'good' data\n",
    "## 7. lpcinc                   log(pcinc)\n",
    "## 8. lland                    log(land)\n",
    "## 9. lopen                    log(open)\n",
    "## 10. linf                    log(inf)\n",
    "## 11. opendec                 open/100\n",
    "## 12. linfdec                 log(inf/100)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The $K$-means clustering procedure results from a simple and intuitive mathematical problem.We begin by defining some notation. Let $C_1,\\ldots , C_K$ denote sets containing the indices of the observations in each cluster. These sets satisfy two properties:\n",
    "\n",
    "1. $C_{1} \\cup C_{2} \\cup \\ldots \\cup C_{K}=\\{1, \\ldots, n\\}$. In other words, each observation belongs to at least one of the $K$ clusters.\n",
    "\n",
    "2. $C_{k} \\cap C_{k^{\\prime}}=\\{\\emptyset\\}$ for all $k \\neq k^{\\prime}$. In other words, the clusters are non-overlapping: no observation belongs to more than one cluster.\n",
    "\n",
    "For instance, if the $i$th observation is in the $k$ th cluster, then $i \\in C_{k}$. The idea behind $K$ -means clustering is that a good clustering is one for which the within-cluster variation is as small as possible. The within-cluster variation for cluster $C_{k}$ is a measure $W\\left(C_{k}\\right)$ of the amount by which the observations within a cluster differ from each other. Hence we want to solve the problem\n",
    "\n",
    "$$\n",
    "\\underset{C_{1}, \\ldots, C_{K}}{\\operatorname{minimize}}\\left\\{\\sum_{k=1}^{K} W\\left(C_{k}\\right)\\right\\}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In words, this formula says that we want to partition the observations into $K$ clusters such that the total within-cluster variation, summed over all $K$ clusters, is as small as possible. Solving this last equation seems like a reasonable idea, but in order to make it actionable we need to define the within-cluster variation. There are many possible ways to define this concept, but by far the most common choice involves squared Euclidean distance. That is, we define\n",
    "\n",
    "$$\n",
    "W\\left(C_{k}\\right)=\\frac{1}{\\left|C_{k}\\right|} \\sum_{i, i^{\\prime} \\in C_{k}} \\sum_{j=1}^{p}\\left(x_{i j}-x_{i^{\\prime} j}\\right)^{2},\n",
    "$$\n",
    "\n",
    "where $p=\\text{dim}(x_{i j})$, and $\\left|C_{k}\\right|$ denotes the number of observations in the $k$ th cluster. In other words, the within-cluster variation for the $k$th cluster is the sum of all of\n",
    "the pairwise squared [Euclidean distances](https://en.wikipedia.org/wiki/Euclidean_distance) between the observations in the $k$th cluster, divided by the total number of observations in the $k$th cluster. Therefore\n",
    "\n",
    "$$\n",
    "\\underset{C_{1}, \\ldots, C_{K}}{\\operatorname{minimize}}\\left\\{\\sum_{k=1}^{K} \\frac{1}{\\left|C_{k}\\right|} \\sum_{i, i^{\\prime} \\in C_{k}} \\sum_{j=1}^{p}\\left(x_{i j}-x_{i^{\\prime} j}\\right)^{2}\\right\\}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "***\n",
    "**Algorithm**: <ins>_K_ Means Clustering</ins>\n",
    "\n",
    "1. Randomly assign a number, from 1 to K, to each of the observations. These serve as initial cluster assignments for the observations.\n",
    "\n",
    "2. Iterate until the cluster assignments stop changing:\n",
    "     1. For each of the $K$ clusters, compute the cluster centroid. The $k$th cluster centroid is the vector of the $p$ feature means for the observations in the $k$th cluster.\n",
    "     2. Assign each observation to the cluster whose centroid is closest (where closest is defined using Euclidean distance).\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "‚úçüèº This algorithm is guaranteed to **decrease** the value of the objective function at each step.  Since the resulting classification will depend on the initial (random) cluster assignment in Step 1, the algorithm is said to find a _local_ rather than a _global_ optimum. Therefore it is important to run the algorithm multiple times from different random initial configurations, e.g., between 25 to 50 times is recommended. Then one selects the _best_ solution, i.e., that for which the objective function is the smallest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "üíª Notice that the $K$-means algorithm described here *only* works for __continuously distributed__ data. It will _not_ work for categorical features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "üíª It is recommended to _scale_ the features prior to applying the algorithm. Different scales among features will severely affect the classification algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import patsy\n",
    "X = patsy.dmatrix('-1 + lopen + linf', data=df, return_type='dataframe')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<ins>**Distance Measures**</ins>: Euclidean Distance\n",
    "\n",
    "This is the default distance measure in most ML algorithms and the one used in the explanation above. If chosen, then observations with high values of the features will be clustered together, and observations with low values of the features will also be clustered together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ö†Ô∏è Unfortunately, `sklearn` <ins>only</ins> provides this distance measure. See this [post](https://stackoverflow.com/questions/5529625/is-it-possible-to-specify-your-own-distance-function-using-scikit-learn-k-means) to learn about alternative packages with different distance measures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "## Perform k-means clustering\n",
    "kmeans = KMeans(n_clusters=3, init='k-means++', max_iter=300, n_init=20, random_state=42)\n",
    "pred_y = kmeans.fit_predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.scatter(X.iloc[:,0], X.iloc[:,1],c=kmeans.labels_.astype(float)) #Plotting the data colored by cluster\n",
    "plt.grid(alpha=0.4,linestyle='--') #Adding a grid\n",
    "plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s=300, c='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Choosing *K*: The Elbow Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The idea is that we want a small within-cluster variance, but that the within-cluster variance tends to decrease toward $0$ as we increase $K$ (the within-cluster variance is exactly $0$ when $K$ is equal to the number of data points in the dataset, because then each data point is its own cluster, and there is no error between it and the center of its cluster). So our goal is to choose a small value of $K$ that still has a low within-cluster variance, and the [elbow](https://en.wikipedia.org/wiki/Elbow_method_(clustering)) usually represents where we start to have diminishing returns by increasing $K$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wcss = []\n",
    "for i in range(1, 11):\n",
    "    kmeans = KMeans(n_clusters=i, init='k-means++', max_iter=300, n_init=10, random_state=0)\n",
    "    kmeans.fit(X)\n",
    "    wcss.append(kmeans.inertia_)\n",
    "    \n",
    "plt.plot(range(1, 11), wcss)\n",
    "plt.grid(alpha=0.4,linestyle='--') #Adding a grid\n",
    "plt.title('Elbow Method')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('WCSS')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing _K_: Silhouette Score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Install the [yellowbrick](https://www.scikit-yb.org/en/latest/index.html) package__:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`xxxxx@ip-172-31-73-5:~$ sudo -i`\n",
    "\n",
    "`[sudo] password for xxxxx: `\n",
    "\n",
    "`(base) root@ip-172-31-73-5:~# conda install -c districtdatalabs yellowbrick`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Silhouette score is used to evaluate the quality of clusters created using clustering algorithms such as _K_-Means in terms of how well samples are clustered with other samples that are similar to each other. The Silhouette score is calculated for each sample of different clusters. To calculate the Silhouette score for each observation/data point, the following distances need to be found out for each observations belonging to all the clusters:\n",
    "\n",
    "1. Mean distance between the observation and all other data points in the _same_ cluster. This distance can also be called a __mean intra-cluster distance__. The mean distance is denoted by $a$.\n",
    "2. Mean distance between the observation and all other data points of the _next nearest_ cluster. This distance can also be called a __mean nearest-cluster distance__. The mean distance is denoted by $b$.\n",
    "\n",
    "Silhouette score, $S$, for each sample is calculated using the following formula:\n",
    "\n",
    "$$S = \\frac{(b - a)}{\\max(a, b)}$$\n",
    "\n",
    "The value of the Silhouette score varies from -1 to 1.\n",
    "\n",
    "üëâüèº If the score is 1, the cluster is dense and well-separated than other clusters.\n",
    "\n",
    "üëâüèº A value near 0 represents overlapping clusters with samples very close to the decision boundary of the neighboring clusters.\n",
    "\n",
    "üëâüèº A negative score [-1, 0] indicates that the samples might have got assigned to the wrong clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "## Perform k-means clustering\n",
    "kmeans = KMeans(n_clusters=3, init='k-means++', max_iter=300, n_init=20, random_state=42)\n",
    "pred_y = kmeans.fit_predict(X)\n",
    "\n",
    "score = silhouette_score(X, kmeans.labels_, metric='euclidean')\n",
    "print('Silhouette Score: %.3f' % score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from yellowbrick.cluster import SilhouetteVisualizer\n",
    "\n",
    "fig, ax = plt.subplots(2, 2, figsize=(15,8))\n",
    "for i in [2, 3, 4, 5]:\n",
    "    '''\n",
    "    Create KMeans instance for different number of clusters\n",
    "    '''\n",
    "    kmeans = KMeans(n_clusters=i, init='k-means++', n_init=20, max_iter=100, random_state=42)\n",
    "    q, mod = divmod(i, 2)\n",
    "    '''\n",
    "    Create SilhouetteVisualizer instance with KMeans instance\n",
    "    Fit the visualizer\n",
    "    '''\n",
    "    visualizer = SilhouetteVisualizer(kmeans, colors='yellowbrick', ax=ax[q-1][mod])\n",
    "    visualizer.fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the Silhouette analysis done on the above plots to select an optimal value for `n_clusters`.\n",
    "\n",
    "The value of `n_clusters` as 5 looks to be suboptimal for the given data due to the following reasons:\n",
    "\n",
    "üõë Wide fluctuations in the size of the silhouette plots.\n",
    "\n",
    "The value of 3 and 4 for `n_clusters` looks to be the optimal one. The silhouette score for each cluster is above average silhouette scores. The thickness of the silhouette plot representing each cluster also is a deciding point. For the plot with `n_cluster=3` (top right), the thickness is more uniform than the plot with `n_cluster` as 2 (top left) with one cluster thickness much more than the other. Thus, one can select the optimal number of clusters as 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using _K_-Means Clustering in Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine you are interested in estimating the model\n",
    "\n",
    "$$\n",
    "pcinc = \\beta_0+\\beta_1 oil+\\beta_2 good + \\beta_3 linf + \\beta_4 lland + \\beta_5 lopen + u\\text{,}\n",
    "$$\n",
    "\n",
    "but the researcher is concerned that countries belong to $K$ clusters and that unobserved cluster-specific characteristics might be correlated with the features $oil$, $good$, $linf$, $lland$, or $lopen$, then a more suitable model would be \n",
    "\n",
    "$$\n",
    "pcinc = \\beta_0+\\beta_1 oil+\\beta_2 good + \\beta_3 linf + \\beta_4 lland + \\beta_5 lopen + \\alpha_2 d_2 +\\alpha_3 d_3 + \\cdots +\\alpha_K d_K + e\\text{,}\n",
    "$$\n",
    "\n",
    "where the dummy variables ${d_j:j=1,\\ldots,K}$ equal one if a country belongs to cluster $j$ and zero otherwise. If we do _not_ know the group a particular country belongs to, we could apply _K_-means clustering, and then include group membership as a control variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Perform k-means clustering\n",
    "kmeans = KMeans(n_clusters=3, init='k-means++', max_iter=300, n_init=20, random_state=42)\n",
    "pred_y = kmeans.fit_predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating the dummy variables based on group membership\n",
    "df['group'] = pd.Series(pred_y, index=df.index)\n",
    "## Checking the numbers in each cluster to select base category\n",
    "df['group'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating and inspecting the extended data set\n",
    "datos = pd.get_dummies(df, columns = ['group'])\n",
    "datos.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üõë You need to install the `stargazer` module using `pip install stargazer` as an admin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.formula.api as sm\n",
    "model1 = sm.ols(formula=\"lpcinc ~ oil + good + linf + lland + lopen\", data=datos).fit(cov_type='HC1')\n",
    "model2 = sm.ols(formula=\"lpcinc ~ oil + good + linf + lland + lopen + group_0 + group_1\", data=datos).fit(cov_type='HC1')\n",
    "\n",
    "from stargazer.stargazer import Stargazer\n",
    "from IPython.core.display import HTML\n",
    "HTML(Stargazer([model1,model2]).render_html())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, if we also believe that group membership affects the variance of the unobserved error, then we should calculate standard errors accordingly, i.e., unobservables affecting the per-capita income in each countries _within_ a cluster are coorelated, but not _between_ countries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model3 = sm.ols(formula=\"lpcinc ~ oil + good + linf + lland + lopen\", data=datos).fit(cov_type='cluster', cov_kwds={'groups': df.group})\n",
    "model4 = sm.ols(formula=\"lpcinc ~ oil + good + linf + lland + lopen + group_0 + group_1\", data=datos).fit(cov_type='cluster', cov_kwds={'groups': df.group})\n",
    "\n",
    "from stargazer.stargazer import Stargazer\n",
    "from IPython.core.display import HTML\n",
    "HTML(Stargazer([model1,model2,model3,model4]).render_html())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
