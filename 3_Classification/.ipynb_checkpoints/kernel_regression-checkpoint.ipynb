{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "personal-footage",
   "metadata": {},
   "source": [
    "<h1>Kernel Smoothing</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "voluntary-bhutan",
   "metadata": {},
   "source": [
    "```MROZ.DES\n",
    "\n",
    "inlf      hours     kidslt6   kidsge6   age       educ      wage      repwage  \n",
    "hushrs    husage    huseduc   huswage   faminc    mtr       motheduc  \n",
    "fatheduc  unem      city      exper     nwifeinc  lwage     expersq   \n",
    "\n",
    "  Obs:   753\n",
    "\n",
    "  1. inlf                     =1 if in labor force, 1975\n",
    "  2. hours                    hours worked, 1975\n",
    "  3. kidslt6                  # kids < 6 years\n",
    "  4. kidsge6                  # kids 6-18\n",
    "  5. age                      woman's age in yrs\n",
    "  6. educ                     years of schooling\n",
    "  7. wage                     estimated wage from earns., hours\n",
    "  8. repwage                  reported wage at interview in 1976\n",
    "  9. hushrs                   hours worked by husband, 1975\n",
    " 10. husage                   husband's age\n",
    " 11. huseduc                  husband's years of schooling\n",
    " 12. huswage                  husband's hourly wage, 1975\n",
    " 13. faminc                   family income, 1975\n",
    " 14. mtr                      fed. marginal tax rate facing woman\n",
    " 15. motheduc                 mother's years of schooling\n",
    " 16. fatheduc                 father's years of schooling\n",
    " 17. unem                     unem. rate in county of resid.\n",
    " 18. city                     =1 if live in SMSA\n",
    " 19. exper                    actual labor mkt exper\n",
    " 20. nwifeinc                 (faminc - wage*hours)/1000\n",
    " 21. lwage                    log(wage)\n",
    " 22. expersq                  exper^2\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "radio-submission",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import wooldridge as woo\n",
    "mroz = woo.dataWoo('mroz').copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "straight-creek",
   "metadata": {},
   "source": [
    "<h2>Density Estimation: Univariate</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lesser-press",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "fig, axs = plt.subplots(1,3,figsize=(26,5))\n",
    "axs[0].hist(mroz.lwage,color='gold',edgecolor='black',linewidth=1.2,bins=20,density=True)\n",
    "axs[0].set_title('log(wages)')\n",
    "axs[1].bar([\"1\",\"0\"],mroz.city.value_counts()/mroz.shape[0],color='azure',edgecolor='black',linewidth=1.2)\n",
    "axs[1].set_title('city')\n",
    "axs[2].bar([\"0\",\"1\",\"2\",\"3\"],mroz.kidslt6.value_counts()/mroz.shape[0],color='ivory',edgecolor='black',linewidth=1.2)\n",
    "axs[2].set_title('kids < 6')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "demonstrated-gnome",
   "metadata": {},
   "source": [
    "üîó You can find all available `matplotlib` styles [here](https://matplotlib.org/stable/gallery/style_sheets/style_sheets_reference.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "visible-acoustic",
   "metadata": {},
   "source": [
    "Recall that for <ins>any</ins> random variable, $X$, the __cumulative distribution function__ (CDF) is defined as\n",
    "$$\n",
    "F(x)=\\Pr\\{X\\le x\\},\n",
    "$$\n",
    "and based on a random sample $\\{X_i;i=1,\\ldots,n\\}$ the __empirical distribution function__ (EDF) is defined as\n",
    "$$\n",
    "\\widehat{F}(x)=\\frac{1}{n} \\sum_{i=1}^{n} \\mathbb{I}\\left(X_{i} \\leq x\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rational-pharmacy",
   "metadata": {},
   "source": [
    "<h3>Continuous</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wound-fellow",
   "metadata": {},
   "source": [
    "In this case, the __probability density function__ (pdf) is defined as\n",
    "$$\n",
    "f(x)=\\frac{d}{d x} F(x)=\\lim _{h \\rightarrow 0} \\frac{F(x+h)-F(x-h)}{2 h}\\text{.}\n",
    "$$\n",
    "Therefore, a natural estimator will be\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\widehat{f}(x) &=\\frac{\\widehat{F}(x+h)-\\widehat{F}(x-h)}{2 h} \\\\\n",
    "&=\\frac{1}{2 h}\\left\\{\\frac{1}{n} \\sum_{i=1}^{n} \\mathbb{I}\\left(X_{i} \\leq x+h\\right)-\\frac{1}{n} \\sum_{i=1}^{n} \\mathbb{I}\\left(X_{i} \\leq x-h\\right)\\right\\} \\\\\n",
    "&=\\frac{1}{n h} \\sum_{i=1}^{n} \\frac{1}{2} \\mathbb{I}\\left(x-h \\leq X_{i} \\leq x+h\\right) \\\\\n",
    "&=\\frac{1}{n h} \\sum_{i=1}^{n} \\frac{1}{2} \\mathbb{I}\\left(-1 \\leq \\frac{X_{i}-x}{h} \\leq 1\\right)=\\frac{1}{n h} \\sum_{i=1}^{n} \\frac{1}{2} \\mathbb{I}\\left(\\left|\\frac{X_{i}-x}{h}\\right| \\leq 1\\right) \\\\\n",
    "&=\\frac{1}{n h} \\sum_{i=1}^{n} k\\left(\\frac{X_{i}-x}{h}\\right), \\text { where } k(u)=\\frac{1}{2} \\mathbb{I}(|u| \\leq 1)\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "subjective-thunder",
   "metadata": {},
   "source": [
    "üìù The $k(\\cdot)$ function is known as the __kernel__ function and the hyperparameter $h$ is known as the __bandwidth__."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "floppy-cancellation",
   "metadata": {},
   "source": [
    "üíª The following code estimates the density of `lwage` at 20 evaluation points, $\\{-2.054,-1.776,\\ldots,2.941,3.219\\}$, i.e., $\\widehat{f}(-2.054), \\widehat{f}(-1.776),\\ldots,\\widehat{f}(2.941),\\widehat{f}(3.219)$ using $h=0.537$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collectible-appointment",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "x = np.linspace(mroz.lwage.min(),mroz.lwage.max(),20)\n",
    "\n",
    "from statsmodels.nonparametric.kde import KDEUnivariate as kde\n",
    "dens = kde(mroz[mroz.lwage.notna()].lwage)\n",
    "dens.fit(kernel='gau',bw=0.537)\n",
    "dens.evaluate(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "about-oxygen",
   "metadata": {},
   "source": [
    "üìù The following code shows the effect the hyperparameter (bandwidth) $h$ has on the estimates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "subjective-prayer",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(mroz.lwage.min(),mroz.lwage.max(),40)\n",
    "\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "fig, ax = plt.subplots()\n",
    "for bandwidth in [0.01, 0.13, 1.0]:\n",
    "    ax.plot(x, kde(mroz[mroz.lwage.notna()].lwage).fit(kernel='gau',bw=bandwidth).evaluate(x),\n",
    "            label='bw={0}'.format(bandwidth), linewidth=3, alpha=0.5)\n",
    "ax.hist(mroz[mroz.lwage.notna()].lwage, bins=20, fc='gray', histtype='stepfilled', alpha=0.3,density=True)\n",
    "ax.legend(loc='upper left')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "measured-inquiry",
   "metadata": {},
   "source": [
    "<h3>Discrete</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "activated-visit",
   "metadata": {},
   "source": [
    "Recall that if the random variable, $X$, can take on a finite number of values in $\\mathcal{S}=\\{0,1,2, \\ldots, c-1\\}$, then its __probability function__ is defined as\n",
    "\n",
    "$$\n",
    "p(s)=\\operatorname{Pr}(X=s)=\\operatorname{Pr}(X \\leq s)-\\operatorname{Pr}(X \\leq s-1)=F(s)-F(s-1)\\text{,}\n",
    "$$\n",
    "\n",
    "and a natural estimator will be\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\widehat{p}(s) &=\\frac{1}{n} \\sum_{i=1}^{n} \\mathbb{I}\\left(X_{i} \\leq s\\right)-\\frac{1}{n} \\sum_{i=1}^{n} \\mathbb{I}\\left(X_{i} \\leq s-1\\right) \\\\\n",
    "&=\\frac{1}{n} \\sum_{i=1}^{n}\\left\\{\\mathbb{I}\\left(X_{i} \\leq s\\right)-\\mathbb{I}\\left(X_{i} \\leq s-1\\right)\\right\\} \\\\\n",
    "&=\\frac{1}{n} \\sum_{i=1}^{n} \\mathbb{I}\\left(s-1<X_{i} \\leq s\\right) \\\\\n",
    "&=\\frac{1}{n} \\sum_{i=1}^{n} \\mathbb{I}\\left(X_{i}=s\\right)=\\frac{1}{n} \\sum_{i=1}^{n} 1^{\\mathbb{I}\\left(X_{i}=s\\right)} \\times 0^{1-\\mathbb{I}\\left(X_{i}=s\\right)}\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lined-vehicle",
   "metadata": {},
   "source": [
    "<h4>Unordered</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "assigned-recycling",
   "metadata": {},
   "source": [
    "$$\n",
    "\\widehat{p}(s)=\\frac{1}{n} \\sum_{i=1}^{n}[1-\\lambda]^{\\mathbb{I}\\left(X_{i}=s\\right)} \\times[\\lambda /(c-1)]^{1-\\mathbb{I}\\left(X_{i}=s\\right)}=\\frac{1}{n} \\sum_{i=1}^{n}l(X_i,s;\\lambda)\n",
    "$$\n",
    "\n",
    "üìù The hyperparameter $\\lambda\\in\\left[0,(c-1)/c\\right]$ is known as the __bandwidth__.\n",
    "\n",
    "   1. If $\\lambda=0$ we have $\\widehat{p}(s)=n^{-1} \\sum_{i=1}^{n} \\mathbb{I}\\left(X_{i}=s\\right)$.\n",
    "   2. If $\\lambda=(c-1)/c$, then $\\widehat{p}(s)=1/c$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "infrared-nirvana",
   "metadata": {},
   "source": [
    "üíª The following code estimates the probability function of `city` at 1 and 0, i.e., $\\widehat{p}(1)$ and $\\widehat{p}(0)$ using $\\lambda=0.1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "municipal-martial",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.nonparametric.kernel_density import KDEMultivariate as KDE\n",
    "x = np.array([1,0])\n",
    "dens_u = KDE(data=mroz.city,var_type='u',bw=np.array([0.1]))\n",
    "dens_u.pdf(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "civilian-locator",
   "metadata": {},
   "source": [
    "<h4>Ordered</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spatial-violin",
   "metadata": {},
   "source": [
    "$$\n",
    "\\widehat{p}(s)=\\frac{1}{n} \\sum_{i=1}^{n}\\lambda^{|s-X_i|}=\\frac{1}{n} \\sum_{i=1}^{n}l(X_i,s;\\lambda)\n",
    "$$\n",
    "\n",
    "üìù The hyperparameter $\\lambda$ is known as the bandwidth and is such that $\\lambda\\in\\left[0,1\\right]$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lonely-married",
   "metadata": {},
   "source": [
    "üíª The following code estimates the probability function of `kidslt6` at 0, 1, 2, and 3, i.e., $\\widehat{p}(0)$, $\\widehat{p}(1)$, $\\widehat{p}(2)$, and $\\widehat{p}(3)$ with a bandwidth $\\lambda=0.15$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cubic-legend",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([0,1,2,3])\n",
    "dens_u = KDE(data=mroz.kidslt6,var_type='o',bw=np.array([0.15]))\n",
    "dens_u.pdf(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "endless-insured",
   "metadata": {},
   "source": [
    "<h2>Density Estimation: Multivariate ($q$ Continuous & $r$ Discrete)</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "needed-tomato",
   "metadata": {},
   "source": [
    "Suppose that\n",
    "\n",
    "$$\n",
    "\\begin{array}{l}\n",
    "X_{i}^{\\mathrm{c}}=\\left(X_{i ; 1}^{\\mathrm{c}}, \\ldots, X_{i ; q}^{\\mathrm{c}}\\right)^{\\top}, x^{\\mathrm{c}}=\\left(x_{1}^{\\mathrm{c}}, \\ldots, x_{q}^{\\mathrm{c}}\\right)^{\\top} \\\\\n",
    "X_{i}^{\\mathrm{d}}=\\left(X_{i ; 1}^{\\mathrm{d}}, \\ldots, X_{i, r}^{\\mathrm{d}}\\right)^{\\top}, x^{\\mathrm{d}}=\\left(x_{1}^{\\mathrm{d}}, \\ldots, x_{r}^{\\mathrm{d}}\\right)^{\\top},\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "then the kernel density estimator is\n",
    "\n",
    "$$\n",
    "\\widehat{f}\\left(x^{\\mathrm{c}}, x^{\\mathrm{d}}\\right)=\\frac{1}{n h_{1} \\times \\ldots \\times h_{q}} \\sum_{i=1}^{n} \\prod_{j=1}^{q} k\\left(X_{i ; j}^{\\mathrm{c}}, x_{j}^{\\mathrm{c}} ; h_{j}\\right) \\prod_{s=1}^{r} l\\left(X_{i ; s}^{\\mathrm{d}}, x_{s}^{\\mathrm{d}} ; \\lambda_{s}\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "departmental-arkansas",
   "metadata": {},
   "source": [
    "üíª Let $X^{\\text{c}}=$ `lwage`, and $X^{\\text{d}}=($ `city` , `kidslt6` $)$, then the following code estimates $\\widehat{f}$ when $x^{\\text{c}}=1.5$ and $X^{\\text{d}}=(1,2)$ using bandwidths $(h,\\lambda_1,\\lambda_2)$ equal to a [rule-of-thumb bandwidth](https://en.wikipedia.org/wiki/Kernel_density_estimation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "excessive-curve",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([1.5,1,2]) # lwage=1.5, city=1, kidslt6=2\n",
    "dens_u = KDE(data=mroz[mroz.lwage.notna()][['lwage','city','kidslt6']],var_type='cuo',bw='normal_reference')\n",
    "print(dens_u.pdf(x))\n",
    "print(dens_u.bw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "legitimate-chambers",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from statsmodels.nonparametric.kernel_density import KDEMultivariate as KDE\n",
    "\n",
    "# Create surface plot data\n",
    "x = np.linspace(mroz.educ.min(),mroz.educ.max(), 100)\n",
    "y = np.linspace(mroz.exper.min(),mroz.exper.max(), 100)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "dens = KDE(data=mroz[['educ','exper']],var_type='cc',bw='normal_reference')\n",
    "Z = np.array([dens.pdf(pair) for pair in zip(X.ravel(), Y.ravel())])\n",
    "Z = Z.reshape(X.shape)\n",
    "# Plot it\n",
    "fig = plt.figure()\n",
    "ax =fig.gca(projection='3d')\n",
    "ax.set_xlabel('educ')\n",
    "ax.set_ylabel('exper')\n",
    "ax.set_zlabel(r'$\\hat{f}(educ,exper)$')\n",
    "ax.azim = -40\n",
    "ax.dist = 10\n",
    "ax.elev = 20\n",
    "surface = ax.plot_surface(X, Y, Z)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "optical-simulation",
   "metadata": {},
   "source": [
    "<h2>Regression Estimation</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "concrete-invention",
   "metadata": {},
   "source": [
    "Suppose we have a sample, $\\left(Y_{1}, X_{1}^{\\mathrm{c}}, X_{1}^{\\mathrm{d}}\\right), \\ldots,\\left(Y_{n}, X_{n}^{\\mathrm{c}}, X_{n}^{\\mathrm{d}}\\right)$\n",
    "and define $E[Y \\mid X=x]=m(x)$ then we are interested in estimating $m(\\cdot)$ at the evaluation point $x=\\left(x^{\\mathrm{c}}, x^{\\mathrm{d}}\\right)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "perfect-indonesia",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = [\"kidslt6\", \"kidsge6\", \"city\", \"age\", \"educ\",\"exper\", \"hushrs\", \"husage\", \"huseduc\",\"huswage\",\"nwifeinc\",\"mtr\",\"unem\"]\n",
    "X = mroz[tmp]\n",
    "y = mroz[\"inlf\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "animated-adoption",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wired-bridge",
   "metadata": {},
   "source": [
    " There are two possibilities:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "changed-notice",
   "metadata": {},
   "source": [
    "<h3>Local Constant</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comprehensive-indonesian",
   "metadata": {},
   "source": [
    "An estimator, $\\widehat{m}(\\cdot)$, at the evaluation point $x=(x^{\\text{c}},x^{\\text{d}})$ can be found as the solution of the following minimization problem\n",
    "\n",
    "$$\n",
    "\\min _{a \\in \\mathbb{R}} \\sum_{i=1}^{n}\\left\\{Y_{i}-a\\right\\}^{2} \\prod_{j=1}^{q_{x}} k\\left(X_{i ; j}^{\\mathrm{c}}, x_{j}^{\\mathrm{c}} ; h_{j}\\right) \\prod_{s=1}^{r_{x}} l\\left(X_{i ; s}^{\\mathrm{d}}, x_{s}^{\\mathrm{d}} ; \\lambda_{s}\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "needed-setting",
   "metadata": {},
   "source": [
    "üìù It can be shown that $\\widehat{a}\\left(x\\right) \\rightarrow m\\left(x\\right)$ as $n \\rightarrow \\infty$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "copyrighted-pencil",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code creates a set of bandwidths that use a rule-of-thumb\n",
    "dens_u = KDE(data=X_train,var_type='ooucccccccccc',bw='normal_reference')\n",
    "bandwidth=dens_u.bw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "written-relay",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code fits a local constant nonparametric regression with rule-of-thumb bandwidths\n",
    "from statsmodels.nonparametric.kernel_regression import KernelReg as KREG\n",
    "reg_lc = KREG(endog=y_train,exog=X_train,reg_type='lc',var_type = 'ooucccccccccc',bw=bandwidth)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "illegal-guatemala",
   "metadata": {},
   "source": [
    "A useful statistic is defined as the $R^2$\n",
    "\n",
    "$$\n",
    "R^{2}:=\\frac{\\left(\\sum_{i=1}^{n}\\left(Y_{i}-\\bar{Y}\\right)\\left(\\hat{Y}_{i}-\\bar{Y}\\right)\\right)^{2}}{\\left(\\sum_{i=1}^{n}\\left(Y_{i}-\\bar{Y}\\right)^{2}\\right)\\left(\\sum_{i=1}^{n}\\left(\\hat{Y}_{i}-\\bar{Y}\\right)^{2}\\right)}\n",
    "$$\n",
    "\n",
    "and is neither the squared correlation coefficient between $Y_{1}, \\ldots, Y_{n}$ and $\\hat{Y}_{1}, \\ldots, \\hat{Y}_{n}$ nor \"the percentage of variance explained\" by the model - this interpretation makes sense within the linear model context only. It is however a quantity in $[0,1]$ that attains $R^{2}=1$ whenever the fit is perfect (zero variability about the curve), so it can give us an idea of how informative the estimated regression function is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "careful-staff",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(reg_lc.r_squared())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "encouraging-nickel",
   "metadata": {},
   "source": [
    "üíª The following code _fit_ the nonparametric regression at each data point in the validation set and return these estimates (`ahat`) as well as estimates of the derivative of this regression evaluated at each data point in the test set (`bhat`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spare-taste",
   "metadata": {},
   "outputs": [],
   "source": [
    "ahat, bhat =reg_lc.fit(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ranking-inspiration",
   "metadata": {},
   "source": [
    "üíª We now evaluate the percentage of correct predictions in the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "theoretical-benchmark",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_test, ahat>=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sublime-cycling",
   "metadata": {},
   "source": [
    "<h4>Hyperparameter Optimal Choice</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "broad-compensation",
   "metadata": {},
   "source": [
    "One can choose $h_{1}, \\ldots, h_{q_{x}}, \\lambda_{1}, \\ldots, \\lambda_{r_{x}}$ that numerically minimize\n",
    "$$\n",
    "\\sum_{i=1}^{n}\\left[Y_{i}-\\widehat{m}_{-i}\\left(X_{i}\\right)\\right]^{2} M\\left(X_{i}\\right)\n",
    "$$\n",
    "where $\\hat{m}_{-i}$ is the local constant estimator evaluated at the $i$ observation but utilizing the $n-1$ data points _excluding_ observation $i$ and $M(\\cdot)$ is a weighting function. This method is called the __least squares cross-validation__ and it is very computationally intensive."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "female-teaching",
   "metadata": {},
   "source": [
    "üíª The following code implements it and then saves the relevant regression object in a _pickle_ for later use, see, e.g., [pickle](https://pythonprogramming.net/python-pickle-module-save-objects-serialization/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "filled-inquiry",
   "metadata": {},
   "source": [
    "````Python\n",
    "import time\n",
    "import statsmodels as sm\n",
    "tic = time.time()\n",
    "reg_lc_cv_ls = sm.nonparametric.kernel_regression.KernelReg(endog=y_train,exog=X_train,\\\n",
    "                  reg_type='lc',var_type = 'ooucccccccccc',bw='cv_ls')\n",
    "toc = time.time()\n",
    "print(\"Total elapsed time:\", (toc-tic)/60, \"minutes.\")\n",
    "\n",
    "import pickle \n",
    "file_reg_lc_cv_ls = open('reg_lc_cv_ls.obj', 'wb') \n",
    "pickle.dump(reg_lc_cv_ls,file_reg_lc_cv_ls)\n",
    "file_reg_lc_cv_ls.close()\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "drawn-walnut",
   "metadata": {},
   "source": [
    "üíª The regression object `reg_lc_cv_ls` was saved in an object called `reg_lc_cv_ls.obj` and the following code loads the content of this object and it is ready for use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "atlantic-walker",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "file_reg_lc_cv_ls = open('reg_lc_cv_ls.obj','rb')\n",
    "reg_lc_cv_ls = pickle.load(file_reg_lc_cv_ls)\n",
    "file_reg_lc_cv_ls.close()\n",
    "print(reg_lc_cv_ls.bw)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "impressive-universe",
   "metadata": {},
   "source": [
    "üíª We now evaluate the proportion of correct predictions in the validation set using the optimal hyperparameters for the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "celtic-civilization",
   "metadata": {},
   "outputs": [],
   "source": [
    "ahat, bhat =reg_lc_cv_ls.fit(X_test)\n",
    "accuracy_score(y_test, ahat>=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "challenging-norway",
   "metadata": {},
   "source": [
    "<h3>Local Linear</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reverse-magnet",
   "metadata": {},
   "source": [
    "Another estimator, $\\widehat{m}(\\cdot)$, at the evaluation point $x=(x^{\\text{c}},x^{\\text{d}})$ can also be found as the solution of the following minimization problem\n",
    "\n",
    "$$\n",
    "\\min _{a, b \\in \\mathbb{R}^{2}} \\sum_{i=1}^{n}\\left\\{Y_{i}-a-\\left(X_{i}^{\\mathrm{c}}-x^{\\mathrm{c}}\\right)^{\\top} b\\right\\}^{2} \\prod_{j=1}^{q_{x}} k\\left(X_{i ; j}^{\\mathrm{c}}, x_{j}^{\\mathrm{c}} ; h_{j}\\right) \\prod_{S=1}^{r_{x}} l\\left(X_{i ; s}^{\\mathrm{d}}, x_{s}^{\\mathrm{d}} ; \\lambda_{s}\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "governing-locking",
   "metadata": {},
   "source": [
    "üìù It can be shown that $\\widehat{a}\\left(x\\right) \\rightarrow m\\left(x\\right)$ and $\\hat{b}\\left(x^{c}\\right) \\rightarrow \\partial m\\left(x^{c}\\right) / \\partial x^{c}$\n",
    "as $n \\rightarrow \\infty$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "herbal-henry",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_ll = KREG(endog=y_train,exog=X_train,reg_type='ll',var_type = 'ooucccccccccc',bw=dens_u.bw)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "partial-concrete",
   "metadata": {},
   "source": [
    "üíª This code will fit the _local linear_ estimator at each observation in the validation set and print the proportion of correct predictions using the _rule-of-thumb_ hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fifteen-canadian",
   "metadata": {},
   "outputs": [],
   "source": [
    "ahat, bhat =reg_ll.fit(X_test)\n",
    "accuracy_score(y_test, ahat>=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "defensive-aaron",
   "metadata": {},
   "source": [
    "<h4>Hyperparameter Optimal Choice</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mechanical-clark",
   "metadata": {},
   "source": [
    "One can choose $h_{1}, \\ldots, h_{q_{x}}, \\lambda_{1}, \\ldots, \\lambda_{r_{x}}$ that numerically minimize\n",
    "$$\n",
    "\\sum_{i=1}^{n}\\left[Y_{i}-\\widehat{m}_{-i}\\left(X_{i}\\right)\\right]^{2} M\\left(X_{i}\\right)\n",
    "$$\n",
    "where $\\hat{m}_{-i}$ is the local constant estimator evaluated at the $i$ observation but utilizing the $n-1$ data points _excluding_ observation $i$ and $M(\\cdot)$ is a weighting function. This method is called the __least squares cross-validation__ and it is very computationally intensive."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "published-trinidad",
   "metadata": {},
   "source": [
    "üíª The following code implements it and then saves the relevant regression object in a _pickle_ for later use, see, e.g., [pickle](https://pythonprogramming.net/python-pickle-module-save-objects-serialization/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "linear-donor",
   "metadata": {},
   "source": [
    "````Python\n",
    "import time\n",
    "import statsmodels as sm\n",
    "tic = time.time()\n",
    "reg_ll_cv_ls = sm.nonparametric.kernel_regression.KernelReg(endog=y_train,exog=X_train,\\\n",
    "                  reg_type='ll',var_type = 'ooucccccccccc',bw='cv_ls')\n",
    "toc = time.time()\n",
    "print(\"Total elapsed time:\", (toc-tic)/60, \"minutes.\")\n",
    "\n",
    "import pickle \n",
    "file_reg_ll_cv_ls = open('reg_ll_cv_ls.obj', 'wb') \n",
    "pickle.dump(reg_ll_cv_ls, file_reg_ll_cv_ls)\n",
    "file_reg_ll_cv_ls.close()\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "internal-allen",
   "metadata": {},
   "source": [
    "üíª The regression object `reg_ll_cv_ls` was saved in an object called `reg_ll_cv_ls.obj` and the following code loads the content of this object and it is ready for use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "junior-handle",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "file_reg_ll_cv_ls = open('reg_ll_cv_ls.obj','rb')\n",
    "reg_ll_cv_ls = pickle.load(file_reg_ll_cv_ls)\n",
    "file_reg_ll_cv_ls.close()\n",
    "reg_ll_cv_ls.bw"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "desirable-tracy",
   "metadata": {},
   "source": [
    "üíª The following code _fit_ the nonparametric regression at each data point in the validation set and return these estimates (`ahat`) as well as estimates of the derivative of this regression evaluated at each data point in the test set (`bhat`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "answering-treat",
   "metadata": {},
   "outputs": [],
   "source": [
    "ahat, bhat =reg_ll_cv_ls.fit(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "removable-australian",
   "metadata": {},
   "source": [
    "üíª We now evaluate the proportion of correct predictions in the validation set using the optimal hyperparameters for the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "angry-democracy",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_test, ahat>=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "engaged-brick",
   "metadata": {},
   "source": [
    "<u>Summary of Hyperparameters</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "parallel-disorder",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "bw = pd.DataFrame(np.array([tmp,bandwidth.round(4),reg_lc_cv_ls.bw.round(4),reg_ll_cv_ls.bw.round(4)]).T)\n",
    "bw.columns =['var', 'rot', 'lc_cv_ls', 'll_cv_ls']\n",
    "bw.set_index('var')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "modular-hypothesis",
   "metadata": {},
   "source": [
    "<h2>Pros & Cons</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rolled-audit",
   "metadata": {},
   "source": [
    "<h3>Pros</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "metropolitan-ability",
   "metadata": {},
   "source": [
    "1. One only needs to provide the set of $k$ features and outcome variable for training. We do not need to provide a _correct_ parametric specification.\n",
    "2. We only need to specify the _nature_ of the features and hyperparameters can be calculated optimally using the training set.\n",
    "3. It has been shown that:\n",
    "    1. If a continuous feature is _irrelevant_ in explaining $Y$, then its optimal bandwidth selected by _least squares cross-validation_ $h_j\\rightarrow\\infty$ as $n\\rightarrow\\infty$.\n",
    "    2. If an unordered discrete feature is _irrelevant_ in explaining $Y$, then its optimal bandwidth selected by _least squares cross-validation_ $\\lambda_j\\rightarrow(c-1)/c$ as $n\\rightarrow\\infty$.\n",
    "    Therefore, hyperparameter selection via least squares cross-validation performs _model selection_."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "right-dynamics",
   "metadata": {},
   "source": [
    "<h3>Cons</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "boring-processing",
   "metadata": {},
   "source": [
    "1. _Least squares cross-validation_ is computationally very intensive and depending on the number of features it can become prohibitive.\n",
    "2. It requires that $E[Y \\mid X=x]=m(x)$ is a smooth function with respect to $x^{\\text{c}}$."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
