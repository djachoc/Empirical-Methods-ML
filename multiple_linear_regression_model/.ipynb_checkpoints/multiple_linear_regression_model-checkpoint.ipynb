{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# _Multiple_ Linear Regression (MLR) Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the population we have <ins>scalar</ins> random variables, $[Y,X_1,\\ldots,X_{k},e]$, that fulfill the following relationship\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "Y=\\beta_0+\\beta_1 X_1+\\ldots+\\beta_k X_k+e\\text{,}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "where $E[e|X_1,\\ldots,X_{k}]=0$, there are no exact linear relationships among the set of regressors (covariates, confounders, independent variables, predictors, etc.) $[X_1,\\ldots,X_{k}]$, and var$(e|X_1,\\ldots,X_{k})<+\\infty$. The scalar random variable, $Y$, is called the outcome, or the dependent variable.\n",
    "\n",
    "\n",
    "\n",
    "<p style='text-align: right;'> <a href=\"https://en.wikipedia.org/wiki/Conditional_expectation\" style=\"color: #cc0000\">Conditional Expectation</a></p>\n",
    "\n",
    "<p style='text-align: right;'> <a href=\"https://en.wikipedia.org/wiki/Conditional_variance\" style=\"color: #cc0000\">Conditional Variance</a></p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe a _random sample_ of $n$ observations taken from $[Y,X_1,\\ldots,X_{k},e]$, i.e., $\\{(y_i,x_{i,1},\\ldots,x_{i,k}):i=1,\\ldots,n\\}$. Therefore one has\n",
    "\n",
    "$$\n",
    "\\begin{array}\n",
    "[c]{c}\n",
    "y_{1}=\\beta_{0}+\\beta_{1}x_{1,1}+\\cdots+\\beta_{k}x_{1,k}+e_{1}\\\\\n",
    "y_{2}=\\beta_{0}+\\beta_{1}x_{2,1}+\\cdots+\\beta_{k}x_{2,k}+e_{2}\\\\\n",
    "y_{3}=\\beta_{0}+\\beta_{1}x_{3,1}+\\cdots+\\beta_{k}x_{3,k}+e_{3}\\\\\n",
    "\\vdots\\\\\n",
    "y_{n}=\\beta_{0}+\\beta_{1}x_{n,1}+\\cdots+\\beta_{k}x_{n,k}+e_{n}\n",
    "\\end{array}\n",
    "$$ or equivalently\n",
    "$$\\left[\n",
    "\\begin{array}\n",
    "[c]{c}\n",
    "y_{1}\\\\\n",
    "y_{2}\\\\\n",
    "y_{3}\\\\\n",
    "\\vdots\\\\\n",
    "y_{n}\n",
    "\\end{array}\n",
    "\\right]  =\\left[\n",
    "\\begin{array}\n",
    "[c]{cccc}\n",
    "1 & x_{1,1} & \\cdots & x_{1,k}\\\\\n",
    "1 & x_{2,1} & \\cdots & x_{2,k}\\\\\n",
    "1 & x_{3,1} & \\cdots & x_{3,k}\\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots\\\\\n",
    "1 & x_{n,1} & \\cdots & x_{n,k}\n",
    "\\end{array}\n",
    "\\right]  \\left[\n",
    "\\begin{array}\n",
    "[c]{c}\n",
    "\\beta_{0}\\\\\n",
    "\\beta_{1}\\\\\n",
    "\\vdots\\\\\n",
    "\\beta_{k}\n",
    "\\end{array}\n",
    "\\right]  +\\left[\n",
    "\\begin{array}\n",
    "[c]{c}\n",
    "e_{1}\\\\\n",
    "e_{2}\\\\\n",
    "e_{3}\\\\\n",
    "\\vdots\\\\\n",
    "e_{n}\n",
    "\\end{array}\n",
    "\\right]\n",
    "$$ that can be rewritten in **matrix form**  as\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathbf{y}=\\mathbf{X}\\mathbf{\\beta}+\\mathbf{e}\\text{,}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "where $\\mathbf{y}$ is a $n\\times 1$ [vector](https://en.wikipedia.org/wiki/Vector_(mathematics_and_physics)), $\\mathbf{X}$ is a $n\\times(k+1)$ [matrix](https://en.wikipedia.org/wiki/Matrix_(mathematics)) - sometimes called the [*design matrix*](https://en.wikipedia.org/wiki/Design_matrix), $\\mathbf{\\beta}$ is a $(k+1)\\times 1$ vector of <ins>unknown</ins> parameters, and $\\mathbf{e}$ is a $n\\times 1$ vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## removing everything from memory\n",
    "rm(list=ls())\n",
    "## turning all warnings off\n",
    "options(warn=-1)\n",
    "\n",
    "## installing the 'wooldridge' package if not previously installed\n",
    "if (!require(wooldridge)) install.packages('wooldridge')\n",
    "\n",
    "## loading the packages\n",
    "library(wooldridge)\n",
    "\n",
    "data(hprice2)\n",
    "\n",
    "##  hprice2\n",
    "##  Obs:   506\n",
    "\n",
    "##  1. price                    median housing price, $\n",
    "##  2. crime                    crimes committed per capita\n",
    "##  3. nox                      nitrous oxide, parts per 100 mill.\n",
    "##  4. rooms                    avg number of rooms per house\n",
    "##  5. dist                     weighted dist. to 5 employ centers\n",
    "##  6. radial                   accessibiliy index to radial hghwys\n",
    "##  7. proptax                  property tax per $1000\n",
    "##  8. stratio                  average student-teacher ratio\n",
    "##  9. lowstat                  % of people 'lower status'\n",
    "## 10. lprice                   log(price)\n",
    "## 11. lnox                     log(nox)\n",
    "## 12. lproptax                 log(proptax)\n",
    "\n",
    "head(hprice2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Assumptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<span style=\"color:blue\">Assumption MLR.1:</span>** $\\mathbf{y}=\\mathbf{X}\\mathbf{\\beta}+\\mathbf{e}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ins>Example</ins>: We specify the following model \n",
    "\n",
    "$$\n",
    "\\texttt{lprice}=\\beta_{0}+\\beta_{1}\\texttt{lnox}+\\beta_{2}\\texttt{lproptax}+\\beta_{3}\\texttt{crime}+\\beta_{4}\\texttt{rooms}+\\beta_{5}\\texttt{dist}+\\beta_{6}\\texttt{radial}+\\beta_{7}\\texttt{stratio}+\\beta_{8}\\texttt{lowstat}+e\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üíª We can use ```as.formula``` to specify the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## specifying the outcome variable (y) and regressors (X)\n",
    "outcome <- \"lprice\"\n",
    "predictors <- c(\"lnox\", \"lproptax\", \"crime\", \"rooms\", \"dist\", \"radial\", \"stratio\", \"lowstat\")\n",
    "\n",
    "## creating a specification of the linear model\n",
    "f <- as.formula(\n",
    "                paste(outcome, \n",
    "                      paste(predictors, collapse = \" + \"), \n",
    "                      sep = \" ~ \")\n",
    "                )\n",
    "print(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<span style=\"color:blue\">Assumption MLR.2:</span>** $\\{(y_i,x_{i,1},\\ldots,x_{i,k}):i=1,\\ldots,n\\}$ is a random sample (independent and identically distributed - i.i.d.)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üíª ```head``` allows us to visualize the structure of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "head(subset(hprice2,select=c(outcome,predictors)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<span style=\"color:blue\">Assumption MLR.3:</span>** rank$[E(\\mathbf{x}_i\\mathbf{x}_i^\\prime)]=k+1$, where $\\mathbf{x}_i^\\prime=[1,x_{i,1},\\ldots,x_{i,k}]$. <p style='text-align: right;'> <a href=\"https://en.wikipedia.org/wiki/Rank_(linear_algebra)\" style=\"color: #cc0000\">Rank of a Matrix</a></p>\n",
    "<p style='text-align: right;'> <a href=\"https://en.wikipedia.org/wiki/Transpose\" style=\"color: #cc0000\">Transpose</a></p> <p style='text-align: right;'> <a href=\"https://en.wikipedia.org/wiki/Expected_value\" style=\"color: #cc0000\">Expected Value</a></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üíª ```model.matrix``` creates a design matrix based on the declared predictors and includes a vector of ones by default as the first column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## asking R to print the design matrix for the chosen model\n",
    "X <- model.matrix(f,data=hprice2)\n",
    "dim(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## calculating & printing the sample counterpart of E[xx']\n",
    "X.X.n <- t(X)%*%X/nrow(X)\n",
    "X.X.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## asking R to calculate the actual rank of the estimated E[xx']\n",
    "qr(X.X.n)$rank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<span style=\"color:blue\">Assumption MLR.4:</span>** $E[\\mathbf{e}|\\mathbf{X}]=\\mathbf{0}$.\n",
    "\n",
    "$$\n",
    "E[\\mathbf{e}|\\mathbf{X}]=\\left[\n",
    "\\begin{array}\n",
    "[c]{c}\n",
    "E[e_{1}|\\mathbf{X}]\\\\\n",
    "E[e_{2}|\\mathbf{X}]\\\\\n",
    "E[e_{3}|\\mathbf{X}]\\\\\n",
    "\\vdots\\\\\n",
    "E[e_{n}|\\mathbf{X}]\n",
    "\\end{array}\n",
    "\\right]  =\\left[\n",
    "\\begin{array}\n",
    "[c]{c}\n",
    "0\\\\\n",
    "0\\\\\n",
    "0\\\\\n",
    "\\vdots\\\\\n",
    "0\n",
    "\\end{array}\n",
    "\\right]=\\mathbf{0}.  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<span style=\"color:blue\">Assumption MLR.5:</span>** var$(\\mathbf{e}\\mathbf{e}^{\\prime}|\\mathbf{X})=E[\\mathbf{e}\\mathbf{e}^{\\prime}|\\mathbf{X}]=\\mathbf{D}$, i.e.,\n",
    "\n",
    "$$\n",
    "E[\\mathbf{e}\\mathbf{e}^{\\prime}|\\mathbf{X}]=\n",
    "\\begin{bmatrix}\n",
    "E[e_{1}^{2}|\\mathbf{X}] & 0 & \\cdots & 0\\\\\n",
    "0 & E[e_{2}^{2}|\\mathbf{X}] & \\cdots & 0\\\\\n",
    "0 & 0 & \\cdots & 0\\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots\\\\\n",
    "0 & 0 & \\cdots & E[e_{n}^{2}|\\mathbf{X}]\n",
    "\\end{bmatrix}  = \\mathbf{D}\n",
    "$$\n",
    "\n",
    "where $\\vert\\mathbf{D}\\vert < +\\infty$.\n",
    "\n",
    "<p style='text-align: right;'> <a href=\"https://en.wikipedia.org/wiki/Determinant\" style=\"color: #cc0000\">Determinant</a></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The *Ordinary Least Squares* (OLS) Estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the Sum of Squared Errors ($\\mathrm{SSE}$) as a function of any candidate guess, $\\mathbf{b}=[b_{0},b_{1},\\cdots,b_{k}]^{\\prime}$, for the unknown $[\\beta_{0},\\beta_{1},\\beta_{2},\\cdots,\\beta_{k}]^{\\prime}\\equiv\\mathbf{\\beta}$, i.e.,\n",
    "\n",
    "$$\n",
    "\\mathrm{SSE}(\\mathbf{b})=\\Sigma_{i=1}^{n}(y_{1}-b_{0}-b_{1}x_{1}-b_{2}x_{2}\n",
    "-\\cdots-b_{k}x_{k})^{2}=(\\mathbf{y}-\\mathbf{Xb})^{\\prime}(\\mathbf{y}\n",
    "-\\mathbf{Xb})\n",
    "$$\n",
    "\n",
    "By standard [matrix calculus](https://en.wikipedia.org/wiki/Matrix_calculus) one has\n",
    "$$\n",
    "\\frac{\\partial \\mathrm{SSE}(\\mathbf{b})}{\\partial\\mathbf{b}}=\\left[\n",
    "\\begin{array}\n",
    "[c]{c}\n",
    "\\partial \\mathrm{SSE}(\\mathbf{b})/\\partial b_{0}\\\\\n",
    "\\partial \\mathrm{SSE}(\\mathbf{b})/\\partial b_{1}\\\\\n",
    "\\partial \\mathrm{SSE}(\\mathbf{b})/\\partial b_{2}\\\\\n",
    "\\vdots\\\\\n",
    "\\partial \\mathrm{SSE}(\\mathbf{b})/\\partial b_{k}\n",
    "\\end{array}\n",
    "\\right]  =-2\\mathbf{X}^{\\prime}(\\mathbf{y}-\\mathbf{Xb})\\text{,}\n",
    "$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\n",
    "\\frac{\\partial^{2}\\mathrm{SSE}(\\mathbf{b})}{\\partial\\mathbf{b}\\partial\\mathbf{b}\n",
    "^{\\prime}}=\\left[\n",
    "\\begin{array}\n",
    "[c]{cccc}\n",
    "\\partial^{2}\\mathrm{SSE}(\\mathbf{b})/\\partial b_{0}^{2} & \\partial^{2}\\mathrm{SSE}(\\mathbf{b}\n",
    ")/\\partial b_{0}\\partial b_{1} & \\cdots & \\partial^{2}\\mathrm{SSE}(\\mathbf{b})/\\partial\n",
    "b_{0}\\partial b_{k}\\\\\n",
    "\\partial^{2}\\mathrm{SSE}(\\mathbf{b})/\\partial b_{1}\\partial b_{0} & \\partial\n",
    "^{2}\\mathrm{SSE}(\\mathbf{b})/\\partial b_{1}^{2} & \\cdots & \\partial^{2}\\mathrm{SSE}(\\mathbf{b}\n",
    ")/\\partial b_{1}\\partial b_{k}\\\\\n",
    "\\partial^{2}\\mathrm{SSE}(\\mathbf{b})/\\partial b_{2}\\partial b_{0} & \\partial\n",
    "^{2}\\mathrm{SSE}(\\mathbf{b})/\\partial b_{2}\\partial b_{1} & \\cdots & \\partial\n",
    "^{2}\\mathrm{SSE}(\\mathbf{b})/\\partial b_{2}\\partial b_{k}\\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots\\\\\n",
    "\\partial^{2}\\mathrm{SSE}(\\mathbf{b})/\\partial b_{k}\\partial b_{0} & \\partial\n",
    "^{2}\\mathrm{SSE}(\\mathbf{b})/\\partial b_{k}\\partial b_{1} & \\cdots & \\partial\n",
    "^{2}\\mathrm{SSE}(\\mathbf{b})/\\partial b_{k}^{2}\n",
    "\\end{array}\n",
    "\\right]  =2\\mathbf{X}^{\\prime}\\mathbf{X}\\text{,}\n",
    "$$\n",
    "\n",
    "where $\\mathbf{X}^{\\prime}\\mathbf{X}$ is a [positive definite matrix](https://en.wikipedia.org/wiki/Definiteness_of_a_matrix) by **<span style=\"color:blue\">Assumption MLR.3</span>**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## printing 2X'X\n",
    "2*t(X)%*%X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore $\\left.  \\partial \\mathrm{SSE}(\\mathbf{b})/\\partial\\mathbf{b}\\right\\vert _{\\mathbf{b}=\\widehat{\\boldsymbol{\\beta}}\n",
    "}=\\mathbf{0}$ defines a maxima, i.e.,\n",
    "\n",
    "$$\\widehat{\\boldsymbol{\\beta}}=(\\mathbf{X}\n",
    "^{\\prime}\\mathbf{X})^{-1}\\mathbf{X}^{\\prime}\\mathbf{y}.$$\n",
    "\n",
    "Here we have used the\n",
    "notation $\\mathbf{A}^{-1}$ to denote the inverse of a matrix $\\mathbf{A}$.\n",
    "\n",
    "<p style='text-align: right;'> <a href=\"https://en.wikipedia.org/wiki/Euclidean_distance\" style=\"color: #cc0000\">Euclidean Distance</a></p>\n",
    "<p style='text-align: right;'> <a href=\"https://en.wikipedia.org/wiki/Invertible_matrix\" style=\"color: #cc0000\">Inverse</a></p>\n",
    "<p style='text-align: right;'> <a href=\"https://en.wikipedia.org/wiki/Maxima_and_minima\" style=\"color: #cc0000\">Maxima</a></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üíª Calculating the OLS estimator by hand first and then using the highly optimize ```lm``` command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## calculating OLS by hand\n",
    "solve(t(X)%*%X)%*%(t(X)%*%hprice2$lprice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## calculating OLS using the `lm' command in R\n",
    "coef(lm(f,data=hprice2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The _Algebra_ of the OLS Estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üíª We now will save the ```lm``` object and called it ```ols```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ols <- lm(f,data=hprice2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Fitted Values*: $\\widehat{\\mathbf{y}}=\\mathbf{X}\\widehat{\\boldsymbol{\\beta}}$.\n",
    "\n",
    "*Residuals*: $\\widehat{\\mathbf{e}}=\\mathbf{y}-\\mathbf{X} \\widehat{\\boldsymbol{\\beta}}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## OLS: printing the first 6 outcome, fitted values, & residuals\n",
    "head(data.frame(y=hprice2$lprice,y.hat=fitted(ols),e.hat=resid(ols)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Orthogonality*: $\\mathbf{X}^{\\prime}\\widehat{\\mathbf{e}}=\\mathbf{0}$.\n",
    "\n",
    "‚úèÔ∏è This follows from $\\left.  \\partial \\mathrm{SSE}(\\mathbf{b})/\\partial\\mathbf{b}\\right\\vert _{\\mathbf{b}=\\widehat{\\boldsymbol{\\beta}}\n",
    "}=\\mathbf{0}$, since $$-2\\mathbf{X}^{\\prime}(\\mathbf{y}-\\mathbf{X\\widehat{\\boldsymbol{\\beta}}})=-2\\mathbf{X}^{\\prime}\\widehat{\\mathbf{e}}=\\mathbf{0}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## OLS: showing the orthogonality of the residuals and predictors\n",
    "round(t(X)%*%resid(ols))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Analysis-of-variance*:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\sum_{i=1}^{n}\\left(y_{i}-\\bar{y}\\right)^{2}&=\\sum_{i=1}^{n}\\left(\\widehat{y}_{i}-\\bar{y}\\right)^{2}+\\sum_{i=1}^{n} \\widehat{e}_{i}^{2},\\\\\n",
    "\\text{Total Sum of Squares}&=\\text{Explained Sum of Squares} + \\text{Residual Sum of Squares},\\\\\n",
    "\\left(\\mathbf{y}-\\boldsymbol{\\iota}_{n} \\bar{y}\\right)^{\\prime}\\left(\\mathbf{y}-\\boldsymbol{\\iota}_{n} \\bar{y}\\right)&=\\left(\\widehat{\\mathbf{y}}-\\boldsymbol{\\iota}_{n} \\bar{y}\\right)^{\\prime}\\left(\\widehat{\\mathbf{y}}-\\boldsymbol{\\iota}_{n} \\bar{y}\\right)+\\widehat{\\mathbf{e}}^{\\prime} \\widehat{\\mathbf{e}}.\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "where $\\boldsymbol{\\iota}_n$ is a $n\\times 1$ vector of ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anova(ols)\n",
    "\n",
    "## OLS: total sum of squares\n",
    "sum(anova(ols)[,2])\n",
    "\n",
    "## OLS: explained sum of squares\n",
    "sum(anova(ols)[-9,2])\n",
    "\n",
    "## OLS: residual sum of squares\n",
    "sum(anova(ols)[9,2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Coefficient of Determination* (*$R^2$*):\n",
    "\n",
    "$$\n",
    "R^{2}=\\frac{\\sum_{i=1}^{n}\\left(\\widehat{y}_{i}-\\bar{y}\\right)^{2}}{\\sum_{i=1}^{n}\\left(y_{i}-\\bar{y}\\right)^{2}}=1-\\frac{\\sum_{i=1}^{n} \\widehat{e}_{i}^{2}}{\\sum_{i=1}^{n}\\left(y_{i}-\\bar{y}\\right)^{2}}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## OLS: R2 (manually)\n",
    "sum(anova(ols)[-9,2])/sum(anova(ols)[,2])\n",
    "\n",
    "## OLS: R2 (from lm)\n",
    "summary(ols)$r.squared"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Adjusted R-squared* ($\\overline{R}^2$):\n",
    "\n",
    "$$\n",
    "\\overline{R}^{2}=1-\\frac{(n-1) \\sum_{i=1}^{n} \\widehat{e}_{i}^{2}}{(n-k) \\sum_{i=1}^{n}\\left(y_{i}-\\bar{y}\\right)^{2}}\n",
    "$$\n",
    "\n",
    "‚úèÔ∏è Unlike the $R^2$ which cannot decrease as $k$ increases, $\\overline{R}^2$ can either increase _or_ decrease with $k$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## OLS: adj. R2 (manually)\n",
    "1-(sum(anova(ols)[9,2])/sum(anova(ols)[,2]))*(sum(anova(ols)[,1])/anova(ols)[9,1])\n",
    "\n",
    "## OLS: adj. R2 (from lm)\n",
    "summary(ols)$adj.r.squared"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Leverage Values*:\n",
    "\n",
    "The [leverage values](https://en.wikipedia.org/wiki/Leverage_(statistics)) for the design matrix $\\mathbf{X}$ are the [diagonal](https://en.wikipedia.org/wiki/Main_diagonal) elements of the matrix $\\mathbf{X}\\left(\\mathbf{X}^{\\prime} \\mathbf{X}\\right)^{-1} \\mathbf{X}^{\\prime}$. There are $n$ leverage values, and are typically written as $h_{i i}$ for $i=1, \\ldots, n$. since\n",
    "\n",
    "$$\n",
    "\\mathbf{X}\\left(\\mathbf{X}^{\\prime} \\mathbf{X}\\right)^{-1} \\mathbf{X}^{\\prime}=\n",
    "\\left(\\begin{array}{c}{\\mathbf{x}_{1}^{\\prime}} \\\\ {\\mathbf{x}_{2}^{\\prime}} \\\\ {\\vdots} \\\\ {\\mathbf{x}_{n}^{\\prime}}\\end{array}\\right)\\left(\\mathbf{X}^{\\prime} \\mathbf{X}\\right)^{-1}\\left(\\begin{array}{llll}{\\mathbf{x}_{1}} & {\\mathbf{x}_{2}} & {\\cdots} & {\\mathbf{x}_{n}}\\end{array}\\right)\n",
    "$$\n",
    "\n",
    "they are\n",
    "$$\n",
    "h_{i i}=\\mathbf{x}_{i}^{\\prime}\\left(\\mathbf{X}^{\\prime} \\mathbf{X}\\right)^{-1} \\mathbf{x}_{i}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## OLS: leverage values\n",
    "hii <- hatvalues(ols)\n",
    "head(hii)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Prediction Error* (leave-one-out residual or prediction residual):\n",
    "\n",
    "$$\\widetilde{e}_{i}=y_{i}-\\widetilde{y}_{i},$$ \n",
    "\n",
    "where we use the leave-one-out predicted value for $y_i$, i.e.,\n",
    "\n",
    "$$\n",
    "\\widetilde{y}_{i}=\\mathbf{x}_{i}^{\\prime} \\widehat{\\boldsymbol{\\beta}}_{(-i)},$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\n",
    "\\begin{aligned} \\widehat{\\boldsymbol{\\beta}}_{(-i)} &=\\left(\\sum_{j \\neq i} \\mathbf{x}_{j} \\mathbf{x}_{j}^{\\prime}\\right)^{-1}\\left(\\sum_{j \\neq i} \\mathbf{x}_{j} y_{j}\\right) \\\\ &=\\left(\\mathbf{X}^{\\prime} \\mathbf{X}-\\mathbf{x}_{i} \\mathbf{x}_{i}^{\\prime}\\right)^{-1}\\left(\\mathbf{X}^{\\prime} \\mathbf{y}-\\mathbf{x}_{i} y_{i}\\right) \\\\ &=\\left(\\mathbf{X}_{(-i)}^{\\prime} \\mathbf{X}_{(-i)}\\right)^{-1} \\mathbf{X}_{(-i)}^{\\prime} \\mathbf{y}_{(-i)}. \\end{aligned}\n",
    "$$\n",
    "\n",
    "Here, $\\mathbf{X}_{(-i)}$ and $\\mathbf{y}_{(-i)}$ are the data matrices omitting the $i$th row. The notation $\\widehat{\\boldsymbol{\\beta}}_{(-i)}$ or $\\widehat{\\boldsymbol{\\beta}}_{-i}$ is commonly\n",
    "used to denote an estimator with the $i$th observation omitted.\n",
    "\n",
    "‚úèÔ∏è There is a leave-one-out estimator for each observation, $i=1,\\ldots,n$ so we have $n$ such estimators, and one can show that\n",
    "\n",
    "$$\n",
    "\\widehat{\\boldsymbol{\\beta}}_{(-i)}=\\widehat{\\boldsymbol{\\beta}}-\\left(\\mathbf{X}^{\\prime} \\mathbf{X}\\right)^{-1} \\mathbf{x}_{i} \\widetilde{e}_{i},\n",
    "$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\n",
    "\\widetilde{e}_{i}=\\left(1-h_{i i}\\right)^{-1} \\widehat{e}_{i}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## OLS: original residuals\n",
    "e.hat <- resid(ols)\n",
    "\n",
    "## OLS: calculating the prediction error\n",
    "e.tilde <- resid(ols)/(1-hii)\n",
    "\n",
    "## OLS: manually re-calculating OLS without observation 156\n",
    "data.frame(beta.hat=coef(ols),\n",
    "           beta.hat.i=coef(ols)-solve(t(X)%*%X)%*%X[156,]%*%e.tilde[156])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Estimation of Error Variance*:\n",
    "\n",
    "The _unconditional_ error variance $\\sigma^2=E(e^2_{i})$ can be estimated as\n",
    "\n",
    "1. Estimator 1:\n",
    "\n",
    "$$s^{2}=\\frac{1}{n-k} \\sum_{i=1}^{n} \\widehat{e}_{i}^{2}.$$\n",
    "\n",
    "2. Estimator 2:\n",
    "\n",
    "$$\\widehat{\\sigma}^{2}=\\frac{1}{n} \\sum_{i=1}^{n} \\widehat{e}_{i}^{2}.$$\n",
    "\n",
    "3. Estimator 3:\n",
    "\n",
    "$$\\bar{\\sigma}^{2}=\\frac{1}{n} \\sum_{i=1}^{n} \\bar{e}_{i}^{2}=\\frac{1}{n} \\sum_{i=1}^{n}\\left(1-h_{i i}\\right)^{-1} \\widehat{e}_{i}^{2}.$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## OLS: error variance estimators\n",
    "s2 <- sum(e.hat^2)/(length(e.hat)-dim(X)[2])\n",
    "sigma.hat2 <- sum(e.hat^2)/length(e.hat)\n",
    "sigma.bar2 <- sum(e.hat^2/(1-hii))/length(e.hat)\n",
    "data.frame(s2=s2,sigma.hat2=sigma.hat2,sigma.bar2=sigma.bar2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚úèÔ∏è When $k / n$ is small (typically, this occurs when $n$ is large), the estimators $\\widehat{\\sigma}^{2}, s^{2}$ and $\\overline{\\sigma}^{2}$ are likely to be similar to one another. However, if $k / n$ is large then $s^{2}$ and $\\overline{\\sigma}^{2}$ are generally preferred to $\\widehat{\\sigma}^{2}$. Consequently it is best to use one of the bias-corrected variance estimators in applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The _Finite Sample_ Properties of the OLS Estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<span style=\"color:blue\">Mean of OLS:</span>** Under Assumptions MLR.1, MLR.2, MLR.3, and MLR.4 one has\n",
    "\n",
    "$\\begin{aligned} E(\\widehat{\\boldsymbol{\\beta}} | \\mathbf{X}) &=E\\left(\\left(\\mathbf{X}^{\\prime} \\mathbf{X}\\right)^{-1} \\mathbf{X}^{\\prime} \\mathbf{y} | \\mathbf{X}\\right) \\\\ &=\\left(\\mathbf{X}^{\\prime} \\mathbf{X}\\right)^{-1} \\mathbf{X}^{\\prime} E(\\mathbf{y} | \\mathbf{X}) \\\\ &=\\left(\\mathbf{X}^{\\prime} \\mathbf{X}\\right)^{-1} \\mathbf{X}^{\\prime} \\mathbf{X} \\boldsymbol{\\beta} \\\\ &=\\boldsymbol{\\beta} \\end{aligned}$\n",
    "\n",
    "**<span style=\"color:blue\">Variance of OLS:</span>** Firstly, let us use the notation $\\mathbf{V}_{\\widehat{\\beta}} \\stackrel{d e f}{=} \\text{var}(\\widehat{\\boldsymbol{\\beta}} | \\mathbf{X})$ and recall from Assumption MLR.5 that var$(\\mathbf{e}\\mathbf{e}^{\\prime}|\\mathbf{X})=E[\\mathbf{e}\\mathbf{e}^{\\prime}|\\mathbf{X}]=\\mathbf{D}$. Then under Assumptions MLR.1, MLR.2, MLR.3, MLR.4, and MLR.5 one has\n",
    "\n",
    "$\\begin{aligned} \\mathbf{V}_{\\widehat{\\beta}} &=\\text{var}(\\widehat{\\boldsymbol{\\beta}} | \\mathbf{X}) \\\\ &=\\left(\\mathbf{X}^{\\prime} \\mathbf{X}\\right)^{-1}\\left(\\mathbf{X}^{\\prime} \\mathbf{D} \\mathbf{X}\\right)\\left(\\mathbf{X}^{\\prime} \\mathbf{X}\\right)^{-1} \\end{aligned}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚úèÔ∏è If one has *homoskedasticity*, i.e., $\\mathbf{D}=\\sigma^2\\mathbf{I}_n$, then $\\mathbf{V}_{\\widehat{\\boldsymbol{\\beta}}}=\\sigma^{2}\\left(\\mathbf{X}^{\\prime} \\mathbf{X}\\right)^{-1}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Asymptotic Properties of the OLS Estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<span style=\"color:blue\">(Asymptotic) Distribution of OLS:</span>** As $n\\rightarrow\\infty$, under Assumptions MLR.1, MLR.2, MLR.3, MLR.4, and MLR.5 one has\n",
    "\n",
    "$$\\sqrt{n}(\\widehat{\\boldsymbol{\\beta}}-\\boldsymbol{\\beta}) \\stackrel{d}{\\longrightarrow} \\mathrm{N}\\left(\\mathbf{0}, \\mathbf{V}_{\\boldsymbol{\\beta}}\\right),$$\n",
    "\n",
    "<p style='text-align: right;'> <a href=\"https://en.wikipedia.org/wiki/Convergence_of_random_variables#Convergence_in_distribution\" style=\"color: #cc0000\">Convergence in Distribution</a></p>\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "\\mathbf{V}_{\\boldsymbol{\\beta}}=E(\\mathbf{x}_i\\mathbf{x}_i^{\\prime})^{-1}E[\\mathbf{x}_{i}E(e_i^2|\\mathbf{x}_i)\\mathbf{x}_{i}^{\\prime}]E(\\mathbf{x}_i\\mathbf{x}_i^{\\prime})^{-1},\n",
    "$$\n",
    "\n",
    "and the notation $\\mathbf{N}(\\boldsymbol{\\mu},\\boldsymbol{\\Sigma})$ denotes a [multivariate normal distribution](https://en.wikipedia.org/wiki/Multivariate_normal_distribution). $\\mathbf{V}_{\\boldsymbol{\\beta}}$ is the asymptotic (asy.) [variance-covariance matrix](https://en.wikipedia.org/wiki/Covariance_matrix) of $\\sqrt{n}(\\widehat{\\boldsymbol{\\beta}}-\\boldsymbol{\\beta})$ and therefore $n^{-1}\\mathbf{V}_{\\boldsymbol{\\beta}}$ is the asymptotic variance of $\\widehat{\\boldsymbol{\\beta}}$ and it is generally _unknown_ and needs to be estimated.\n",
    "\n",
    "$$\n",
    "n^{-1}\\mathbf{V}_{\\boldsymbol{\\beta}}=\\left[\n",
    "\\begin{array}\n",
    "[c]{cccc}\n",
    "\\text{asy. var}(\\widehat{\\beta}_{0}) & \\text{asy. cov}(\\widehat{\\beta}\n",
    "_{0},\\widehat{\\beta}_{1}) & \\cdots & \\text{asy. cov}(\\widehat{\\beta}\n",
    "_{0},\\widehat{\\beta}_{k})\\\\\n",
    "\\text{asy. cov}(\\widehat{\\beta}_{1},\\widehat{\\beta}_{0}) & \\text{asy.\n",
    "var}(\\widehat{\\beta}_{1}) & \\cdots & \\text{asy. cov}(\\widehat{\\beta}\n",
    "_{1},\\widehat{\\beta}_{k})\\\\\n",
    "\\text{asy. cov}(\\widehat{\\beta}_{2},\\widehat{\\beta}_{0}) & \\text{asy.\n",
    "cov}(\\widehat{\\beta}_{2},\\widehat{\\beta}_{1}) & \\cdots & \\text{asy.\n",
    "cov}(\\widehat{\\beta}_{2},\\widehat{\\beta}_{k})\\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots\\\\\n",
    "\\text{asy. cov}(\\widehat{\\beta}_{k},\\widehat{\\beta}_{0}) & \\text{asy.\n",
    "cov}(\\widehat{\\beta}_{k},\\widehat{\\beta}_{1}) & \\cdots & \\text{asy.\n",
    "var}(\\widehat{\\beta}_{k})\n",
    "\\end{array}\n",
    "\\right]\n",
    "$$\n",
    "\n",
    "‚úèÔ∏è Notice that $\\mathbf{V}_{\\boldsymbol{\\beta}}$ is a $(k+1)\\times(k+1)$ [square](https://en.wikipedia.org/wiki/Square_matrix), [symmetric](https://en.wikipedia.org/wiki/Symmetric_matrix), and [positive semi-definite](https://en.wikipedia.org/wiki/Definiteness_of_a_matrix) matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ins>(Asymptotic) OLS Variance Estimation</ins>:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly notice that\n",
    "\n",
    "$$\n",
    "\\begin{aligned} n\\mathbf{V}_{\\widehat{\\boldsymbol{\\beta}}} &=n\\text{ var}(\\widehat{\\boldsymbol{\\beta}} | \\mathbf{X}) \\\\ &=\\left(n^{-1}\\mathbf{X}^{\\prime} \\mathbf{X}\\right)^{-1}\\left(n^{-1}\\mathbf{X}^{\\prime} \\mathbf{D} \\mathbf{X}\\right)\\left(n^{-1}\\mathbf{X}^{\\prime} \\mathbf{X}\\right)^{-1}, \\end{aligned}\n",
    "$$\n",
    "\n",
    "and it has been shown that\n",
    "\n",
    "$$n \\mathbf{V}_{\\widehat{\\boldsymbol{\\beta}}} \\stackrel{p}{\\longrightarrow} \\mathbf{V}_{\\boldsymbol{\\beta}}$$\n",
    "\n",
    "<p style='text-align: right;'> <a href=\"https://en.wikipedia.org/wiki/Convergence_of_random_variables#Convergence_in_probability\" style=\"color: #cc0000\">Convergence in probability</a></p>\n",
    "\n",
    "üõë Unfortunately $n\\mathbf{V}_{\\widehat{\\boldsymbol{\\beta}}}$ is _infeasible_ as matrix $\\mathbf{D}$ (defined in Assumption MLR.5) is *not known*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<span style=\"color:red\">HC0:</span>**\n",
    "\n",
    "$$\n",
    "\\widehat{\\mathbf{V}}_{\\widehat{\\boldsymbol{\\beta}}}^{\\mathrm{HCl}}=\\left(\\mathbf{X}^{\\prime} \\mathbf{X}\\right)^{-1}\\left(\\sum_{i=1}^{n} \\mathbf{x}_{i} \\widehat{e}_{i}^{2}\\mathbf{x}_{i}^{\\prime} \\right)\\left(\\mathbf{X}^{\\prime} \\mathbf{X}\\right)^{-1}\n",
    "$$\n",
    "\n",
    "**<span style=\"color:red\">HC1:</span>** (most common in *econometrics*)\n",
    "\n",
    "$$\n",
    "\\widehat{\\mathbf{V}}_{\\widehat{\\boldsymbol{\\beta}}}^{\\mathrm{HCl}}=\\left(\\frac{n}{n-k}\\right)\\left(\\mathbf{X}^{\\prime} \\mathbf{X}\\right)^{-1}\\left(\\sum_{i=1}^{n} \\mathbf{x}_{i} \\widehat{e}_{i}^{2}\\mathbf{x}_{i}^{\\prime}\\right)\\left(\\mathbf{X}^{\\prime} \\mathbf{X}\\right)^{-1}\n",
    "$$\n",
    "\n",
    "**<span style=\"color:red\">HC2:</span>**\n",
    "\n",
    "$$\n",
    "\\widehat{\\mathbf{V}}_{\\widehat{\\boldsymbol{\\beta}}}^{\\mathrm{HC2}}=\\left(\\mathbf{X}^{\\prime} \\mathbf{X}\\right)^{-1}\\left(\\sum_{i=1}^{n} \\mathbf{x}_{i} \\left(1-h_{i i}\\right)^{-1}\\widehat{e}_{i}^{2} \\mathbf{x}_{i}^{\\prime} \\right)\\left(\\mathbf{X}^{\\prime} \\mathbf{X}\\right)^{-1}\n",
    "$$\n",
    "\n",
    "**<span style=\"color:red\">HC3:</span>**\n",
    "\n",
    "$$\n",
    "\\widehat{\\mathbf{V}}_{\\widehat{\\boldsymbol{\\beta}}}^{\\mathrm{HC3}}=\\left(\\mathbf{X}^{\\prime} \\mathbf{X}\\right)^{-1}\\left(\\sum_{i=1}^{n} \\mathbf{x}_{i} \\left(1-h_{i i}\\right)^{-2}\\widehat{e}_{i}^{2} \\mathbf{x}_{i}^{\\prime} \\right)\\left(\\mathbf{X}^{\\prime} \\mathbf{X}\\right)^{-1}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
