{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Resampling Methods\n",
    "\n",
    "There are two most commonly used _resampling methods,_ _cross-validation,_ and the _bootstrap_.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext rpy2.ipython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "## removing everything from memory\n",
    "rm(list=ls())\n",
    "## turning all warnings off\n",
    "options(warn=-1)\n",
    "\n",
    "## installing the 'wooldridge' package if not previously installed\n",
    "if (!require(wooldridge)) install.packages('wooldridge')\n",
    "\n",
    "## loading the packages\n",
    "library(wooldridge)\n",
    "\n",
    "## see the raw data\n",
    "head(hprice3)\n",
    "\n",
    "## pre-processing the data set\n",
    "hprice3_processed <- subset(hprice3,select=c(\"lprice\",\"lland\",\"larea\",\"nbh\",\"rooms\",\"cbd\",\"y81\",\"ldist\",\"baths\",\"age\",\"agesq\"))\n",
    "hprice3_processed$cbd <- log(hprice3_processed$cbd)\n",
    "hprice3_processed$y81 <- as.factor(hprice3_processed$y81)\n",
    "hprice3_processed$nbh <- as.factor(hprice3_processed$nbh)\n",
    "\n",
    "head(hprice3_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import log\n",
    "import wooldridge as woo\n",
    "hprice3 = woo.dataWoo('hprice3')\n",
    "\n",
    "hprice3_processed = hprice3[['lprice','lland','larea','nbh','rooms','cbd','y81','ldist','baths','age','agesq']].copy()\n",
    "# In Python, the equal sign (“=”), creates a reference to that object.\n",
    "# Because of this, you’ll run into issues when trying to modify a copied dataframe.\n",
    "# In order to avoid this, you’ll want to use the .copy() method to create a brand new object, that isn’t just a reference to the original.\n",
    "\n",
    "hprice3_processed['cbd'] = log(hprice3_processed['cbd'])\n",
    "hprice3_processed['y81'] = hprice3_processed['y81'].astype('category')\n",
    "hprice3_processed['nbh'] = hprice3_processed['nbh'].astype('category')\n",
    "print(hprice3_processed.head(6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Cross-Validation\n",
    "\n",
    "These methods are used to do two things\n",
    "\n",
    "1. _Model Assessment_ - the process of evaluating a model's performance.\n",
    "2. _Model Selection_ - the process of selecting the proper level of flexibility for a model.\n",
    "\n",
    "Since models are 'trained' using training data sets, they are by construction suitable to fit data in these training data sets only (they were fit by minimizing in-sample fitted errors). Since the validation set was not used to fit the model, these set of observations can be used to assess the performance of the model and therefore will allow us to do model selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "## installing the 'caret' package if not previously installed\n",
    "if (!require(caret)) install.packages('caret')\n",
    "\n",
    "## loading the packages\n",
    "library(caret)\n",
    "\n",
    "## continue pre-processing\n",
    "## converting every categorical variable to numerical using dummy variables\n",
    "dmy <- dummyVars(\" ~ .\", data = hprice3_processed,fullRank = T)\n",
    "hprice3_processed <- data.frame(predict(dmy, newdata = hprice3_processed))\n",
    "\n",
    "## looking at the structure of caret package.\n",
    "str(hprice3_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "hprice3_processed = pd.get_dummies(hprice3_processed,drop_first=True).copy()\n",
    "# Dropping the First Categorical Variable with 'drop_first=True'\n",
    "\n",
    "print(hprice3_processed.head(6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The Validation Set Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![validation_set_approach.png](img/validation_set_approach.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "A schematic display of the validation set approach. A set of $n$ observations are randomly split into a training set (shown in blue, containing observations 7, 22, and 13, among others) and a validation set (shown in beige, and containing observation 91, among others). The statistical learning method is fit on the training set, and its performance is evaluated on the validation set. This can be done many (say $J$) times randomly and then all model assessment measures like $RMSE$, $\\bar{R}^2$, $C_p$, $BIC$, and $AIC$ can be calculated many (say $J$) times.\n",
    "\n",
    "<ins>*Note*</ins>: Recall that if we have a sample $\\{y_1,y_2,\\ldots,y_m\\}$ for which we have predicted $\\{\\widehat{y}_1,\\widehat{y}_2,\\ldots,\\widehat{y}_m\\}$, then the $MSE$ for these samples is defined as\n",
    "\n",
    "$$\n",
    " \\begin{aligned}\n",
    " MSE = \\frac{1}{m}\\sum_{j=1}^m (y_j-\\widehat{y}_j)^2\\text{,}\n",
    " \\end{aligned}\n",
    "$$ \n",
    " and the $RMSE$ is simply defined as $RMSE=\\sqrt{MSE}$. Also\n",
    " \n",
    "$$\n",
    " \\begin{aligned}\n",
    " MAE = \\frac{1}{m}\\sum_{j=1}^m |y_j-\\widehat{y}_j|\\text{.}\n",
    " \\end{aligned}\n",
    "$$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "## set the seed for reproducibility\n",
    "set.seed(100)\n",
    "\n",
    "## spliting processed data set into two parts based on outcome: 80% and 20%\n",
    "index <- createDataPartition(hprice3_processed$lprice, p=0.80, list=FALSE)\n",
    "trainSet <- hprice3_processed[ index,]\n",
    "validationSet <- hprice3_processed[-index,]\n",
    "\n",
    "## checking the structure of trainSet\n",
    "str(trainSet)\n",
    "\n",
    "## checking the structure of validationSet\n",
    "str(validationSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "trainSet, validationSet = train_test_split(hprice3_processed, test_size=0.2, random_state=42)\n",
    "print(trainSet.shape)\n",
    "print(validationSet.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**__Example__**: Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "## train a linear regression model\n",
    "model <- lm(lprice~., data=trainSet)\n",
    "summary(model)\n",
    "\n",
    "## making predictions\n",
    "predictions <- predict(model, validationSet)\n",
    "\n",
    "##summarize results\n",
    "postResample(pred = predictions, obs = validationSet$lprice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "import numpy as np\n",
    "\n",
    "linear_reg = LinearRegression()\n",
    "model = linear_reg.fit(trainSet.drop('lprice',axis=1)\\\n",
    "                       ,trainSet['lprice'])\n",
    "predictions = model.predict(validationSet.drop('lprice',axis=1))\n",
    "\n",
    "import sklearn.metrics as metrics\n",
    "mse = metrics.mean_squared_error(validationSet['lprice'], predictions)\n",
    "rmse = np.sqrt(mse) #mse**(0.5)  \n",
    "r2 = metrics.r2_score(validationSet['lprice'], predictions)\n",
    "mae = metrics.mean_absolute_error(validationSet['lprice'], predictions)\n",
    "\n",
    "print(\"Results of sklearn.metrics:\")\n",
    "print(\"RMSE:\", rmse)\n",
    "print(\"R-Squared:\", r2)\n",
    "print(\"MAE:\",mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Leave-One-Out Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![LOOCV.png](img/LOOCV.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "A schematic display of **LOOCV**. A set of $n$ data points is repeatedly split into a training set (shown in blue) containing all but one observation, and a validation set that contains only that observation (shown in beige). The test error is then estimated by averaging the $n$ resulting $MSE$’s. The first training set contains all but observation 1, the second training set contains all but observation 2, and so forth.\n",
    "\n",
    "Notice that a total of $n$ training data sets containing exactly $n-1$ observations have been constructed along with $n$ corresponding validation data sets containing exactly $1$ observation each.\n",
    "\n",
    "<ins>*Note*</ins>: Recall that in this case we will be using the training data set $\\{(y_1,\\mathbf{x}_1^\\prime),\\ldots,(y_{i-1},\\mathbf{x}_{i-1}^\\prime),(y_{i+1},\\mathbf{x}_{i+1}^\\prime),\\ldots,(y_n,\\mathbf{x}_n^\\prime)\\}$ to predict $y_i$ for $i=1,\\ldots,n$. Therefore the $MSE$ is defined as before for pairs $\\{(y_1,\\widehat{y}_1),\\ldots,(y_n,\\widehat{y}_n)\\}$, i.e., $MSE_i=(y_i-\\widehat{y}_i)^2$, $MSE=n^{-1}\\sum_{i=1}^{n}MSE_i$, and $RMSE=\\sqrt{MSE}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**__Example__**: Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "## define training control\n",
    "train_control <- trainControl(method=\"LOOCV\")\n",
    "\n",
    "## train the model\n",
    "model <- train(lprice~., data=hprice3_processed, trControl=train_control, method=\"glm\")\n",
    "\n",
    "## summarize results\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from numpy import mean\n",
    "from numpy import absolute\n",
    "from numpy import sqrt\n",
    "\n",
    "#define predictor and response variables\n",
    "X = hprice3_processed.drop('lprice',axis=1)\n",
    "y = hprice3_processed['lprice']\n",
    "#define cross-validation method to use\n",
    "cv = LeaveOneOut()\n",
    "\n",
    "#build multiple linear regression model\n",
    "model = LinearRegression()\n",
    "\n",
    "#use LOOCV to evaluate model\n",
    "scores = cross_validate(model, X, y, scoring={'neg_mean_absolute_error','neg_mean_squared_error'},\n",
    "                         cv=cv, n_jobs=-1)\n",
    "\n",
    "print(pd.DataFrame({'RMSE':[sqrt(mean(absolute(scores['test_neg_mean_squared_error'])))],\\\n",
    "                    'MAE':[mean(absolute(scores['test_neg_mean_absolute_error']))]}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### $k$-Fold Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![k_fold_CV.PNG](img/k_fold_CV.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "A schematic display of 5-fold CV. A set of $n$ observations is randomly split into five non-overlapping groups. Each of these fifths acts as a validation set (shown in beige), and the remainder as a training set (shown in blue). The test error is estimated by averaging the five resulting $MSE$ estimates.\n",
    "\n",
    "Notice that this approach involves randomly dividing the set of observations into $k$ groups (called __folds__), of approximately equal size. The first fold is treated as a validaton set, and the method is fit on the remaining $k−1$ folds.\n",
    "\n",
    "<ins>*Note*</ins>: For each fold $k$, notice that we can calculate the corresponding $MSE$ using the validation set for that fold. Therefore, we will have $MSE_1,\\ldots,MSE_k$ so $MSE=k^{-1}\\sum_{j=1}^{k}MSE_j$, and $RMSE=\\sqrt{MSE}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**__Example__**: Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "## define training control\n",
    "train_control <- trainControl(method=\"cv\", number=10)\n",
    "\n",
    "## set the seed for reproducibility\n",
    "set.seed(100)\n",
    "\n",
    "## train the model\n",
    "model <- train(lprice~., data=hprice3_processed, trControl=train_control, method=\"glm\")\n",
    "\n",
    "## summarize results\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# prepare the cross-validation procedure\n",
    "cv = KFold(n_splits=10, random_state=44, shuffle=True)\n",
    "\n",
    "#build multiple linear regression model\n",
    "model = LinearRegression()\n",
    "\n",
    "#use LOOCV to evaluate model\n",
    "scores = cross_validate(model, X, y, scoring={'neg_mean_absolute_error','neg_mean_squared_error'},\n",
    "                         cv=cv, n_jobs=-1)\n",
    "\n",
    "print(pd.DataFrame({'RMSE':[sqrt(mean(absolute(scores['test_neg_mean_squared_error'])))],\\\n",
    "                    'MAE':[mean(absolute(scores['test_neg_mean_absolute_error']))]}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "⚠️ If $k=n$ the method is identical to _leave-one-out cross validation_.\n",
    "\n",
    "⚠️ A disadvantage of $k$-fold cross-validation is that the results can be sensitive to the initial random sorting of the observations. Consequently some practitioners calculate the criterion $M$ times and then average the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "## define training control (Repeat the CV 5 times)\n",
    "train_control <- trainControl(method=\"repeatedcv\", number=10,repeats=5)\n",
    "\n",
    "## set the seed for reproducibility\n",
    "set.seed(100)\n",
    "\n",
    "## train the model\n",
    "model <- train(lprice~., data=hprice3_processed, trControl=train_control, method=\"glm\")\n",
    "\n",
    "## summarize results\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RepeatedKFold\n",
    "\n",
    "#Setup and configure settings for Repeated k-Fold CV (k-folds=10, repeats=5)\n",
    "rcv = RepeatedKFold(n_splits=10, n_repeats=5, random_state=42)\n",
    "\n",
    "#build multiple linear regression model\n",
    "model = LinearRegression()\n",
    "\n",
    "#use LOOCV to evaluate model\n",
    "scores = cross_validate(model, X, y, scoring={'neg_mean_absolute_error','neg_mean_squared_error'},\n",
    "                         cv=rcv, n_jobs=-1)\n",
    "\n",
    "print(pd.DataFrame({'RMSE':[sqrt(mean(absolute(scores['test_neg_mean_squared_error'])))],\\\n",
    "                    'MAE':[mean(absolute(scores['test_neg_mean_absolute_error']))]}))"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
