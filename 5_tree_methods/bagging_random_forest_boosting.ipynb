{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5936f8e3",
   "metadata": {},
   "source": [
    "<h1> Main Idea</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60711fe2",
   "metadata": {},
   "source": [
    "**Bagging**, **random forests**, and **boosting** involves producing multiple trees which are then combined to yield a single consensus prediction. We will see that combining a large number of trees can often result in dramatic improvements in prediction accuracy, at the expense of some loss in interpretation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d7ee45b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pydot\n",
    "from IPython.display import Image\n",
    "from six import StringIO  \n",
    "from sklearn.tree import export_graphviz\n",
    "\n",
    "# This function creates images of tree models using pydot\n",
    "def print_tree(estimator, features, class_names=None, filled=True):\n",
    "    tree = estimator\n",
    "    names = features\n",
    "    color = filled\n",
    "    classn = class_names\n",
    "    \n",
    "    dot_data = StringIO()\n",
    "    export_graphviz(estimator, out_file=dot_data, feature_names=features, class_names=classn, filled=filled)\n",
    "    (graph,) = pydot.graph_from_dot_data(dot_data.getvalue())\n",
    "    return(graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d85a70",
   "metadata": {},
   "source": [
    "```\n",
    "Boston.csv\n",
    "\n",
    "A data frame containing 506 observations on housing values of suburbs in Boston and the following 14 variables:\n",
    "\n",
    "crim    per capita crime rate by town.\n",
    "zn      proportion of residential land zoned for lots over 25,000 sq.ft.\n",
    "indus   proportion of non-retail business acres per town.\n",
    "chas    Charles River dummy variable (= 1 if tract bounds river; 0 otherwise).\n",
    "nox     nitrogen oxides concentration (parts per 10 million).\n",
    "rm      average number of rooms per dwelling.\n",
    "age     proportion of owner-occupied units built prior to 1940.boxcox 21\n",
    "dis     weighted mean of distances to five Boston employment centres.\n",
    "rad     index of accessibility to radial highways.\n",
    "tax     full-value property-tax rate per $10,000.\n",
    "ptratio pupil-teacher ratio by town.\n",
    "black   1000(Bk ‚àí 0.63)2 where Bk is the proportion of blacks by town.\n",
    "lstat   lower status of the population (percent).\n",
    "medv    median value of owner-occupied homes in $1000s\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4fb2036",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "boston_df = pd.read_csv('https://r-data.pmagunia.com/system/files/datasets/dataset-70319.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59194704",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = boston_df.drop('medv', axis=1)\n",
    "y = boston_df.medv\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "902318ca",
   "metadata": {},
   "source": [
    "<h2>Bagging</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "895dfebf",
   "metadata": {},
   "source": [
    "Recall that given a set of $n$ independent observations $Z_{1}, \\ldots, Z_{n}$, each with variance $\\sigma^{2}$, the variance of the mean $\\bar{Z}$ of the observations is given by $\\sigma^{2} / n .$ In other words, averaging a set of observations reduces variance. Hence a natural way to reduce the variance and hence increase the prediction accuracy of a statistical learning method is to take many training sets from the population, build a separate prediction model using each training set, and average the resulting predictions. In other words, we could calculate $\\hat{f}^{1}(x), \\hat{f}^{2}(x), \\ldots, \\hat{f}^{B}(x)$ using $B$ separate training sets, and average them in order to obtain a single low-variance statistical learning model, given by\n",
    "$$\n",
    "\\hat{f}_{\\text {avg }}(x)=\\frac{1}{B} \\sum_{b=1}^{B} \\hat{f}^{b}(x)\n",
    "$$\n",
    "Of course, this is not practical because we generally do not have access to multiple training sets. Instead, we can [**bootstrap**](https://en.wikipedia.org/wiki/Bootstrapping_(statistics)), by taking repeated samples from the (single) training data set. In this approach we generate $B$ different bootstrapped training data sets. We then train our method on the $b$th bootstrapped training set in order to get $\\hat{f}^{* b}(x)$, and finally average all the predictions, to obtain\n",
    "$$\n",
    "\\hat{f}_{\\mathrm{bag}}(x)=\\frac{1}{B} \\sum_{b=1}^{B} \\hat{f}^{* b}(x)\n",
    "$$\n",
    "This is called [**bagging**](https://en.wikipedia.org/wiki/Bootstrap_aggregating), i.e., from _**b**ootstrap **agg**regat**ing**_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8681f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "import IPython.display as display\n",
    "## Read images from file\n",
    "img1 = open('img/bootstrap.png', 'rb').read()\n",
    "img2 = open('img/bagging.png', 'rb').read()\n",
    "## Create image widgets. You can use layout of ipywidgets only with widgets.\n",
    "## Set image variable, image format and dimension.\n",
    "wi1 = widgets.Image(value=img1, format='png', width=500, height=600)\n",
    "wi2 = widgets.Image(value=img2, format='png', width=500, height=600)\n",
    "## Side by side thanks to HBox widgets\n",
    "sidebyside = widgets.HBox([wi1, wi2])\n",
    "## Finally, show.\n",
    "display.display(sidebyside)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec4bba12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "\n",
    "regbag = BaggingRegressor(base_estimator=DecisionTreeRegressor(),oob_score=True,\n",
    "                          n_estimators=100, random_state=42).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c3da26e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read images from file\n",
    "img3 = open('img/bagging_prediction.png', 'rb').read()\n",
    "## Create image widgets. You can use layout of ipywidgets only with widgets.\n",
    "## Set image variable, image format and dimension.\n",
    "wi3 = widgets.Image(value=img3, format='png', width=400, height=500)\n",
    "## Side by side thanks to HBox widgets\n",
    "pic = widgets.HBox([wi3])\n",
    "## Finally, show.\n",
    "display.display(pic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa506f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "pred = regbag.predict(X_test)\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-white')\n",
    "\n",
    "plt.scatter(pred, y_test, label='medv')\n",
    "plt.plot([0, 1], [0, 1], '--k', transform=plt.gca().transAxes)\n",
    "plt.xlabel('pred')\n",
    "plt.ylabel('y_test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d63fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "mean_squared_error(y_test, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60201260",
   "metadata": {},
   "source": [
    "<h3><em>Out-of-Bag</em> Error Estimation</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8f4bec",
   "metadata": {},
   "source": [
    "Recall that the key to bagging is that trees are repeatedly fit to bootstrapped subsets of the observations. <ins>One can show that on average, each bagged tree makes use of around two-thirds of the observations</ins>:\n",
    "<blockquote>\n",
    "If there are $n$ rows in the training data set. Then, the probability of not picking a row in a random draw is\n",
    "\n",
    "$$\n",
    "\\frac{n-1}{n}\n",
    "$$\n",
    "    \n",
    "Using _sampling-with-replacement_ the probability of not picking $n$ rows in random draws is\n",
    "\n",
    "$$\n",
    "\\left(\\frac{n-1}{n}\\right)^n\n",
    "$$\n",
    "In the limit as $n\\rightarrow\\infty$ one has\n",
    "    \n",
    "$$\n",
    "\\lim_{n\\rightarrow\\infty}\\left(1-\\frac{1}{n}\\right)^n=\\exp{(-1)}=0.368\n",
    "$$\n",
    "</blockquote>\n",
    "\n",
    "The remaining one-third of the observations not used to fit a given bagged tree are referred to as the __out-of-bag (OOB)__ observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6669db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read images from file (because this is binary, maybe you can find how to use ByteIO) but this is more easy\n",
    "img4 = open('img/OOB_evaluation.png', 'rb').read()\n",
    "## Create image widgets. You can use layout of ipywidgets only with widgets.\n",
    "## Set image variable, image format and dimension.\n",
    "wi4 = widgets.Image(value=img4, format='png', width=600, height=700)\n",
    "## Side by side thanks to HBox widgets\n",
    "pic = widgets.HBox([wi4])\n",
    "## Finally, show.\n",
    "display.display(pic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba857ac6",
   "metadata": {},
   "source": [
    "We can predict the response for the $i$th observation using each of the trees in which that observation was OOB. This will yield around $B/3$ predictions for the $i$th observation. In order to obtain a single prediction for the $i$th observation:\n",
    "\n",
    "üëâüèº Regression: We can average these predicted responses\n",
    "\n",
    "üëâüèº Classification: We can take a majority vote.\n",
    "\n",
    "This leads to a single OOB prediction for the $i$th observation.\n",
    "\n",
    "üìù An OOB prediction can be obtained in this way for each of the $n$ observations and then the `r2_score` can be calculated for the entire training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cfdefff",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"oob score is: \", regbag.oob_score_) # The oob score is an estimate of the r2 score of the ensemble regressor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae7b19d",
   "metadata": {},
   "source": [
    "<h2>Random Forest</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47fe5fba",
   "metadata": {},
   "source": [
    "Random forests provide an improvement over bagged trees by way of a small tweak that decorrelates the trees. As in bagging, we build a number of decision trees on bootstrapped training samples. But when building these decision trees, <ins>each time a split in a tree is considered, a random sample of $m$ predictors is chosen as split candidates from the full set of $p$ predictors</ins>.\n",
    "\n",
    "üëâüèº The split is allowed to use only one of those $m$ predictors.\n",
    "\n",
    "üëâüèº A fresh sample of $m$ predictors is taken at each split, and typically we choose $m \\approx \\sqrt{p}$ - that is, the number of predictors considered at each split is approximately equal to the square root of the total number of predictors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b0f009",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import sqrt as sqrt\n",
    "# There are 13 features in the dataset\n",
    "print(X.shape)\n",
    "print(sqrt(X.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "585acc1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest: using 4 features\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "regrf = RandomForestRegressor(max_features=4, random_state=42)\n",
    "regrf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "559891e7",
   "metadata": {},
   "source": [
    "üìù The main difference between bagging and random forests is the choice of predictor subset size $m$. For instance, if a random forest is built using\n",
    "$m = p$, then this amounts simply to bagging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c36aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = regrf.predict(X_test)\n",
    "\n",
    "plt.scatter(pred, y_test, label='medv')\n",
    "plt.plot([0, 1], [0, 1], '--k', transform=plt.gca().transAxes)\n",
    "plt.xlabel('pred')\n",
    "plt.ylabel('y_test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcfd90b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_squared_error(y_test, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e647d8",
   "metadata": {},
   "source": [
    "<h3>Variance Importance Measures</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d3e47e",
   "metadata": {},
   "source": [
    "Although the collection of trees is much more difficult to interpret than a single tree, one can obtain an overall summary of the importance of each predictor using the $RSS$ (for regression trees) or the <em>Gini index</em> (for classification trees). In the case of regression trees, we can record the total amount that the $RSS$ is decreased due to splits over a given predictor, averaged over all $B$ trees. A large value indicates an important predictor. Similarly, in the context of classification trees, we can add up the total amount that the <em>Gini index</em> is decreased by splits over a given predictor, averaged over all $B$ trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62315a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random forests: using 4 features\n",
    "regrf2 = RandomForestRegressor(max_features=4, random_state=42)\n",
    "regrf2.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0068dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from yellowbrick.model_selection import FeatureImportances\n",
    "viz = FeatureImportances(regrf2)\n",
    "viz.fit(X_train, y_train)\n",
    "viz.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd74db6",
   "metadata": {},
   "source": [
    "The variables with the largest _mean_ decrease in $RSS$ are `rm` and `lstat`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3116b9b",
   "metadata": {},
   "source": [
    "<h2>Boosting</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dfd467f",
   "metadata": {},
   "source": [
    "Recall that bagging involves creating multiple copies of the original training data set using the bootstrap, fitting a separate decision tree to each\n",
    "copy, and then combining all of the trees in order to create a single predictive model. Notably, each tree is built on a bootstrap data set, independent of the other trees. Boosting works in a similar way, except that the <ins>trees are grown sequentially</ins>: each tree is grown using information from previously grown trees.\n",
    "\n",
    "üîî Boosting does not involve bootstrap sampling; instead each tree is fit on a modified version of the original data set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e0b0d98",
   "metadata": {},
   "source": [
    "***\n",
    "**Algorithm**: <ins>_Boosting_ for Regression Trees</ins>\n",
    "\n",
    "1. Set $\\hat{f}(x)=0$ and $r_{i}=y_{i}$ for all $i$ in the training set.\n",
    "2. For $b=1,2, \\ldots, B$(=`n_estimators`), repeat:\\\n",
    "(a) Fit a tree $\\hat{f}^{b}$ with $d$ splits (`max_leaf_nodes=`$d+1$ terminal nodes ) to the training data $(X, r)$.\\\n",
    "(b) Update $\\hat{f}$ by adding in a shrunken version ($\\lambda$=`learning_rate`) of the new tree:\n",
    "$$\n",
    "\\hat{f}(x) \\leftarrow \\hat{f}(x)+\\lambda \\hat{f}^{b}(x)\n",
    "$$\n",
    "(c) Update the residuals,\n",
    "$$\n",
    "r_{i} \\leftarrow r_{i}-\\lambda \\hat{f}^{b}\\left(x_{i}\\right)\n",
    "$$\n",
    "3. Output the boosted model,\n",
    "$$\n",
    "\\hat{f}(x)=\\sum_{b=1}^{B} \\lambda \\hat{f}^{b}(x) .\n",
    "$$\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6654afb",
   "metadata": {},
   "source": [
    "üìù The number $d$ of splits in each tree controls the complexity of the boosted ensemble. Often $d=1$ works well, in which case each tree is a _stump_, consisting of a single split (`max_depth=1`). In this case, the boosted ensemble is fitting an additive model, since each term involves only a single variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d360657",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "regb = GradientBoostingRegressor(n_estimators=500,\n",
    "                                 learning_rate=0.01,\n",
    "                                 max_leaf_nodes=10,\n",
    "                                 random_state=42)\n",
    "regb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ebfe90d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_squared_error(y_train, regb.predict(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e7b5fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "viz = FeatureImportances(regb)\n",
    "viz.fit(X_train, y_train)\n",
    "viz.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74cd2b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_squared_error(y_test, regb.predict(X_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
