{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Support Vector Machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### What is a *Hyperplane*?\n",
    "\n",
    "In a $k$-dimensional space, a [hyperplane](https://en.wikipedia.org/wiki/Hyperplane) is a flat affine subspace of dimension $k-1$. For instance, in two dimensions, a hyperplane is a flat one-dimensional subspace - in other words, a line. In three dimensions, a hyperplane is a flat two-dimensional subspace - that is, a plane. In $k>3$ dimensions, it can be hard to visualize a hyperplane, but the notion of a $(k-1)$ -dimensional flat subspace still applies.\n",
    "\n",
    "The mathematical definition of a hyperplane is quite simple. In two dimensions, a hyperplane is defined by the equation\n",
    "\n",
    "$$\n",
    "\\beta_{0}+\\beta_{1} X_{1}+\\beta_{2} X_{2}=0\n",
    "$$\n",
    "\n",
    "for parameters $\\beta_{0}, \\beta_{1}$, and $\\beta_{2}$. When we say that the above equation \"defines\" the hyperplane, we mean that any $X=\\left(X_{1}, X_{2}\\right)^{\\prime}$ for which the equation holds is a point on the hyperplane. Note this equation is simply the equation of a line, since indeed in two dimensions a hyperplane is a line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Clearly in the multidimensional case one has\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\beta_{0}+\\beta_{1} X_{1}+\\beta_{2} X_{2}+\\ldots+\\beta_{k} X_{k}&=0\\\\\n",
    "\\mathbf{x}^{\\prime}\\boldsymbol{\\beta}&=0,\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "and this defines a $k$-dimensional hyperplane in the sense that if $X=[X_1,X_2,\\ldots,X_k]^{\\prime}\\in\\mathbb{R}^{k}$ satisfies $\\mathbf{x}^{\\prime}\\boldsymbol{\\beta}=0$, then it is said that $X$ *lies* on the hyperplane."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Now, suppose that $X$ does not satisfy $\\mathbf{x}^{\\prime}\\boldsymbol{\\beta}=0$ rather,\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\beta_{0}+\\beta_{1} X_{1}+\\beta_{2} X_{2}+\\ldots+\\beta_{k} X_{k}&>0\\\\\n",
    "\\mathbf{x}^{\\prime}\\boldsymbol{\\beta}&>0.\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Then this tells us that $X$ lies to one side of the hyperplane. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "On the other hand, if\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\beta_{0}+\\beta_{1} X_{1}+\\beta_{2} X_{2}+\\ldots+\\beta_{k} X_{k}&<0\\\\\n",
    "\\mathbf{x}^{\\prime}\\boldsymbol{\\beta}&<0.\n",
    "\\end{aligned}\n",
    "$$\n",
    "then $X$ lies on the other side of the hyperplane. So we can think of the hyperplane as dividing p-dimensional space into two halves. One can easily determine on which side of the hyperplane a point lies by simply calculating the sign of the left hand side of $\\mathbf{x}^{\\prime}\\boldsymbol{\\beta}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div>\n",
    "<img src=\"img/91.png\" width=\"700\" align=\"center\"/>\n",
    "</div>\n",
    "ðŸ‘†ðŸ¼ The hyperplane $1+2 X_{1}+3 X_{2}=0$ is shown. The blue region is\n",
    "the set of points for which $1+2 X_{1}+3 X_{2}>0$, and the purple region is the set of\n",
    "points for which $1+2 X_{1}+3 X_{2}<0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Classification using a *Separating* Hyperplane\n",
    "\n",
    "<div>\n",
    "<img src=\"img/92.png\" width=\"1000\" align=\"center\"/>\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Suppose we have observations $\\{(y_{1},x_{1,1},x_{1,2}), (y_{2},x_{2,1},x_{2,2}),\\ldots,(y_{n},x_{n1},x_{n2})\\}$ and we know that these $n$ observations fall into two classes - that is $\\{y_1,y_2,\\ldots,y_n\\}\\in\\{-1,1\\}$ where -1 represents one class (<font color='purple'>purple</font>) and 1 the other class (<font color='blue'>blue</font>). *Left*: Three (out of many) possible separating hyperplanes. *Right*: Decision rule made by a *classifier* based on this particular hyperplane (black line). If a test observation falls in the blue portion of the grid, it will be assigned to the <font color='blue'>blue</font> case, and to the <font color='purple'>purple</font> otherwise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Then a separating hyperplane for any $k$ has the property that\n",
    "\n",
    "$$\n",
    "\\beta_{0}+\\beta_{1} x_{i, 1}+\\beta_{2} x_{i, 2}+\\ldots+\\beta_{k} x_{i, k}>0 \\text { if } y_{i}=1\n",
    "$$\n",
    "and\n",
    "$$\n",
    "\\beta_{0}+\\beta_{1} x_{i, 1}+\\beta_{2} x_{i, 2}+\\ldots+\\beta_{k} x_{i, k}<0 \\text { if } y_{i}=-1\\text{.}\n",
    "$$\n",
    "Equivalently, a separating hyperplane has the property that\n",
    "$$\n",
    "y_{i}\\left(\\beta_{0}+\\beta_{1} x_{i, 1}+\\beta_{2} x_{i, 2}+\\ldots+\\beta_{k} x_{i, k}\\right)>0\n",
    "$$\n",
    "for all $i=1, \\ldots, n$ since $y_i\\in\\{-1,1\\}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<ins>Example</ins>: Imagine we are given a *test* observation $\\mathbf{x}^\\ast=\\left[x_{1}^\\ast,\\ldots,x_{k}^\\ast\\right]^\\prime$, then we 'assign' it to a class based on the sign of $\\mathbf{x}^{\\ast\\prime}\\boldsymbol{\\beta}=:f(\\mathbf{x}^\\ast)$, i.e., if $f(\\mathbf{x}^\\ast)>0$ then we assign this test observation to class 1, and if $f(\\mathbf{x}^\\ast)<0$ then we assign it to class -1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The **magnitude** of $f(\\mathbf{x}^\\ast)$ is also useful, since $f(\\mathbf{x}^\\ast)$ being far from zero makes us confident about its classification, but when $f(\\mathbf{x}^\\ast)$ is close to zero, then $\\mathbf{x}^\\ast$ is located near the hyperplane, and so we are less confident about the class assignment for it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### The Maximal Margin Classifier\n",
    "\n",
    "<div class=\"foo\">\n",
    "    \n",
    "&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;| &nbsp; \n",
    "------ | -----\n",
    "<div><img src=\"img/93.png\" width=\"5500\" align=\"center\"/></div> | We can compute the (perpendicular) distance from each training observation to a given separating hyperplane; the smallest such distance is the minimal distance from the observations to the hyperplane, and is known as the *margin*. The maximal margin hyperplane is the separating hyperplane for which the margin is largestâ€”that is, it is the hyperplane that has the farthest minimum distance to the training observations. We can then classify a test observation based on which side of the maximal margin hyperplane it lies. This is known as the **maximal margin classifier**. <ins>Example</ins>: we see that three training observations are equidistant from the maximal margin hyperplane and lie along the dashed lines indicating the width of the margin. These three observations are known as **support vectors**, since they are vectors in $k$-dimensional space ($k = 2$), and they \"support\" the maximal margin hyperplane in the sense vector that if these points were moved slightly then the maximal margin hyperplane would move as well.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "ðŸ’» This is a simulated data set of 10,000 observations and the objective is to build a machine that can predict loan ```default``` ('no' or 'yes') based on the ```balance``` and ```income``` of the customers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "## installing the 'ISLR' package if not previously installed\n",
    "if (!require(ISLR)) install.packages('ISLR')\n",
    "\n",
    "data(Default)\n",
    "\n",
    "##  Obs:   10,000\n",
    "\n",
    "## 1. default     A factor with levels No and Yes indicating whether the customer defaulted on their debt\n",
    "## 2. student     A factor with levels No and Yes indicating whether the customer is a student\n",
    "## 3. balance     The average balance that the customer has remaining on their credit card after making their \n",
    "##                monthly payment\n",
    "## 4. income      Income of customer\n",
    "\n",
    "Default.copy <- Default[,-2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "âš ï¸ <ins>The separating hyperplane might not exist</ins>, and so there is no maximal margin classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "## installing the 'lattice' package if not previously installed\n",
    "if (!require(lattice)) install.packages('lattice')\n",
    "\n",
    "xyplot(scale(balance) ~ scale(income), group=default, data=Default.copy, \n",
    "       auto.key=list(space=\"right\"), \n",
    "       jitter.x=TRUE, jitter.y=TRUE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "## installing the 'tidyverse' package if not previously installed\n",
    "if (!require(tidyverse)) install.packages('tidyverse')\n",
    "\n",
    "## installing the 'caret' package if not previously installed\n",
    "if (!require(caret)) install.packages('caret')\n",
    "\n",
    "## split the data into training and test set\n",
    "set.seed(2020)\n",
    "training.samples <- Default.copy$default %>% \n",
    "  createDataPartition(p = 0.75, list = FALSE)\n",
    "train.data  <- Default.copy[training.samples, ]\n",
    "test.data <- Default.copy[-training.samples, ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Support Vector Classifier\n",
    "\n",
    "The support vector classifier classifies a test observation depending on which side of a hyperplane it lies. The hyperplane is chosen to correctly separate most of the training observations into the two classes, but <ins>may misclassify a few observations</ins>. It is the solution to the optimization problem\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "& \\underset{\\beta_{0}, \\beta_{1}, \\ldots, \\beta_{k}, \\epsilon_{1}, \\ldots, \\epsilon_{n}}{\\operatorname{maximimize}} M\\\\\n",
    "& {\\text { subject to } \\sum_{j=1}^{k} \\beta_{j}^{2}=1} \\\\\n",
    "& {y_{i}\\left(\\beta_{0}+\\beta_{1} x_{i,1}+\\beta_{2} x_{i,2}+\\ldots+\\beta_{k} x_{i,k}\\right) \\geq M\\left(1-\\epsilon_{i}\\right)} \\\\\n",
    "& {\\quad \\epsilon_{i} \\geq 0, \\quad \\sum_{i=1}^{n} \\epsilon_{i} \\leq C}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "where $C$ is a nonnegative tuning parameter, $M$ is the width of the margin (which we want to make as large as possible). The $\\epsilon_{1}, \\ldots, \\epsilon_{n}$ are *slack variables* that allow observations to be on the wrong side of the margin."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "1. The slack variable $\\epsilon_{i}$ tells us where the $i$th observation is located relative to the hyperplane and the margin, i.e.,\n",
    " 1. $\\epsilon_{i}=0$ - the $i$th observation is on the correct side of the margin.\n",
    " 2. $\\epsilon_{i}>0$ - the $i$th observation is on the wrong side of the margin.\n",
    " 3. $\\epsilon_{i}>1$ - the $i$th observation is on the wrong side of the hyperplane.\n",
    "2. The tuning parameter $C$  bounds the sum of the $\\epsilon_{i}$'s, and can be considered a *tolerance* parameter.\n",
    " 1. If $C=0$ there we are not allowing for violations so it must be the case that $\\epsilon_{1}=\\ldots=\\epsilon_{n}=0$ in which case we have the maximal margin classifier (if it exists).\n",
    " 2. For $C>0$ no more than $C$ observations can be on the wrong side of the hyperplane because in this case $\\epsilon_{i}>1$ and we have that $\\sum_{i=1}^{n} \\epsilon_{i} \\leq C$. As $C$ increases, we become more tolerant of violations to the margin, and so the margin will widen. Conversely, as $C$ decreases, we become less tolerant of violations to the margin and so the margin narrows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "set.seed(2020)\n",
    "\n",
    "model <- train(default~.,data=train.data,method=\"svmLinear\",\n",
    "              trControl=trainControl(\"cv\",number=10),\n",
    "              preProcess=c(\"center\",\"scale\")\n",
    "              )\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "predicted.classes <- model %>% predict(test.data)\n",
    "\n",
    "## assessing model accuracy\n",
    "confusionMatrix(data = test.data$default,\n",
    "                reference = predicted.classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Support Vector Machine\n",
    "\n",
    "The support vector classifier is a natural approach for classification in the two-class setting, if the boundary between the two classes is linear. However, in practice we are sometimes faced with non-linear class boundaries, i.e.,\n",
    "\n",
    "<div class=\"foo\">\n",
    "    \n",
    "&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;| &nbsp; \n",
    "------ | -----\n",
    "<div><img src=\"img/98.png\" width=\"5500\" align=\"center\"/></div> | Left: The observations fall into two classes, with a non-linear boundary between them. Right: The support vector classifier seeks a linear boundary, and consequently performs very poorly.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "It turns out that the solution to the support vector classifier problem involves only the [*inner products*](https://en.wikipedia.org/wiki/Inner_product_space) between the point $\\mathbf{x}$ and the support vectors. So if $\\mathcal{S}$ is the collection of indices of these support points, we can rewrite any solution function as\n",
    "\n",
    "$$\n",
    "f(\\mathbf{x})=\\beta_{0}+\\sum_{i \\in \\mathcal{S}} \\alpha_{i}\\left\\langle \\mathbf{x}, \\mathbf{x}_{i}\\right\\rangle,\n",
    "$$\n",
    "\n",
    "where $\\langle \\mathbf{a}, \\mathbf{b}\\rangle=\\sum_{j=1}^{r} a_{j} b_{j}$. Therefore we can **generalize** this solution to \n",
    "\n",
    "$$\n",
    "f(\\mathbf{x})=\\beta_{0}+\\sum_{i \\in \\mathcal{S}} \\alpha_{i} K\\left(\\mathbf{x}, \\mathbf{x}_{i}\\right),\n",
    "$$\n",
    "\n",
    "where $K$ is called a *kernel*.\n",
    "\n",
    "<div class=\"foo\">\n",
    "    \n",
    "&nbsp; &nbsp; &nbsp; | &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;  &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;  &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;  &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;  &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;  &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;  &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;  &nbsp; &nbsp; &nbsp;| &nbsp; &nbsp; &nbsp; \n",
    " ------ | ------ | ------ \n",
    "  Name | Kernel | Parameter \n",
    "  Linear | $K\\left(\\mathbf{x}_{i}, \\mathbf{x}_{i^{\\prime}}\\right)=\\sum_{j=1}^{k} \\mathbf{x}_{i j} \\mathbf{x}_{i^{\\prime} j}$ | ```C```\n",
    " Polynomial | $K\\left(\\mathbf{x}_{i}, \\mathbf{x}_{i^{\\prime}}\\right)=\\left(1+\\sum_{j=1}^{k} x_{i j} x_{i^{\\prime} j}\\right)^{d}$ | $d$ (```degree```) and ```C```\n",
    " Radial | $K\\left(\\mathbf{x}_{i}, \\mathbf{x}_{i^{\\prime}}\\right)=\\exp \\left(-\\sigma \\sum_{j=1}^{k}\\left(x_{i j}-x_{i^{\\prime} j}\\right)^{2}\\right)$ | $\\sigma$ (```sigma```) and ```C```\n",
    " \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "set.seed(2020)\n",
    "model <- train(default~.,data=train.data,method=\"svmPoly\",\n",
    "              trControl=trainControl(\"cv\",number=10),\n",
    "              preProcess=c(\"center\",\"scale\"),\n",
    "              tuneLength=4\n",
    "              )\n",
    "model$bestTune\n",
    "predicted.classes <- model %>% predict(test.data)\n",
    "## assessing model accuracy\n",
    "confusionMatrix(data = test.data$default,\n",
    "                reference = predicted.classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "set.seed(24)\n",
    "model <- train(default~.,data=train.data,method=\"svmRadial\",\n",
    "              trControl=trainControl(\"cv\",number=10),\n",
    "              preProcess=c(\"center\",\"scale\"),\n",
    "              tuneLength=10\n",
    "              )\n",
    "model$bestTune\n",
    "predicted.classes <- model %>% predict(test.data)\n",
    "\n",
    "## assessing model accuracy\n",
    "confusionMatrix(data = test.data$default,\n",
    "                reference = predicted.classes)"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.6.1"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
