{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Support Vector Machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### What is a *Hyperplane*?\n",
    "\n",
    "In a $k$-dimensional space, a [hyperplane](https://en.wikipedia.org/wiki/Hyperplane) is a flat affine subspace of dimension $k-1$. For instance, in two dimensions, a hyperplane is a flat one-dimensional subspace - in other words, a line. In three dimensions, a hyperplane is a flat two-dimensional subspace - that is, a plane. In $k>3$ dimensions, it can be hard to visualize a hyperplane, but the notion of a $(k-1)$ -dimensional flat subspace still applies.\n",
    "\n",
    "The mathematical definition of a hyperplane is quite simple. In two dimensions, a hyperplane is defined by the equation\n",
    "\n",
    "$$\n",
    "\\beta_{0}+\\beta_{1} X_{1}+\\beta_{2} X_{2}=0\n",
    "$$\n",
    "\n",
    "for parameters $\\beta_{0}, \\beta_{1}$, and $\\beta_{2}$. When we say that the above equation \"defines\" the hyperplane, we mean that any $X=\\left(X_{1}, X_{2}\\right)^{\\prime}$ for which the equation holds is a point on the hyperplane. Note this equation is simply the equation of a line, since indeed in two dimensions a hyperplane is a line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Clearly in the multidimensional case one has\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\beta_{0}+\\beta_{1} X_{1}+\\beta_{2} X_{2}+\\ldots+\\beta_{k} X_{k}&=0\\\\\n",
    "\\mathbf{x}^{\\prime}\\boldsymbol{\\beta}&=0,\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "and this defines a $k$-dimensional hyperplane in the sense that if $X=[X_1,X_2,\\ldots,X_k]^{\\prime}\\in\\mathbb{R}^{k}$ satisfies $\\mathbf{x}^{\\prime}\\boldsymbol{\\beta}=0$, then it is said that $X$ *lies* on the hyperplane."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Now, suppose that $X$ does not satisfy $\\mathbf{x}^{\\prime}\\boldsymbol{\\beta}=0$ rather,\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\beta_{0}+\\beta_{1} X_{1}+\\beta_{2} X_{2}+\\ldots+\\beta_{k} X_{k}&>0\\\\\n",
    "\\mathbf{x}^{\\prime}\\boldsymbol{\\beta}&>0.\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Then this tells us that $X$ lies to one side of the hyperplane. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "On the other hand, if\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\beta_{0}+\\beta_{1} X_{1}+\\beta_{2} X_{2}+\\ldots+\\beta_{k} X_{k}&<0\\\\\n",
    "\\mathbf{x}^{\\prime}\\boldsymbol{\\beta}&<0.\n",
    "\\end{aligned}\n",
    "$$\n",
    "then $X$ lies on the other side of the hyperplane. So we can think of the hyperplane as dividing p-dimensional space into two halves. One can easily determine on which side of the hyperplane a point lies by simply calculating the sign of the left hand side of $\\mathbf{x}^{\\prime}\\boldsymbol{\\beta}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div>\n",
    "<img src=\"img/91.png\" width=\"700\" align=\"center\"/>\n",
    "</div>\n",
    "👆🏼 The hyperplane $1+2 X_{1}+3 X_{2}=0$ is shown. The blue region is\n",
    "the set of points for which $1+2 X_{1}+3 X_{2}>0$, and the purple region is the set of\n",
    "points for which $1+2 X_{1}+3 X_{2}<0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Classification using a *Separating* Hyperplane\n",
    "\n",
    "<div>\n",
    "<img src=\"img/92.png\" width=\"1000\" align=\"center\"/>\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Suppose we have observations $\\{(y_{1},x_{1,1},x_{1,2}), (y_{2},x_{2,1},x_{2,2}),\\ldots,(y_{n},x_{n1},x_{n2})\\}$ and we know that these $n$ observations fall into two classes - that is $\\{y_1,y_2,\\ldots,y_n\\}\\in\\{-1,1\\}$ where -1 represents one class (<font color='purple'>purple</font>) and 1 the other class (<font color='blue'>blue</font>). *Left*: Three (out of many) possible separating hyperplanes. *Right*: Decision rule made by a *classifier* based on this particular hyperplane (black line). If a test observation falls in the blue portion of the grid, it will be assigned to the <font color='blue'>blue</font> case, and to the <font color='purple'>purple</font> otherwise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Then a separating hyperplane for any $k$ has the property that\n",
    "\n",
    "$$\n",
    "\\beta_{0}+\\beta_{1} x_{i, 1}+\\beta_{2} x_{i, 2}+\\ldots+\\beta_{k} x_{i, k}>0 \\text { if } y_{i}=1\n",
    "$$\n",
    "and\n",
    "$$\n",
    "\\beta_{0}+\\beta_{1} x_{i, 1}+\\beta_{2} x_{i, 2}+\\ldots+\\beta_{k} x_{i, k}<0 \\text { if } y_{i}=-1\\text{.}\n",
    "$$\n",
    "Equivalently, a separating hyperplane has the property that\n",
    "$$\n",
    "y_{i}\\left(\\beta_{0}+\\beta_{1} x_{i, 1}+\\beta_{2} x_{i, 2}+\\ldots+\\beta_{k} x_{i, k}\\right)>0\n",
    "$$\n",
    "for all $i=1, \\ldots, n$ since $y_i\\in\\{-1,1\\}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<ins>Example</ins>: Imagine we are given a *test* observation $\\mathbf{x}^\\ast=\\left[x_{1}^\\ast,\\ldots,x_{k}^\\ast\\right]^\\prime$, then we 'assign' it to a class based on the sign of $\\mathbf{x}^{\\ast\\prime}\\boldsymbol{\\beta}=:f(\\mathbf{x}^\\ast)$, i.e., if $f(\\mathbf{x}^\\ast)>0$ then we assign this test observation to class 1, and if $f(\\mathbf{x}^\\ast)<0$ then we assign it to class -1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The **magnitude** of $f(\\mathbf{x}^\\ast)$ is also useful, since $f(\\mathbf{x}^\\ast)$ being far from zero makes us confident about its classification, but when $f(\\mathbf{x}^\\ast)$ is close to zero, then $\\mathbf{x}^\\ast$ is located near the hyperplane, and so we are less confident about the class assignment for it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### The Maximal Margin Classifier\n",
    "\n",
    "<div class=\"foo\">\n",
    "    \n",
    "&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;| &nbsp; \n",
    "------ | -----\n",
    "<div><img src=\"img/93.png\" width=\"5500\" align=\"center\"/></div> | We can compute the (perpendicular) distance from each training observation to a given separating hyperplane; the smallest such distance is the minimal distance from the observations to the hyperplane, and is known as the *margin*. The maximal margin hyperplane is the separating hyperplane for which the margin is largest—that is, it is the hyperplane that has the farthest minimum distance to the training observations. We can then classify a test observation based on which side of the maximal margin hyperplane it lies. This is known as the **maximal margin classifier**. <ins>Example</ins>: we see that three training observations are equidistant from the maximal margin hyperplane and lie along the dashed lines indicating the width of the margin. These three observations are known as **support vectors**, since they are vectors in $k$-dimensional space ($k = 2$), and they \"support\" the maximal margin hyperplane in the sense vector that if these points were moved slightly then the maximal margin hyperplane would move as well.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "💻 This is a simulated data set of 10,000 observations and the objective is to build a machine that can predict loan ```default``` ('no' or 'yes') based on the ```balance``` and ```income``` of the customers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "## installing the 'ISLR' package if not previously installed\n",
    "if (!require(ISLR)) install.packages('ISLR')\n",
    "\n",
    "data(Default)\n",
    "\n",
    "##  Obs:   10,000\n",
    "\n",
    "## 1. default     A factor with levels No and Yes indicating whether the customer defaulted on their debt\n",
    "## 2. student     A factor with levels No and Yes indicating whether the customer is a student\n",
    "## 3. balance     The average balance that the customer has remaining on their credit card after making their \n",
    "##                monthly payment\n",
    "## 4. income      Income of customer\n",
    "\n",
    "Default.copy <- Default[,-2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "⚠️ <ins>The separating hyperplane might not exist</ins>, and so there is no maximal margin classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "## installing the 'lattice' package if not previously installed\n",
    "if (!require(lattice)) install.packages('lattice')\n",
    "\n",
    "xyplot(scale(balance) ~ scale(income), group=default, data=Default.copy, \n",
    "       auto.key=list(space=\"right\"), \n",
    "       jitter.x=TRUE, jitter.y=TRUE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "## installing the 'tidyverse' package if not previously installed\n",
    "if (!require(tidyverse)) install.packages('tidyverse')\n",
    "\n",
    "## installing the 'caret' package if not previously installed\n",
    "if (!require(caret)) install.packages('caret')\n",
    "\n",
    "## split the data into training and test set\n",
    "set.seed(2020)\n",
    "training.samples <- Default.copy$default %>% \n",
    "  createDataPartition(p = 0.75, list = FALSE)\n",
    "train.data  <- Default.copy[training.samples, ]\n",
    "test.data <- Default.copy[-training.samples, ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Support Vector Classifier\n",
    "\n",
    "The support vector classifier classifies a test observation depending on which side of a hyperplane it lies. The hyperplane is chosen to correctly separate most of the training observations into the two classes, but <ins>may misclassify a few observations</ins>. It is the solution to the optimization problem\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "& \\underset{\\beta_{0}, \\beta_{1}, \\ldots, \\beta_{k}, \\epsilon_{1}, \\ldots, \\epsilon_{n}}{\\operatorname{maximimize}} M\\\\\n",
    "& {\\text { subject to } \\sum_{j=1}^{k} \\beta_{j}^{2}=1} \\\\\n",
    "& {y_{i}\\left(\\beta_{0}+\\beta_{1} x_{i,1}+\\beta_{2} x_{i,2}+\\ldots+\\beta_{k} x_{i,k}\\right) \\geq M\\left(1-\\epsilon_{i}\\right)} \\\\\n",
    "& {\\quad \\epsilon_{i} \\geq 0, \\quad \\sum_{i=1}^{n} \\epsilon_{i} \\leq C}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "where $C$ is a nonnegative tuning parameter, $M$ is the width of the margin (which we want to make as large as possible). The $\\epsilon_{1}, \\ldots, \\epsilon_{n}$ are *slack variables* that allow observations to be on the wrong side of the margin."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "1. The slack variable $\\epsilon_{i}$ tells us where the $i$th observation is located relative to the hyperplane and the margin, i.e.,\n",
    " 1. $\\epsilon_{i}=0$ - the $i$th observation is on the correct side of the margin.\n",
    " 2. $\\epsilon_{i}>0$ - the $i$th observation is on the wrong side of the margin.\n",
    " 3. $\\epsilon_{i}>1$ - the $i$th observation is on the wrong side of the hyperplane.\n",
    "2. The tuning parameter $C$  bounds the sum of the $\\epsilon_{i}$'s, and can be considered a *tolerance* parameter.\n",
    " 1. If $C=0$ there we are not allowing for violations so it must be the case that $\\epsilon_{1}=\\ldots=\\epsilon_{n}=0$ in which case we have the maximal margin classifier (if it exists).\n",
    " 2. For $C>0$ no more than $C$ observations can be on the wrong side of the hyperplane because in this case $\\epsilon_{i}>1$ and we have that $\\sum_{i=1}^{n} \\epsilon_{i} \\leq C$. As $C$ increases, we become more tolerant of violations to the margin, and so the margin will widen. Conversely, as $C$ decreases, we become less tolerant of violations to the margin and so the margin narrows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "set.seed(2020)\n",
    "\n",
    "model <- train(default~.,data=train.data,method=\"svmLinear\",\n",
    "              trControl=trainControl(\"cv\",number=10),\n",
    "              preProcess=c(\"center\",\"scale\")\n",
    "              )\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "predicted.classes <- model %>% predict(test.data)\n",
    "\n",
    "## assessing model accuracy\n",
    "confusionMatrix(data = test.data$default,\n",
    "                reference = predicted.classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Support Vector Machine\n",
    "\n",
    "The support vector classifier is a natural approach for classification in the two-class setting, if the boundary between the two classes is linear. However, in practice we are sometimes faced with non-linear class boundaries, i.e.,\n",
    "\n",
    "<div class=\"foo\">\n",
    "    \n",
    "&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;| &nbsp; \n",
    "------ | -----\n",
    "<div><img src=\"img/98.png\" width=\"5500\" align=\"center\"/></div> | Left: The observations fall into two classes, with a non-linear boundary between them. Right: The support vector classifier seeks a linear boundary, and consequently performs very poorly.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "It turns out that the solution to the support vector classifier problem involves only the [*inner products*](https://en.wikipedia.org/wiki/Inner_product_space) between the point $\\mathbf{x}$ and the support vectors. So if $\\mathcal{S}$ is the collection of indices of these support points, we can rewrite any solution function as\n",
    "\n",
    "$$\n",
    "f(\\mathbf{x})=\\beta_{0}+\\sum_{i \\in \\mathcal{S}} \\alpha_{i}\\left\\langle \\mathbf{x}, \\mathbf{x}_{i}\\right\\rangle,\n",
    "$$\n",
    "\n",
    "where $\\langle \\mathbf{a}, \\mathbf{b}\\rangle=\\sum_{j=1}^{r} a_{j} b_{j}$. Therefore we can **generalize** this solution to \n",
    "\n",
    "$$\n",
    "f(\\mathbf{x})=\\beta_{0}+\\sum_{i \\in \\mathcal{S}} \\alpha_{i} K\\left(\\mathbf{x}, \\mathbf{x}_{i}\\right),\n",
    "$$\n",
    "\n",
    "where $K$ is called a *kernel*.\n",
    "\n",
    "<div class=\"foo\">\n",
    "    \n",
    "&nbsp; &nbsp; &nbsp; | &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;  &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;  &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;  &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;  &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;  &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;  &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;  &nbsp; &nbsp; &nbsp;| &nbsp; &nbsp; &nbsp; \n",
    " ------ | ------ | ------ \n",
    "  Name | Kernel | Parameter \n",
    "  Linear | $K\\left(\\mathbf{x}_{i}, \\mathbf{x}_{i^{\\prime}}\\right)=\\sum_{j=1}^{k} \\mathbf{x}_{i j} \\mathbf{x}_{i^{\\prime} j}$ | ```C```\n",
    " Polynomial | $K\\left(\\mathbf{x}_{i}, \\mathbf{x}_{i^{\\prime}}\\right)=\\left(1+\\sum_{j=1}^{k} x_{i j} x_{i^{\\prime} j}\\right)^{d}$ | $d$ (```degree```) and ```C```\n",
    " Radial | $K\\left(\\mathbf{x}_{i}, \\mathbf{x}_{i^{\\prime}}\\right)=\\exp \\left(-\\sigma \\sum_{j=1}^{k}\\left(x_{i j}-x_{i^{\\prime} j}\\right)^{2}\\right)$ | $\\sigma$ (```sigma```) and ```C```\n",
    " \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "set.seed(2020)\n",
    "model <- train(default~.,data=train.data,method=\"svmPoly\",\n",
    "              trControl=trainControl(\"cv\",number=10),\n",
    "              preProcess=c(\"center\",\"scale\"),\n",
    "              tuneLength=4\n",
    "              )\n",
    "model$bestTune\n",
    "predicted.classes <- model %>% predict(test.data)\n",
    "## assessing model accuracy\n",
    "confusionMatrix(data = test.data$default,\n",
    "                reference = predicted.classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "set.seed(24)\n",
    "model <- train(default~.,data=train.data,method=\"svmRadial\",\n",
    "              trControl=trainControl(\"cv\",number=10),\n",
    "              preProcess=c(\"center\",\"scale\"),\n",
    "              tuneLength=10\n",
    "              )\n",
    "model$bestTune\n",
    "predicted.classes <- model %>% predict(test.data)\n",
    "\n",
    "## assessing model accuracy\n",
    "confusionMatrix(data = test.data$default,\n",
    "                reference = predicted.classes)"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.6.1"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
