{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Model Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Selection Criteria: MLR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We first list selection criteria for the linear regression model $y_{i}=x_{i}^{\\prime} \\boldsymbol{\\beta}+e_{i}$ with $\\sigma^{2}=E\\left(e_{i}^{2}\\right)$ and a $(k+1)\\times 1$ coefficient vector $\\boldsymbol{\\beta}$. Let $\\widehat{\\boldsymbol{\\beta}}$ be the OLS estimator, $\\widehat{e}_{i}$ the OLS residual for the $i$ observation, and $\\widehat{\\sigma}^{2}=n^{-1} \\sum_{i=1}^{n} \\widehat{e}_{i}^{2}$ be the variance estimator. The number of estimated parameters ( $\\boldsymbol{\\beta}$ and $\\sigma^{2}$ ) is $K=k+2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "## for easy data manipulation and visualization\n",
    "if (!require(tidyverse)) install.packages('tidyverse')\n",
    "library(tidyverse)\n",
    "\n",
    "## provides helper functions for computing regression model performance metrics\n",
    "if (!require(modelr)) install.packages('modelr')\n",
    "library(modelr)\n",
    "\n",
    "## creates easily a tidy data frame containing the model statistical metrics\n",
    "if (!require(broom)) install.packages('broom')\n",
    "library(broom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "## installing the 'wooldridge' package if not previously installed\n",
    "if (!require(wooldridge)) install.packages('wooldridge')\n",
    "\n",
    "data(hprice3)\n",
    "\n",
    "## Obs:   321\n",
    "\n",
    "##  1. year                     1978, 1981\n",
    "##  2. age                      age of house\n",
    "##  3. agesq                    age^2\n",
    "##  4. nbh                      neighborhood, 1 to 6\n",
    "##  5. cbd                      dist. to central bus. dstrct, feet\n",
    "##  6. inst                     dist. to interstate, feet\n",
    "##  7. linst                    log(inst)\n",
    "##  8. price                    selling price\n",
    "##  9. rooms                    # rooms in house\n",
    "## 10. area                     square footage of house\n",
    "## 11. land                     square footage lot\n",
    "## 12. baths                    # bathrooms\n",
    "## 13. dist                     dist. from house to incin., feet\n",
    "## 14. ldist                    log(dist)\n",
    "## 15. lprice                   log(price)\n",
    "## 16. y81                      =1 if year = 1981\n",
    "## 17. larea                    log(area)\n",
    "## 18. lland                    log(land)\n",
    "## 19. linstsq                  linst^2\n",
    "\n",
    "hprice3.copy <- hprice3\n",
    "hprice3.copy$lcbd <- log(hprice3.copy$cbd)\n",
    "hprice3.copy$y81 <- as.factor(hprice3.copy$y81)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "## for easy data manipulation and visualization\n",
    "if (!require(caret)) install.packages('caret')\n",
    "library(caret)\n",
    "\n",
    "## creating dummy variable for y81==1 and dropping dummy y81==0\n",
    "dmy <- dummyVars(\" ~ y81\", data = hprice3.copy, fullRank=T)\n",
    "datos <- subset(hprice3.copy, select=c(\"lprice\",\"lland\",\"larea\",\"lcbd\",\"nbh\",\"rooms\",\n",
    "                                       \"linst\",\"linstsq\",\"ldist\",\"baths\",\"age\",\"agesq\"))\n",
    "datos <- cbind(datos,data.frame(predict(dmy, newdata = hprice3.copy)))\n",
    "head(datos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "üíª The following code estimates two linear regression models ```model1``` uses all the predictors while ```model2``` excludes ```linstsq``` and ```agesq```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "model1 <- lm(lprice ~., data = datos)\n",
    "model2 <- lm(lprice ~. -linstsq-agesq, data = datos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "***\n",
    "**_Adjusted $\\bar{R}^2$_**\n",
    "$$\n",
    "\\bar{R}^{2}=1-\\left(1-R^{2}\\right) \\frac{n-1}{n-K-1},\n",
    "$$\n",
    "where $R^2$ is the standard regression coefficient of determination.\n",
    "\n",
    "**_Bayesian Information Criterion_**\n",
    "$$\n",
    "\\mathrm{BIC}=n+n \\log \\left(2 \\pi \\widehat{\\sigma}^{2}\\right)+K \\log (n).\n",
    "$$\n",
    "**_Akaike Information Criterion_**\n",
    "$$\n",
    "\\mathrm{AIC}=n+n \\log \\left(2 \\pi \\widehat{\\sigma}^{2}\\right)+2 K.\n",
    "$$\n",
    "***\n",
    "\n",
    "<ins>Note</ins>: As it is often done in the _statistical learning literature_ we will use $\\mathrm{BIC}$ and $\\mathrm{AIC}$ defined without the additive constants $n+n\\log(2\\pi)$.\n",
    "\n",
    "Therefore, we can rewrite them both as a member of a *class* of *Information Criteria*, $\\mathrm{IC}$,\n",
    "\n",
    "$$\n",
    "\\mathrm{IC}=n\\log(\\widehat{\\sigma}^{2})+c(n,K).\n",
    "$$\n",
    "\n",
    "Clearly one has the $\\mathrm{AIC}$ when $c=2K$ and the $\\mathrm{BIC}$ when $c=K\\log(n)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "üíª Here, we use the function ```glance()``` to simply compare the overall quality of our two models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "## metrics for model 1\n",
    "glance(model1) %>%\n",
    "  dplyr::select(adj.r.squared, BIC, AIC)\n",
    "\n",
    "## metrics for model 2\n",
    "glance(model2) %>%\n",
    "  dplyr::select(adj.r.squared, BIC, AIC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "üíª The first model has higher $\\bar{R}^2$ and lower value for $\\mathrm{BIC}$ and $\\mathrm{AIC}$, so we would choose ```model1``` over ```model2```."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "***\n",
    "**_Mallows'_** **$C_p$**\n",
    "$$\n",
    "C_{p}=n \\widehat{\\sigma}^{2}+2 K \\widetilde{\\sigma}^{2},\n",
    "$$\n",
    "where $\\widetilde{\\sigma}^{2}$ is a preliminary estimator of $\\sigma^{2}$ (typically based on fitting a large model, i.e., the one containing all the predictors).\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "## recovering sigma for models 1 and 2\n",
    "sigma1 <- glance(model1) %>%\n",
    "             dplyr::select(sigma)\n",
    "sigma2 <- glance(model2) %>%\n",
    "             dplyr::select(sigma)\n",
    "\n",
    "## calculating Mallows' Cp for models 1 and 2\n",
    "Cp1 <- nobs(model1)*(sigma1^2) + 2*(model1$rank)*(sigma1^2)\n",
    "Cp2 <- nobs(model2)*(sigma2^2) + 2*(model2$rank)*(sigma1^2)\n",
    "\n",
    "cbind(Cp1,Cp2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "üíª Again, the ```model1``` has smaller $C_p$ than ```model2```, so ```model1``` is preferred."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "***\n",
    "**_Shibata_**\n",
    "$$\n",
    "\\text{Shibata}=\\widehat{\\sigma}^{2}\\left(1+\\frac{2 K}{n}\\right).\n",
    "$$\n",
    "\n",
    "**_Final Prediction Error_**\n",
    "$$\n",
    "\\mathrm{FPE}=\\widehat{\\sigma}^{2}\\left(\\frac{1+K / n}{1-K / n}\\right).\n",
    "$$\n",
    "\n",
    "**_Generalized Cross-Validation_**\n",
    "$$\n",
    "\\mathrm{GCV}=\\frac{n \\widehat{\\sigma}^{2}}{(n-K)^{2}}.\n",
    "$$\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "## manually calculating Shibata, FPE, and GCV for model1\n",
    "shibata1 <- (sigma1^2)*(1+2*model1$rank/nobs(model1))\n",
    "FPE1 <- (sigma1^2)*(1+model1$rank/nobs(model1))/(1-model1$rank/nobs(model1))\n",
    "GCV1 <- nobs(model1)*(sigma1^2)/(nobs(model1)-model1$rank)\n",
    "\n",
    "## manually calculating Shibata, FPE, and GCV for model2\n",
    "shibata2 <- (sigma2^2)*(1+2*model2$rank/nobs(model2))\n",
    "FPE2 <- (sigma1^2)*(1+model2$rank/nobs(model2))/(1-model2$rank/nobs(model2))\n",
    "GCV2 <- nobs(model2)*(sigma2^2)/(nobs(model2)-model2$rank)\n",
    "\n",
    "data.frame(shibata=unname(rbind(shibata1,shibata2)),\n",
    "           FPE=unname(rbind(FPE1,FPE2)),\n",
    "           GCV=unname(rbind(GCV1,GCV2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "üíª Again, the ```model1``` has smaller $\\mathrm{Shibata}$ and $\\mathrm{GCV}$ than ```model2```, but ```model2``` is preferred based on the $\\mathrm{FPE}$ criteria."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "***\n",
    "**_Cross-Validation_**\n",
    "$$\n",
    "\\mathrm{CV}=\\frac{1}{n}\\sum_{i=1}^{n} \\widetilde{e}_{i}^{2},\n",
    "$$\n",
    "where $\\widetilde{e}_{i}$ are the least squares leave-one-out prediction errors.\n",
    "\n",
    "<ins>Prediction erros</ins>: We define the leave-one-out estimator as that obtained by applying an estimation formula to the sample omitting the $i$th observation, i.e.,\n",
    "\n",
    "$$\n",
    "\\widehat{\\boldsymbol{\\beta}}_{(-i)}=\\widehat{\\boldsymbol{\\beta}}-\\frac{1}{\\left(1-h_{i i}\\right)}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\boldsymbol{x}_{i} \\widehat{e}_{i},\n",
    "$$\n",
    "\n",
    "where $\\widehat{e}_{i}$ are the least squares residuals and $h_{ii}$ are the [leverage](https://en.wikipedia.org/wiki/Leverage_(statistics)) values. We also define the leave-one-out residual or prediction error as that obtained using the leave-one-out regression estimator, thus\n",
    "\n",
    "$$\n",
    "\\tilde{e}_{i}=y_{i}-x_{i}^{\\prime} \\widehat{\\boldsymbol{\\beta}}_{(-i)}=\\left(1-h_{i i}\\right)^{-1} \\widehat{e}_{i}.\n",
    "$$\n",
    "\n",
    "We define the out-of-sample mean squared error as\n",
    "$$\n",
    "\\tilde{\\sigma}^{2}=\\frac{1}{n} \\sum_{i=1}^{n} \\widetilde{e}_{i}^{2}=\\frac{1}{n} \\sum_{i=1}^{n}\\left(1-h_{i i}\\right)^{-2} \\widehat{e}_{i}^{2}\n",
    "$$\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "## calculating manually the CV criteria for models 1 and 2\n",
    "CV1 <- mean((resid(model1)/(1 - hatvalues(model1)))^2) \n",
    "CV2 <- mean((resid(model2)/(1 - hatvalues(model2)))^2)\n",
    "data.frame(CV=unname(rbind(CV1,CV2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "üíª Again, the ```model1``` has smaller $\\mathrm{CV}$ than ```model2```."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Relationship among Selection Criteria"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We first point out _three_ approximations that will be useful to see the relationships among these selection criteria:\n",
    "\n",
    "1. $\\log(1+x)\\approx x$ for small $x$.\n",
    "2. $(1-x)^{-1}\\approx 1+x$ for small $x$.\n",
    "3. $(1+x)^2\\approx 1+2x$ for small $x$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "## for easy visualization\n",
    "if (!require(ggplot2)) install.packages('ggplot2')\n",
    "library(ggplot2)\n",
    " \n",
    "base <- ggplot(data.frame(x = c(-0.25, 0.25)), aes(x)) + theme(axis.title.y = element_blank())\n",
    "\n",
    "## plotting approximation 1\n",
    "p1 <- base + \n",
    "    stat_function(fun = function(.x) .x) +\n",
    "    stat_function(fun = function(.x) log(1+.x),colour = \"blue\")\n",
    "\n",
    "## plotting approximation 2                  \n",
    "p2 <- base +\n",
    "    stat_function(fun = function(.x) 1+.x) +\n",
    "    stat_function(fun = function(.x) 1/(1-.x),colour = \"blue\")\n",
    "\n",
    "## plotting approximation 3                  \n",
    "p3 <- base + \n",
    "    stat_function(fun = function(.x) 1+2*.x) +\n",
    "    stat_function(fun = function(.x) (1+2*.x)^2,colour = \"blue\")\n",
    "                  \n",
    "## to plot multiple graphs on the same figure\n",
    "if (!require(gridExtra)) install.packages('gridExtra')\n",
    "library(gridExtra)\n",
    "grid.arrange(p1, p2, p3, nrow = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "üëâ If $\\widehat{\\sigma}^2=\\widetilde{\\sigma}^2$, then we have that $$\\mathrm{Shibata}=C_p/n.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "üëâ Taking logarithms and using the approximation $\\log (1+x) \\simeq x$ for small $x$, we have $$n\\log(\\mathrm{Shibata})=n \\log \\left(\\widehat{\\sigma}^{2}\\right)+n \\log \\left(1+\\frac{2 K}{n}\\right) \\simeq n \\log \\left(\\widehat{\\sigma}^{2}\\right)+2 K=\\mathrm{AIC}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "üëâ Using the expansions $(1-x)^{-1} \\simeq 1+x$ and $(1+x)^{2} \\simeq 1+2 x$ we see that $$\\mathrm{FPE}=\\widehat{\\sigma}^{2}\\left(\\frac{1+K / n}{1-K / n}\\right)=\\widehat{\\sigma}^{2}\\left(\\frac{1}{1-K / n}\\right)\\left(1+\\frac{K}{n}\\right)\\simeq\\widehat{\\sigma}^{2}\\left(1+\\frac{K}{N}\\right)\\left(1+\\frac{K}{N}\\right)\\simeq\\widehat{\\sigma}^{2}\\left(1+\\frac{2 K}{n}\\right)=\\mathrm{Shibata}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "üëâ By the expansion $(1-x)^{-2} \\simeq 1+2 x$ we find that\n",
    "$$\n",
    "n \\mathrm{GCV}=\\frac{\\widehat{\\sigma}^{2}}{(1-K / n)^{2}} \\simeq \\widehat{\\sigma}^{2}\\left(1+\\frac{2 K}{n}\\right)=\\mathrm{ Shibata}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The above calculations show that the $\\mathrm{AIC}$, $\\mathrm{Shibata}$, $\\mathrm{FPE}$, $\\mathrm{GCV}$, and Mallows criterion are all close approximations to one another when $K/n$ is small. Differences arise in finite samples for large $K$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "üëâ There also is a connection between $\\mathrm{CV}$ and the above criteria. Again using the expansion\n",
    "$(1-x)^{-2} \\simeq 1+2 x$ we find that\n",
    "$$\n",
    "\\begin{aligned} n\\mathrm{CV} =\\sum_{i=1}^{n}\\left(1-h_{i i}\\right)^{-2} \\widehat{e}_{i}^{2} & \\simeq \\sum_{i=1}^{n} \\widehat{e}_{i}^{2}+\\sum_{i=1}^{n} 2 h_{i i} \\widehat{e}_{i}^{2} \\\\ &=n \\widehat{\\sigma}^{2}+2 \\operatorname{tr}\\left(\\left(X^{\\prime} X\\right)^{-1}\\left(\\sum_{i=1}^{n} x_{i} x_{i}^{\\prime} \\widehat{e}_{i}^{2}\\right)\\right) \\\\ & \\simeq n \\widehat{\\sigma}^{2}+2 \\text{tr}\\left(\\left(E\\left(\\boldsymbol{x}_{i} \\boldsymbol{x}_{i}^{\\prime}\\right)\\right)^{-1}\\left(E\\left(\\boldsymbol{x}_{i} \\boldsymbol{x}_{i}^{\\prime} e_{i}^{2}\\right)\\right)\\right)\\\\ & \\simeq n \\widehat{\\sigma}^{2}+2 K \\sigma^{2} \\\\ &=\\mathrm{Shibata} \\text{, by replacing }\\sigma^2\\text{ by }\\widehat{\\sigma}^2. \\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The third-to-last line holds asymptotically by the [WLLN](https://en.wikipedia.org/wiki/Law_of_large_numbers). The following equality holds under conditional homoskedasiticity. The final approximation replaces $\\sigma^{2}$ by the estimator $\\widehat{\\sigma}^{2}$. This calculation shows that under the assumption of conditional homoskedasticity the $\\mathrm{CV}$ criterion is similar to the other criteria. It differs under heteroskedasticity, however, which is one of its primary advantages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Consistent Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "‚ÅâÔ∏è There may be _multiple true_ models.\n",
    "\n",
    "<ins>Example</ins>: Consider the two models $\\mathscr{M}_1$: $y_{i}=x_{1 i}^{\\prime} \\boldsymbol{\\beta}_{1}+e_{i}$ and $\\mathscr{M}_2$: $y_{i}=\\boldsymbol{x}_{1 i}^{\\prime} \\boldsymbol{\\beta}_{1}+\\boldsymbol{x}_{2 i}^{\\prime} \\boldsymbol{\\beta}_{2}+e_{i}$. If $\\boldsymbol{\\beta}_{2} \\neq 0$ then only $\\mathscr{M}_2$ is true. But if $\\boldsymbol{\\beta}_{2}=0$ then both, $\\mathscr{M}_1$ and $\\mathscr{M}_2$, are true models!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We call a _model selection procedure_ **consistent** if it selects a true model (out of possibly many) in large samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In general, given a set of models $\\overline{\\mathscr{M}}=\\left\\{\\mathscr{M}_{1}, \\ldots, \\mathscr{M}_{M}\\right\\}$, a subset $\\overline{\\mathscr{M}}^\\ast$ are true models (as in the two linear regression models above)\n",
    "while the remainder are not true models. A model selection rule $\\widehat{\\mathscr{M}}$ selects _one model_ from the set $\\overline{\\mathscr{M}}$. We say a method is consistent if it asymptotically selects a true model, i.e.,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "__Definition__: A model selection rule is model selection consistent if\n",
    "$$\n",
    "\\text{Pr}\\left(\\widehat{\\mathscr{M}} \\in \\overline{\\mathscr{M}}^{*}\\right) \\rightarrow 1\n",
    "$$\n",
    "as $n \\rightarrow \\infty$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "A major limitation with this result is that the definition of model selection consistency is _weak_. A model may be true but over parameterized. To understand the distinction consider $\\mathscr{M}_{1}$ and $\\mathscr{M}_{2}$ above. If $\\boldsymbol{\\beta}_{2}=0$ then both $\\mathscr{M}_{1}$ and $\\mathscr{M}_{2}$ are true, but $\\mathscr{M}_{1}$ would be the preferred model as it is more parsimonious. <ins>When two nested models are both true models, it is conventional to think of the more parsimonious model as the correct model</ins>. In this context we do not describe the larger model as an incorrect model, but rather as over-parameterized. If a selection rule asymptotically selects an over-parameterized model we say that it \"over-selects.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "__Definition__: A model selection rule asymptotically _over-selects_ if there are\n",
    "models $\\mathscr{M}_{1} \\subset \\mathscr{M}_{2}$ such that\n",
    "$$\n",
    "\\liminf _{n \\rightarrow \\infty} \\text{Pr}\\left(\\widehat{\\mathscr{M}}=\\mathscr{M}_{2} | \\mathscr{M}_{1}\\right)>0.\n",
    "$$\n",
    "\n",
    "<p style='text-align: right;'> <a href=\"https://en.wikipedia.org/wiki/Limit_superior_and_limit_inferior\" style=\"color: #cc0000\">Limit Inferior</a></p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The definition states that over-selection occurs when two models are nested and the smaller (short)\n",
    "model is true (so both models are true models but the smaller model is more parsimonious), if the larger model is asymptotically selected with positive probabilty."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Information Criteria"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<span style=\"color:blue\">Result 1:</span> Under some regularity conditions, selection based on $\\mathrm{IC}$ asymptotically over-selects if $c(n,K)=O(1)$ as $n\\rightarrow\\infty$.\n",
    "\n",
    "<p style='text-align: right;'> <a href=\"https://en.wikipedia.org/wiki/Big_O_notation\" style=\"color: #cc0000\">Big O Notation</a></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "‚úçüèΩ Clearly $c=2K=O(1)$ so the $\\mathrm{AIC}$ *over-selects*.\n",
    "\n",
    "<ins>Example</ins>: This means that if the models are $\\mathscr{M}_1$: $y_{i}=x_{1 i}^{\\prime} \\boldsymbol{\\beta}_{1}+e_{i}$ and $\\mathscr{M}_2$: $y_{i}=\\boldsymbol{x}_{1 i}^{\\prime} \\boldsymbol{\\beta}_{1}+\\boldsymbol{x}_{2 i}^{\\prime} \\boldsymbol{\\beta}_{2}+e_{i}$ and $\\boldsymbol{\\beta}_{2}=0$ holds, then the $\\mathrm{AIC}$ will select $\\mathscr{M}_2$ with positive probability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let us now define the set of parsimonious models $\\overline{\\mathscr{M}}^{0} \\subset \\overline{\\mathscr{M}}^{*}$ as the set of true models with the fewest number of parameters. When the models in $\\overline{\\mathscr{M}}^{*}$ are nested then $\\overline{\\mathscr{M}}^{0}$ will be a singleton (just one).\n",
    "\n",
    "<ins>Example</ins>: In the regression example when $\\boldsymbol{\\beta}_2$, then $\\mathscr{M}_1$ is the parsimonious model among $\\{\\mathscr{M}_1,\\mathscr{M}_2\\}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<span style=\"color:blue\">Result 2:</span> Under some regularity conditions, selection based on $\\mathrm{IC}$ is consistent for parsimonious models if for all $K_{2}>K_{1}$ as $n \\rightarrow \\infty$, yet $c(n, K)=o(n)$ as $n\\rightarrow \\infty$.\n",
    "\n",
    "<p style='text-align: right;'> <a href=\"https://en.wikipedia.org/wiki/Big_O_notation#Little-o_notation\" style=\"color: #cc0000\">Small o Notation</a></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "‚úçüèΩ For $\\mathrm{BIC}$ one has that $c\\left(n, K_{2}\\right)-c\\left(n, K_{1}\\right)=\\left(K_{2}-K_{1}\\right) \\log (n) \\rightarrow \\infty$ if $K_{2}>K_{1}$, and $c(n,K)=K\\log(n)=n\\times(K\\cdot\\log(n)/n)=o(n)$ for a fixed $K$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "## plotting log(n)/n\n",
    "base <- ggplot(data.frame(n = c(1, 1000)), aes(n)) + theme(axis.title.y = element_blank())\n",
    "base + \n",
    "    stat_function(fun = function(.n) log(.n)/.n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Therefore one can conclude that $\\mathrm{BIC}$ is preferred over the other criteria if we seek to try to find the parsimonious true model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Asymptotic Selection Optimality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The following analysis is based on the following _homoskedastic_ regression model:\n",
    "\n",
    "$$\n",
    "\\begin{array}{c}\n",
    "{y_{i}=m_{i}+e_{i}}\\text{,} \\\\\n",
    "{m_{i}=\\sum_{j=1}^{\\infty} \\mathbf{x}_{j i} \\beta_{j}}\\text{,} \\\\\n",
    "{\\mathbb{E}\\left(e_{i} | \\mathbf{x}_{i}\\right)=0}\\text{,} \\\\\n",
    "{\\mathbb{E}\\left(e_{i}^{2} | \\mathbf{x}_{i}\\right)=\\sigma^{2}}\\text{,} \\\\\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "where $\\mathbf{x}_{i}=\\left(\\mathbf{x}_{1 i}, \\mathbf{x}_{2 i}, \\ldots\\right)^\\prime$.\n",
    "\n",
    "The $K^{\\text {th }}$ regression model uses the first $K$ regressors $\\mathbf{x}_{K i}=\\left(\\mathbf{x}_{1 i}, \\mathbf{x}_{2 i}, \\ldots, \\mathbf{x}_{K i}\\right)$. The least squares estimates in matrix notation are\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathbf{y}&=\\mathbf{X}_{K} \\widehat{\\boldsymbol{\\beta}}_{K}+\\widehat{\\mathbf{e}}_{K}\\\\\n",
    "&=\\widehat{\\mathbf{m}}+\\widehat{\\mathbf{e}}_{K}\\text{,}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "and regression fit (out-of-sample sum of expected squared prediction errors) is defined as\n",
    "$$\n",
    "R_{n}(K)=E\\left((\\widehat{\\mathbf{m}}-\\mathbf{m})^{\\prime}(\\widehat{\\mathbf{m}}-\\mathbf{m})|\\mathbf{X}\\right).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Now, define the _infeasible_ optimal model $K$ which minimizes $R_{n}(K)$\n",
    "\n",
    "$$K_{n}^{\\mathrm{opt}}=\\underset{K}{\\text{arg min}} R_{n}(K).$$\n",
    "\n",
    "The _infeasible_ optimal model $K_{n}^{\\mathrm{opt}}$ obtains the minimized value of $R_{n}(K)$ as\n",
    "$$\n",
    "R_{n}^{\\mathrm{opt}}=R_{n}\\left(K_{n}^{\\mathrm{opt}}\\right)=\\min _{K} R_{n}(K)\n",
    "$$\n",
    "\n",
    "Now consider model selection using the Mallow‚Äôs criterion for regression models\n",
    "\n",
    "$$\n",
    "C_{p}(K)=\\widehat{e}_{K}^{\\prime} \\widehat{e}_{K}+2 \\widetilde{\\sigma}^{2} K\n",
    "$$\n",
    "\n",
    "where we explicitly index by $K$ and $\\widetilde{\\sigma}^{2}$ is a preliminary estimator of $\\sigma^{2}$ (typically based on fitting a large model, i.e., the one containing all the predictors in your sample). Let the selected model be\n",
    "\n",
    "$$\n",
    "\\widehat{K}_{n}=\\underset{K}{\\text{arg min}} C_{p}(K)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "üìÑ Li, Ker-Chau (1987): \"Asymptotic optimality for $C_p$, $C_L$, cross-validation and generalized crossvalidation: Discrete Index Set,\" *Annals of Statistics*, 15, pp. 958-975."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "üü• Showed that if\n",
    "\n",
    "1. The observations $\\left(y_{i}, x_{i}\\right), i=1, \\ldots, n,$ are independent and identically distributed.\n",
    "2. $E\\left(e_{i} | \\mathbf{x}_{i}\\right)=0$.\n",
    "3. $E\\left(e_{i}^{2} | \\mathbf{x}_{i}\\right)=\\sigma^{2}$.\n",
    "4. $E\\left[\\left|e_{i}\\right|^{4 r} | \\mathbf{x}_{i}\\right) \\leq B<\\infty$ for some $r>1$.\n",
    "5. $R_{n}^{\\mathrm{opt}} \\rightarrow \\infty$ as $n \\rightarrow \\infty$.\n",
    "6. The estimated models are nested.\n",
    "\n",
    "Then\n",
    "\n",
    "$$\n",
    "\\frac{R_{n}(\\widehat{K}_{n})}{R_{n}^{\\mathrm{opt}}} \\stackrel{p}{\\longrightarrow} 1.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This means that the selection procedure that utilizes the $C_p$ selection criterion is _asymptotically optimal_ in the sense that the prediction accuracy is asymptotically equivalent with the _infeasible_ optimum.\n",
    "\n",
    "Condition 5 effectively states that there is no correctly specified finite-dimensional model. To see this, suppose that there is a $K<\\infty$ such that the model is correctly specified, i.e., $m_{i}=\\sum_{j=1}^{K} x_{j i} \\beta_{j}$. In this case one has that $(\\widehat{\\mathbf{m}}-\\mathbf{m})^{\\prime}(\\widehat{\\mathbf{m}}-\\mathbf{m})=(\\widehat{\\boldsymbol{\\beta}}_K-\\boldsymbol{\\beta}_K)^\\prime\\mathbf{X}_K^\\prime\\mathbf{X}_K(\\widehat{\\boldsymbol{\\beta}}_K-\\boldsymbol{\\beta}_K)$. Then by noticing that $\\mathbf{X}_K^\\prime\\mathbf{X}_K\\stackrel{p}{\\longrightarrow}E(\\mathbf{x}_i\\mathbf{x}_i^{\\prime})$ by the WLLN and recalling that the Asymptotic variance-covariance matrix (under homoskedasticity) of the OLS is $\\sigma^2E(\\mathbf{x}_i\\mathbf{x}_i^{\\prime})$ one has that $R_n(K)\\approx \\sigma^2(\\widehat{\\boldsymbol{\\beta}}_K-\\boldsymbol{\\beta}_K)^\\prime\\mathbf{V}_{\\widehat{\\mathbf{\\beta}}_K}^{-1}(\\widehat{\\boldsymbol{\\beta}}_K-\\boldsymbol{\\beta}_K)\\stackrel{d}{\\longrightarrow}\\sigma^2\\chi^2(K)$ as $n\\rightarrow\\infty$ asymptotically and therefore $R_n(K)\\approx \\sigma^2\\times E[\\chi^2(K)]=\\sigma^2K$ as $n\\rightarrow\\infty$ which violates condition 5 above.\n",
    "\n",
    "<p style='text-align: right;'> <a href=\"https://en.wikipedia.org/wiki/Chi-squared_distribution\" style=\"color: #cc0000\">Chi-squared Distribution</a></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "üìÑ Andrews, Donald W. K. (1991c): \"Asymptotic optimality of generalized $C_{L},$ cross-validation, and generalized cross-validation in regression with heteroskedastic errors,\" *Journal of Econometrics*, 47, pp. 359-377."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "üü• Showed that selection by $\\mathrm{CV}$ satisfies the same asymptotic optimality condition _without_ requiring conditional homoskedasticity."
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.5.0"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
