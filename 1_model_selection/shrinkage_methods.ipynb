{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Shrinkage Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The subset selection methods described before involves using OLS to fit a linear model that contains a subset of the predictors. As an alternative, we can fit a model containing _all_ predictors using a technique that constrains or 'regularizes' the coefficient estimates, or equivalently, that _shrinks_ the coefficient estimates towards zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "💻 Previously we were interested in finding the most parsimonious model for the following data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "## for easy data manipulation and visualization\n",
    "if (!require(tidyverse)) install.packages(\"tidyverse\")\n",
    "library(tidyverse)\n",
    "\n",
    "## for easy machine learning workflow\n",
    "if (!require(caret)) install.packages(\"caret\")\n",
    "library(caret)\n",
    "\n",
    "## installing the 'wooldridge' package if not previously installed\n",
    "if (!require(wooldridge)) install.packages(\"wooldridge\")\n",
    "library(wooldridge)\n",
    "\n",
    "data(hprice2)\n",
    "\n",
    "##  hprice2\n",
    "##  Obs:   506\n",
    "\n",
    "##  1. price                    median housing price, $\n",
    "##  2. crime                    crimes committed per capita\n",
    "##  3. nox                      nitrous oxide, parts per 100 mill.\n",
    "##  4. rooms                    avg number of rooms per house\n",
    "##  5. dist                     weighted dist. to 5 employ centers\n",
    "##  6. radial                   accessibiliy index to radial hghwys\n",
    "##  7. proptax                  property tax per $1000\n",
    "##  8. stratio                  average student-teacher ratio\n",
    "##  9. lowstat                  % of people 'lower status'\n",
    "## 10. lprice                   log(price)\n",
    "## 11. lnox                     log(nox)\n",
    "## 12. lproptax                 log(proptax)\n",
    "\n",
    "## specifying the outcome variable (y) and original predictors (X)\n",
    "outcome <- \"lprice\"\n",
    "predictors <- c(\"lnox\", \"lproptax\", \"crime\", \"rooms\", \"dist\", \"radial\", \"stratio\", \n",
    "    \"lowstat\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We actually found out that the chosen model by 'best,' 'forward,' and 'backward' subset search was\n",
    "\n",
    "$$\n",
    "\\texttt{lprice}=\\beta_{0}+\\beta_{1}\\texttt{lnox}+\\beta_{2}\\texttt{lproptax}+\\beta_{3}\\texttt{crime}+\\beta_{4}\\texttt{rooms}+\\beta_{5}\\texttt{dist}+\\beta_{6}\\texttt{radial}+\\beta_{7}\\texttt{stratio}+\\beta_{8}\\texttt{lowstat}+e\n",
    "$$\n",
    "\n",
    "However this model excludes the possibility of interaction terms among the predictors. Instead consider a most complete potential model with _all_ possible cross-products among the regressors _after_ they are re-center at their mean, i.e.,\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\texttt{lprice}&=\\beta_{0}+\\beta_{1}\\texttt{lnox}+\\beta_{2}\\texttt{lproptax}+\\beta_{3}\\texttt{crime}+\\beta_{4}\\texttt{rooms}+\\beta_{5}\\texttt{dist}+\\beta_{6}\\texttt{radial}\\\\\n",
    "&+\\beta_{7}\\texttt{stratio}+\\beta_{8}\\texttt{lowstat}+\\beta_{9}(\\texttt{lnox}-\\mu_{\\texttt{lnox}})(\\texttt{lproptax}-\\mu_{\\texttt{lproptax}})\\\\\n",
    "&+\\beta_{10}(\\texttt{lnox}-\\mu_{\\texttt{lnox}})(\\texttt{crime}-\\mu_{\\texttt{crime}})+\\ldots+\\beta_{35}(\\texttt{radial}-\\mu_{\\texttt{radial}})(\\texttt{lowstat}-\\mu_{\\texttt{lowstat}})\\\\\n",
    "&+\\beta_{36}(\\texttt{stratio}-\\mu_{\\texttt{stratio}})(\\texttt{lowstat}-\\mu_{\\texttt{lowstat}})+e.\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "In this model with over 4 times the original number of predictors, one has that $\\beta_{2}$ represents the constant elasticity of home prices with respect to property tax at the _mean_ value, i.e. $\\mu_{\\texttt{predictor}}$, of the other $\\texttt{predictor}$. Similar interpretations can be given to the coefficients multiplying the other predictors.\n",
    "\n",
    "✍🏽 One can replace $\\mu_{\\texttt{predictor}}$ with other values of the $\\texttt{predictor}$ that may be of interest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "💻 The following block of code will substract the _sample_ mean from each predictor using the ```scale``` function and attach this new set of predictors to the original data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "## demeaning all predictors, renaming them and saving their variable names\n",
    "x <- scale(model.matrix(lprice ~ ., hprice2)[, -1], center = TRUE, scale = FALSE)\n",
    "colnames(x) <- paste(\"d_\", colnames(x), sep = \"\")\n",
    "drops <- colnames(x)\n",
    "\n",
    "## attaching the demeaned predictors to a copy of the original data set\n",
    "hprice2.copy <- cbind(hprice2, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "💻 This code will partitioned the original data set into a train (80%) and validation (20%) data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Split the data into training and test set\n",
    "set.seed(42)\n",
    "training.samples <- hprice2.copy$lprice %>% createDataPartition(p = 0.8, list = FALSE)\n",
    "train.data <- hprice2.copy[training.samples, ]\n",
    "test.data <- hprice2.copy[-training.samples, ]\n",
    "\n",
    "## creating a copy of the data sets containing only relevant outcome and features\n",
    "train.data <- subset(train.data, select = c(outcome, predictors, drops))\n",
    "test.data <- subset(test.data, select = c(outcome, predictors, drops))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "💻 This block will add _all_ cross-products among the _demeaned_ set of predictors for both data sets (training and test)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "## train data: creating the model matrix and save it in placeholder `a'\n",
    "a <- model.matrix(lprice ~ lnox + lproptax + crime + rooms + dist + radial + stratio + \n",
    "    lowstat + (d_lnox + d_lproptax + d_crime + d_rooms + d_dist + d_radial + d_stratio + \n",
    "    d_lowstat)^2, train.data)\n",
    "\n",
    "## test data: creating the model matrix and save it in placeholder `b'\n",
    "b <- model.matrix(lprice ~ lnox + lproptax + crime + rooms + dist + radial + stratio + \n",
    "    lowstat + (d_lnox + d_lproptax + d_crime + d_rooms + d_dist + d_radial + d_stratio + \n",
    "    d_lowstat)^2, test.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "💻 This last set of commands keeps only the 36 predictors that will be used for estimation excluding the vector of ones (intercept) for both data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "## train data: removing demeaned predictor without a cross-product\n",
    "x.train <- a[, !(colnames(a) %in% drops)][, -1]\n",
    "y.train <- train.data$lprice\n",
    "\n",
    "## test data: removing demeaned predictor without a cross-product\n",
    "x.test <- b[, !(colnames(b) %in% drops)][, -1]\n",
    "y.test <- test.data$lprice\n",
    "\n",
    "## checking that predictors matrices have been correctly constructed\n",
    "data.frame(n.train=dim(x.train)[1],n.test=dim(x.test)[1])\n",
    "cbind(colnames(x.train),colnames(x.test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "💻 The ```glmnet``` packge in R implements the shrinkage estimation procedures described here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "## for shrinkage estimation\n",
    "if (!require(glmnet)) install.packages(\"glmnet\")\n",
    "library(glmnet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## _Ridge_ Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The _traditional_ motivation is to reduce the degree of collinearity among the regressors. The _modern_ motivation is regularization of high-dimensional and ill-posed inverse problems.\n",
    "\n",
    "<p style='text-align: right;'> <a href=\"https://en.wikipedia.org/wiki/Tikhonov_regularization\" style=\"color: #cc0000\">Tikhonov Regularization</a></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Traditional Motivation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Take a linear regression model $y_{i}=\\mathbf{x}_{i}^{\\prime} \\boldsymbol{\\beta}+e_{i}$. In \"machine learning\" applications the dimension of $\\boldsymbol{\\beta}$ can be very large, and often the regressors are highly correlated. In these cases the least squares estimator may be undefined and/or the $\\mathbf{X}^{\\prime}\\mathbf{X}$ matrix ill-conditioned, which can mean that the least squares coefficient estimates are numerically unreliable. As a numerical solution to this dilemma, Hoerl and Kennard (1970) proposed the ridge regression estimator\n",
    "\n",
    "$$\\widehat{\\boldsymbol{\\beta}}_{\\text {ridge}}=\\left(\\mathbf{X}^{\\prime} \\mathbf{X}+\\lambda \\mathbf{I}_{k}\\right)^{-1} \\mathbf{X}^{\\prime} \\mathbf{y}$$\n",
    "\n",
    "where <span style=\"color:blue\">$\\lambda>0$</span> is a shrinkage parameter and treated later on as a <span style=\"color:blue\">tuning parameter</span>.\n",
    "\n",
    "✍🏽 The ridge regression estimator has the property that it is well-defined and does not suffer from multicollinearity or ill-conditioning so long as $\\lambda>0$. This even holds if $k>n$! That is, the ridge regression estimator can be calculated even when the number of regressors exceeds the sample size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "***\n",
    "To see how $\\lambda>0$ ensures that the inverse problem is solved, use the spectral decomposition to write $\\mathbf{X}^{\\prime} \\mathbf{X}=\\mathbf{H}^{\\prime}\\mathbf{D}\\mathbf{H}$ where $\\mathbf{H}$ is orthonormal and $\\mathbf{D}=\\text{diag}\\left\\{r_{1}, \\ldots, r_{k}\\right\\}$ is a diagonal matrix with the eigenvalues $r_{j}$ of $\\mathbf{X}^{\\prime} \\mathbf{X}$ on the diagonal. Let $\\Lambda=\\lambda \\mathbf{I}_{k}$. We can write\n",
    "$$\n",
    "\\mathbf{X}^{\\prime} \\mathbf{X}+\\lambda \\mathbf{I}_{k}=\\mathbf{H}^{\\prime} \\mathbf{D} \\mathbf{H}+\\lambda \\mathbf{H}^{\\prime} \\mathbf{H}=\\mathbf{H}^{\\prime} \\mathbf{D} \\mathbf{H}+\\mathbf{H}^{\\prime} \\Lambda \\mathbf{H}=\\mathbf{H}^{\\prime}(\\mathbf{D}+\\Lambda) \\mathbf{H}\n",
    "$$\n",
    "which has eigenvalues $r_{j}+\\lambda>0$. Thus all eigenvalues are bounded away from zero so $\\mathbf{X}^{\\prime} \\mathbf{X}+\\lambda \\mathbf{I}_{k}$ is full\n",
    "rank and therefore invertible.\n",
    "\n",
    "<p style='text-align: right;'> <a href=\"https://en.wikipedia.org/wiki/Eigendecomposition_of_a_matrix\" style=\"color: #cc0000\">Spectral Decomposition of a Matrix</a></p>\n",
    "\n",
    "<p style='text-align: right;'> <a href=\"https://en.wikipedia.org/wiki/Orthogonal_matrix\" style=\"color: #cc0000\">Orthonormal Matrix</a></p>\n",
    "\n",
    "<p style='text-align: right;'> <a href=\"https://en.wikipedia.org/wiki/Diagonal_matrix\" style=\"color: #cc0000\">Diagonal Matrix</a></p>\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Modern Motivation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The second motivation is based on penalization. When $\\mathbf{X}^{\\prime}\\mathbf{X}$ is ill-conditioned computing its inverse is \"ill-posed.\" Techniques to deal with ill-posed estimators are called \"regularization\" and a leading method is penalization. Consider the penalized regression criterion (Sum of Squared Errors)\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\\text{SSE}_2(\\mathbf{b}, \\lambda) &=(\\mathbf{y}-\\mathbf{X} \\mathbf{b})^{\\prime}(\\mathbf{y}-\\mathbf{X} \\mathbf{b})+\\lambda \\mathbf{b}^{\\prime} \\mathbf{b} \\\\ &=\\|\\mathbf{y}-\\mathbf{X} \\mathbf{b}\\|_{2}^{2}+\\lambda\\|\\mathbf{b}\\|_{2}^{2} \\end{aligned}\n",
    "$$\n",
    "\n",
    "where $\\|\\mathbf{a}\\|_{2}=\\left(\\mathbf{a}^{\\prime} \\mathbf{a}\\right)^{1 / 2}$ is the $L_2$-norm. The minimizer of $\\mathrm{SSE}_2(\\mathbf{b}, \\lambda)$ is a regularized least squares estimator. The first order condition for minimization of $\\mathrm{SSE}_{2}(\\mathbf{b}, \\lambda)$ over $\\mathbf{b}$ is\n",
    "\n",
    "$$\n",
    "-2 \\mathbf{X}^{\\prime}(\\mathbf{y}-X \\mathbf{b})+2 \\lambda \\mathbf{b}=0\n",
    "$$\n",
    "\n",
    "<p style='text-align: right;'> <a href=\"https://en.wikipedia.org/wiki/Euclidean_distance\" style=\"color: #cc0000\">$L_2$-norm</a></p>\n",
    "\n",
    "The solution is $\\widehat{\\boldsymbol{\\beta}}_{\\text {ridge }}$. Thus the regularized (penalized) least squares estimator equals ridge regression. This shows that the ridge regression estimator minimizes the sum of squared errors subject to a penalty on the $L_2$-norm magnitude of the regression coefficient. Penalizing large coefficient vectors keeps the latter from being too large and erratic. _Hence one interpretation of $\\lambda$ is the degree of penalty on the\n",
    "magnitude of the coefficient vector._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "***\n",
    "Minimization subject to a penalty has a dual representation as constrained minimization. The latter is\n",
    "\n",
    "$$\n",
    "\\min _{\\mathbf{b}}(\\mathbf{y}-\\mathbf{X} \\mathbf{b})^{\\prime}(\\mathbf{y}-\\mathbf{X} \\mathbf{b})\n",
    "$$\n",
    "\n",
    "subject to $\\mathbf{b}^{\\prime} \\mathbf{b} \\leq \\tau$, for some $\\tau>0$. To see the connection, the Lagrangian for the constrained problem is\n",
    "$$\n",
    "\\min _{\\mathbf{b}}(\\mathbf{y}-\\mathbf{X} \\mathbf{b})^{\\prime}(\\mathbf{y}-\\mathbf{X} \\mathbf{b})+\\lambda\\left(\\mathbf{b}^{\\prime} \\mathbf{b}-\\tau\\right),\n",
    "$$\n",
    "\n",
    "where $\\lambda$ is a [Lagrange multiplier](https://en.wikipedia.org/wiki/Lagrange_multiplier).\n",
    "\n",
    "The practical difference between the penalization and constraint problems is that in the first you specify the ridge parameter $\\lambda$ while in the second you specify the constraint $\\tau$. They are connected, since the values of $\\lambda$ and $\\tau$ satisfy the relationship\n",
    "\n",
    "$$\n",
    "\\mathbf{y}^{\\prime} \\mathbf{X}\\left(\\mathbf{X}^{\\prime} \\mathbf{X}+\\lambda \\mathbf{I}_{k}\\right)^{-1}\\left(\\mathbf{X}^{\\prime} \\mathbf{X}+\\lambda \\mathbf{I}_{k}\\right)^{-1} \\mathbf{X}^{\\prime} \\mathbf{y}=\\tau.\n",
    "$$\n",
    "\n",
    "Thus to find $\\lambda$ given $\\tau$ it is sufficient to (numerically) solve this equation.\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"img/ridge.png\" alt=\"drawing\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This figure help us visualize the constraint problem as it plots an example in $\\mathbb{R}^2$. The constraint set $\\mathbf{b}^\\prime\\mathbf{b}\\le\\tau$ is displayed as the ball about the origin and the contour sets of the $\\mathrm{SSE}$ are displayed as ellipses. The least squares estimator is the center of the ellipses, while the ridge regression\n",
    "estimator is the point on the circle where the contour is tangent. This shrinks the least squares coefficient towards the zero vector. It shrinks along a trajectory determined by the degree of correlation between the variables. This trajectory is displayed with the dashed lines, marked as 'Ridge path'. This is the sequence of ridge regression coefficients obtained as $\\lambda$ (or $\\tau$) is varied from small to large. When $\\lambda=0$ (or $\\tau$ is large) the ridge estimator equals least squares. For small $\\lambda$ the ridge estimator moves slightly towards the origin by sliding along the ridge of the contour set. As $\\lambda$ increases the ridge estimator takes a more direct path towards the origin.\n",
    "\n",
    "<p style='text-align: right;'> <a href=\"https://en.wikipedia.org/wiki/Real_coordinate_space\" style=\"color: #cc0000\">$\\mathbb{R}^2$</a></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "💻 In these set of commands the Ridge estimators are calculated for a sequence of values of $\\lambda$ and we observe their performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "## sequence of lambdas\n",
    "lambda <- seq(0, 4, by = 0.5)\n",
    "\n",
    "## Ridge regression with lambda=0\n",
    "model <- glmnet(x.train, y.train, alpha = 0, lambda = lambda[1])\n",
    "beta.hat <- coef(model)\n",
    "\n",
    "## Ridge regression with lambda=.5,1,1.5,...,4\n",
    "for (i in 2:length(lambda)) {\n",
    "    model <- glmnet(x.train, y.train, alpha = 0, lambda = lambda[i])\n",
    "    beta.hat <- cBind(beta.hat, coef(model))\n",
    "}\n",
    "\n",
    "par(mar = c(5, 5, 5, 5))\n",
    "plot(lambda, beta.hat[\"rooms\", ], las = 1, main = \"Predictor: avg number of rooms per house\", \n",
    "    ylab = expression(hat(beta)[ridge]), xlab = expression(lambda))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<ins>Selecting the Tuning Parameter</ins>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "As it can be seen from the empirical example above the actual value of the tuning parameter $\\lambda$ has an effect on the resulting $\\widehat{\\boldsymbol{\\beta}}_{\\text{ridge}}$ values. LOOCV provides a simple way to tackle this problem. We choose a grid of $\\lambda$ values, and compute the LOOCV error for each value of $\\lambda$. We then select the tuning parameter value for which the cross-validation error is the smallest. In this case the leave-one-out\n",
    "ridge regression estimator, prediction errors, and CV criterion are\n",
    "\n",
    "$$\n",
    "\\begin{aligned} \\widehat{\\boldsymbol{\\beta}}_{-i}(\\lambda) &=\\left(\\sum_{j \\neq i} \\mathbf{x}_{j} \\mathbf{x}_{j}^{\\prime}+\\lambda\\mathbf{I}_k\\right)^{-1}\\left(\\sum_{j \\neq i} \\mathbf{x}_{j} y_{i}\\right), \\\\ \\widetilde{e}_{i}(\\lambda) &=y_{i}-\\mathbf{x}_{i}^{\\prime} \\widehat{\\boldsymbol{\\beta}}_{-i}, \\\\ \\text{CV}(\\lambda) &=\\sum_{i=1}^{n} \\widetilde{e}_{i}(\\lambda)^{2}. \\end{aligned}\n",
    "$$\n",
    "\n",
    "The CV-selected shrinkage parameter $\\widehat{\\lambda}$ minimizes $\\mathrm{CV}(\\lambda)$. The cross-validation ridge estimator is calculated using $\\widehat{\\lambda}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "## RIDGE: find the best lambda using LOOCV\n",
    "set.seed(42)\n",
    "cv <- cv.glmnet(x.train, y.train, alpha = 0, nfolds = length(y.train))\n",
    "\n",
    "## RIDGE: display the best lambda value\n",
    "cv$lambda.min"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Finally, the model is re-fit using all the available observations and the selected value of the tuning parameter, i.e., $\\widehat{\\lambda}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "## RIDGE: fit the final model on the training data\n",
    "model <- glmnet(x.train, y.train, alpha = 0, lambda = cv$lambda.min)\n",
    "\n",
    "## RIDGE: display regression coefficients\n",
    "coef(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "One can then use the resulting model for prediction using the validation data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "## RIDGE: make predictions on the test data\n",
    "predictions <- model %>% predict(x.test) %>% as.vector()\n",
    "\n",
    "## RIDGE: model performance metrics\n",
    "data.frame(RMSE = RMSE(predictions, test.data$lprice), Rsquare = R2(predictions, \n",
    "    test.data$lprice))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "✍🏽 One disadvantage of the ridge regression is that, it will include _all_ the predictors in the final model. Ridge regression shrinks the coefficients towards zero, but it will not set any of them exactly to zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## _Lasso_ Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The LASSO stands for **Least Absolute Shrinkage and Selection Operator**. It shrinks the regression coefficients towards zero by penalizing the regression model with a $L_1$ penalty. The $L_{1}$ penalized least squares criterion is\n",
    "$$\n",
    "\\begin{aligned} \\mathrm{SSE}_{1}(\\mathbf{b}, \\lambda) &=(\\mathbf{y}-\\mathbf{X} \\mathbf{b})^{\\prime}(\\mathbf{y}-\\mathbf{X} \\mathbf{b})+\\lambda \\sum_{j=1}^{k}\\left|\\beta_{j}\\right| \\\\ &=\\|\\mathbf{y}-\\mathbf{X} \\mathbf{b}\\|_{2}^{2}+\\lambda\\|\\mathbf{b}\\|_{1}, \\end{aligned}\n",
    "$$\n",
    "where $\\|\\mathbf{a}\\|_{1}=\\sum_{j=1}^{k}\\left|a_{j}\\right|$ is the $L_{1}$-norm. The LASSO estimator is then defined as\n",
    "$$\n",
    "\\widehat{\\boldsymbol{\\beta}}_{\\text{lasso}}=\\underset{\\mathbf{b}}{\\text{arg min }} \\mathrm{SSE}_{1}(\\mathbf{b}, \\lambda).\n",
    "$$\n",
    "\n",
    "<p style='text-align: right;'> <a href=\"https://en.wikipedia.org/wiki/Taxicab_geometry\" style=\"color: #cc0000\">$L_1$-norm</a></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "***\n",
    "The LASSO minimization problem has the dual constrained optimization problem\n",
    "$$\n",
    "\\widehat{\\boldsymbol{\\beta}}_{\\text {lasso }}=\\underset{\\|\\mathbf{b}\\|_{1} \\leq \\tau}{\\text{arg min } \\mathrm{SSE}_1}(\\mathbf{b}).\n",
    "$$\n",
    "To see that the two problems are the same observe that the constrained optimization problem has the Lagrangian\n",
    "$$\n",
    "\\min _{\\mathbf{b}}(\\mathbf{y}-\\mathbf{X} \\mathbf{b})^{\\prime}(\\mathbf{y}-\\mathbf{X} \\mathbf{b})+\\lambda\\left(\\sum_{j=1}^{k}\\left|b_{j}\\right|-\\tau\\right)\n",
    "$$\n",
    "which has first order conditions\n",
    "$$\n",
    "-2 \\mathbf{x}_{j}^{\\prime}(\\mathbf{y}-\\mathbf{X} \\mathbf{b})+\\lambda \\text{ sgn}\\left(b_{j}\\right)=0\n",
    "$$\n",
    "\n",
    "<p style='text-align: right;'> <a href=\"https://en.wikipedia.org/wiki/Sign_function\" style=\"color: #cc0000\">Sign Function</a></p>\n",
    "\n",
    "which are the same as those for minimization of the penalized criterion. Thus the solutions are identical.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"img/lasso.png\" alt=\"drawing\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The constrained minimization problem in $\\mathbb{R}^2$ is shown here. As one can see the constraint set $\\left\\{\\|\\mathbf{b}\\|_{1} \\leq \\tau\\right\\}$ for the dual problem is a cross-polytope, resembling a multi-faceted diamond. As before the sum of squared error contour sets are the ellipses with the least squares solution at the center. The constraint set is the shaded polytope. The LASSO estimator is the intersection point between the constraint set and the largest ellipse drawn, and in this example hits a vertex of the constraint set, and so the constrained estimator sets $\\widehat{\\beta}_1=0$. This is a typical outcome in LASSO estimation. Since we are minimizing a quadratic subject to a polytope constraint, the solution tends to be at vertices which eliminate a subset of the coefficients.\n",
    "\n",
    "The LASSO path is drawn with the dashed line. This is the sequence of solution paths obtained as the constraint set is varied. The solution path has the property that it is a straight line from the least squares estimator to the y-axis (in this example), at which point $\\beta_1$ is set to zero, and then the solution path follows the y-axis to the origin. With a general number of coefficients the solution path has a similar property, where the solution path is linear on segments until each coefficient hits zero, at which point it is eliminated. In this particular example the solution path shows $\\beta_2$ increasing while $\\beta_1$ decreases. Thus while LASSO is a shrinkage estimator it does not necessarily shrink the individual coefficients monotonically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "💻 In these set of commands the LASSO estimators are calculated for a sequence of values of $\\lambda$ and we observe their performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "## sequence of lambdas\n",
    "lambda <- seq(0, 4, by = 0.5)\n",
    "\n",
    "## LASSO regression with lambda=0\n",
    "model <- glmnet(x.train, y.train, alpha = 1, lambda = lambda[1])\n",
    "beta.hat <- coef(model)\n",
    "\n",
    "## LASSO regression with lambda=.5,1,1.5,...,4\n",
    "for (i in 2:length(lambda)) {\n",
    "    model <- glmnet(x.train, y.train, alpha = 1, lambda = lambda[i])\n",
    "    beta.hat <- cBind(beta.hat, coef(model))\n",
    "}\n",
    "\n",
    "par(mar = c(5, 5, 5, 5))\n",
    "plot(lambda, beta.hat[\"rooms\", ], las = 1, main = \"Predictor: avg number of rooms per house\", \n",
    "    ylab = expression(hat(beta)[lasso]), xlab = expression(lambda))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<ins>Selecting the Tuning Parameter</ins>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Critically important for LASSO estimation is the choice of penalty $\\lambda$. The most common choice is minimization of $k$-fold cross validation. Leave-one-out CV is not used as it is computationally expensive. $k$-fold is a computationally feasible substitute. Many programs set the default number of folds as $k=10$, though some authors use $k=5$, while others recommend $k=20$. It is common to find that the results of $k$-fold CV can be sensitive across runs (the methods depends on the random sorting of the observations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "## LASSO: find the best lambda using k-fold (k=10) cross-validation\n",
    "set.seed(42)\n",
    "cv <- cv.glmnet(x.train, y.train, alpha = 1, nfold = 10)\n",
    "\n",
    "## LASSO: display the best lambda value\n",
    "cv$lambda.min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "## LASSO: fit the final model on the training data\n",
    "model <- glmnet(x.train, y.train, alpha = 1, lambda = cv$lambda.min)\n",
    "\n",
    "## LASSO: display regression coefficients\n",
    "coef(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "## LASSO: make predictions on the test data\n",
    "predictions <- model %>% predict(x.test) %>% as.vector()\n",
    "\n",
    "## LASSO: model performance metrics\n",
    "data.frame(RMSE = RMSE(predictions, test.data$lprice), Rsquare = R2(predictions, \n",
    "    test.data$lprice))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### OLS vs Ridge vs LASSO: A Special Case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "One case where we can explicitly calculate the LASSO estimates is when the regressors are orthogonal. Suppose that $\\mathbf{X}^{\\prime} \\mathbf{X}=\\mathbf{I}_{k}$ and $k<n$. Then the first order condition for minimization simplifies to\n",
    "\n",
    "$$\n",
    "-2\\left(\\widehat{\\beta}_{\\mathrm{ols}, j}-\\widehat{\\beta}_{\\text{lasso}, j}\\right)+\\lambda \\text{ sgn}\\left(\\widehat{\\beta}_{\\text{lasso}, j}\\right)=0\n",
    "$$\n",
    "\n",
    "which has the explicit solution\n",
    "\n",
    "$\\widehat{\\beta}_{\\text {lasso, } j}=\\left\\{\\begin{array}{ll}{\\widehat{\\beta}_{\\text {ols, }, j}-\\lambda / 2} & {\\widehat{\\beta}_{\\text {ols, }, j}>\\lambda / 2} \\\\ {0} & {\\left|\\widehat{\\beta}_{\\text {ols, }, j}\\right| \\leq \\lambda / 2} \\\\ {\\widehat{\\beta}_{\\text {ols, } j}+\\lambda / 2} & {\\widehat{\\beta}_{\\text {ols, } j}<-\\lambda / 2}\\end{array}\\right.$\n",
    "\n",
    "\n",
    "Similarly the ridge estimator equals\n",
    "\n",
    "$$\n",
    "\\widehat{\\boldsymbol{\\beta}}_{\\text {ridge}}=(1+\\lambda)^{-1} \\widehat{\\boldsymbol{\\beta}}_{\\text {ols}}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "✍🏽 For small values of the least squares estimate the LASSO estimate is set to zero. For all other values the LASSO\n",
    "estimate moves the least squares estimate towards zero by $\\lambda/2$.\n",
    "\n",
    "✍🏽 The ridge estimate shrinks the coefficients towards zero by a common multiple."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## _Elastic Net_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The difference between LASSO and ridge regression is that the LASSO uses an $L_1$ penalty while ridge uses an $L_2$ penalty. Since the two procedures both have advantages it seems reasonable that further improvements can be obtained by taking a compromise between the two, i.e.,\n",
    "\n",
    "$$\n",
    "\\mathrm{SSE}_3(\\mathbf{b}, \\lambda, \\alpha)=(\\mathbf{y}-\\mathbf{X} \\mathbf{b})^{\\prime}(\\mathbf{y}-\\mathbf{X} \\mathbf{b})+\\lambda\\left((1-\\alpha)\\|\\mathbf{b}\\|_{2}^{2}+\\alpha\\|\\mathbf{b}\\|_{1}\\right)\n",
    "$$\n",
    "\n",
    "for $0 < \\alpha < 1$ and is called the **Elastic Net**. For $\\alpha=1$ we obtain LASSO and for $\\alpha=0$ we obtain ridge\n",
    "regression. For small but positive $\\alpha$ the constraint sets are similar to “rounded” versions of the LASSO constraint sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<ins>Selecting the Tuning Parameters</ins>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Typically the parameters $(\\alpha,\\lambda)$ are selected by joint minimization of the $k$-fold cross-validation criterion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "💻 In the following set of commands we will test the combination of 15 different values for $\\alpha$ and $\\lambda$. This is specified using the option ```tuneLength``` below. It will also perform 10-fold cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "## ELASTIC: building the model\n",
    "set.seed(42)\n",
    "elastic <- train(lprice ~ ., data = data.frame(lprice = y.train, x.train), method = \"glmnet\", \n",
    "    trControl = trainControl(\"cv\", number = 10), tuneLength = 15)\n",
    "\n",
    "## ELASTIC: Best CV tuning parameters\n",
    "elastic$bestTune\n",
    "\n",
    "## ELASTIC: model coefficients\n",
    "coef(elastic$finalModel, elastic$bestTune$lambda)\n",
    "\n",
    "## ELASTIC: making predictions\n",
    "predictions <- elastic %>% predict(data.frame(lprice = y.test, x.test))\n",
    "\n",
    "## ELASTIC: model prediction performance\n",
    "data.frame(RMSE = RMSE(predictions, test.data$lprice), Rsquare = R2(predictions, \n",
    "    test.data$lprice))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Ridge vs LASSO vs Elastic Net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "💻 The ```caret``` package will automatically choose the best tuning parameter values, compute the final model, and evaluate the model performance using cross-validation techniques. We start by setting up a grid range of $\\lambda$ values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "## setting up a grid range of lambda values\n",
    "lambda <- 10^seq(-3, 3, length = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "## RIDGE: building the model\n",
    "set.seed(42)\n",
    "ridge <- train(lprice ~ ., data = data.frame(lprice = y.train, x.train), method = \"glmnet\", \n",
    "    trControl = trainControl(\"cv\", number = 10), tuneGrid = expand.grid(alpha = 0, \n",
    "        lambda = lambda))\n",
    "\n",
    "## RIDGE: model coefficients\n",
    "coef(ridge$finalModel, ridge$bestTune$lambda)\n",
    "\n",
    "## RIDGE: making predictions\n",
    "predictions <- ridge %>% predict(data.frame(lprice = y.test, x.test))\n",
    "\n",
    "## RIDGE: model prediction performance\n",
    "data.frame(RMSE = RMSE(predictions, test.data$lprice), Rsquare = R2(predictions, \n",
    "    test.data$lprice))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "## LASSO: building the model\n",
    "set.seed(42)\n",
    "lasso <- train(lprice ~ ., data = data.frame(lprice = y.train, x.train), method = \"glmnet\", \n",
    "    trControl = trainControl(\"cv\", number = 10), tuneGrid = expand.grid(alpha = 1, \n",
    "        lambda = lambda))\n",
    "\n",
    "## LASSO: model coefficients\n",
    "coef(lasso$finalModel, lasso$bestTune$lambda)\n",
    "\n",
    "## LASSO: making predictions\n",
    "predictions <- lasso %>% predict(data.frame(lprice = y.test, x.test))\n",
    "\n",
    "## LASSO: model prediction performance\n",
    "data.frame(RMSE = RMSE(predictions, test.data$lprice), Rsquare = R2(predictions, \n",
    "    test.data$lprice))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "💻 The following set of commands identify the _best_ model in terms of the prediction error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "models <- list(ridge = ridge, lasso = lasso, elastic = elastic)\n",
    "resamples(models) %>% summary(metric = \"RMSE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "💻 It seems that the LASSO model has the lowest median RMSE."
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.0.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
