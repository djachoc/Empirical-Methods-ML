{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Subset Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This approach involves identifying a subset of the $p$ features (predictors) out of the $k$ that we believe to be related to the response. We then fit a model using OLS on the reduced set of variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "## removing everything from memory\n",
    "rm(list=ls())\n",
    "## turning all warnings off\n",
    "options(warn=-1)\n",
    "\n",
    "## installing the 'wooldridge' package if not previously installed\n",
    "if (!require(wooldridge)) install.packages('wooldridge')\n",
    "\n",
    "## loading the packages\n",
    "library(wooldridge)\n",
    "\n",
    "data(hprice2)\n",
    "\n",
    "##  hprice2\n",
    "##  Obs:   506\n",
    "\n",
    "##  1. price                    median housing price, $\n",
    "##  2. crime                    crimes committed per capita\n",
    "##  3. nox                      nitrous oxide, parts per 100 mill.\n",
    "##  4. rooms                    avg number of rooms per house\n",
    "##  5. dist                     weighted dist. to 5 employ centers\n",
    "##  6. radial                   accessibiliy index to radial hghwys\n",
    "##  7. proptax                  property tax per $1000\n",
    "##  8. stratio                  average student-teacher ratio\n",
    "##  9. lowstat                  % of people 'lower status'\n",
    "## 10. lprice                   log(price)\n",
    "## 11. lnox                     log(nox)\n",
    "## 12. lproptax                 log(proptax)\n",
    "\n",
    "## specifying the outcome variable (y) and predictors (X)\n",
    "outcome <- \"lprice\"\n",
    "predictors <- c(\"lnox\", \"lproptax\", \"crime\", \"rooms\", \"dist\", \"radial\", \"stratio\", \"lowstat\")\n",
    "\n",
    "## creating a copy of the data set containing only relevant outcome and features\n",
    "datos <- subset(hprice2,select=c(outcome,predictors))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## _Best_ Subset Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "To perform best subset selection, we fit a separate OLS regression best subset for each possible combination of the $k$ predictors. That is, we fit _all_ $k$ models selection that contain exactly one predictor, all $\\left(\\begin{array}{l}{k} \\\\ {2}\\end{array}\\right)=k(k-1) / 2$ models that contain exactly two predictors, and so forth. We then look at all of the resulting models, with the goal of identifying the one that is best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "## for easy data manipulation and visualization\n",
    "if (!require(tidyverse)) install.packages('tidyverse')\n",
    "library(tidyverse)\n",
    "\n",
    "## for easy machine learning workflow\n",
    "if (!require(caret)) install.packages('caret')\n",
    "library(caret)\n",
    "\n",
    "## for computing best subsets regression\n",
    "if (!require(leaps)) install.packages('leaps')\n",
    "library(leaps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "***\n",
    "**Algorithm**: <ins>_Best_ Subset Selection</ins>\n",
    "\n",
    "1. Let $\\mathcal{M}_{0}$ denote the null model, which contains no predictors. This model simply predicts the sample mean for each observation.\n",
    "2. For $p=1,2, \\ldots k$:\n",
    " 1. Fit all $\\left(\\begin{array}{l}{k} \\\\ {p}\\end{array}\\right)$ models that contain exactly $p$ predictors.\n",
    " 2. Pick the best among these $\\left(\\begin{array}{l}{k} \\\\ {p}\\end{array}\\right)$ models, and call it $\\mathcal{M}_{p}$. Here best is defined as having the smallest $RSS$, or equivalently largest $R^{2}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "models.best <- regsubsets(lprice~., data = datos, nvmax = 8,method=\"exhaustive\")\n",
    "\n",
    "## returning the set of Mp models\n",
    "summary(models.best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "3. Select a single best model from among $\\mathcal{M}_{0}, \\ldots, \\mathcal{M}_{k}$ using a *model selection criteria* previously discussed such as cross-validated prediction error, $C_{p}(\\mathrm{AIC}), \\mathrm{BIC}$, adjusted $R^{2}$, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "## selecting a single model based on selection criteria: Adjusted R2, Cp and BIC\n",
    "res.sum <- summary(models.best)\n",
    "data.frame(Adj.R2 = which.max(res.sum$adjr2),\n",
    "           CP = which.min(res.sum$cp),\n",
    "           BIC = which.min(res.sum$bic)\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "par(mfrow=c(1,3))\n",
    "\n",
    "# Plot the Adjusted R2 vs number of variables \n",
    "plot(res.sum$adjr2,xlab=\"Number of Variables\",ylab=\"Adj. R2\",type=\"l\",main=\"Best fit Adj. R2 vs No of features\")\n",
    "# Get the index of the minimum value\n",
    "a=which.max(res.sum$adjr2)\n",
    "# Mark this in red\n",
    "points(a,res.sum$adjr2[a],col=\"red\",cex=2,pch=20)\n",
    "\n",
    "# Plot the Cp vs number of variables \n",
    "plot(res.sum$cp,xlab=\"Number of Variables\",ylab=\"Cp\",type=\"l\",main=\"Best fit Cp vs No of features\")\n",
    "# Get the index of the minimum value\n",
    "b=which.min(res.sum$cp)\n",
    "# Mark this in red\n",
    "points(b,res.sum$cp[b],col=\"red\",cex=2,pch=20)\n",
    "\n",
    "# Plot the BIC vs number of variables \n",
    "plot(res.sum$bic,xlab=\"Number of Variables\",ylab=\"BIC\",type=\"l\",main=\"Best fit BIC vs No of features\")\n",
    "# Get the index of the minimum value\n",
    "c=which.min(res.sum$bic)\n",
    "# Mark this in red\n",
    "points(c,res.sum$bic[c],col=\"red\",cex=2,pch=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "***\n",
    "⚠️ The number of possible models that must be considered grows rapidly as $k$ increases, e.g., if $k$=10, then there are approximately 1,000 possible models to be considered, and if $k=20$, there are over one million possibilities (1,048,576)!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Stepwise Selection: _Forward_ Stepwise Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "For computational reason, the previous '_best_ subset selection' cannot be applied with very large $k$ (remember it consinders all $2^k$  possible models). The algorithm begins with a model containing no predictors, and then adds predictors to the model, one-at-a-time, untill all the predictors are in the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "***\n",
    "**Algorithm**: <ins>_Forward_ Stepwise Selection</ins>\n",
    "\n",
    "1. Let $\\mathcal{M}_{0}$ denote the null model, which contains _no_ predictors.\n",
    "2. For $p=0,1,2,\\ldots, k-1$:\n",
    " 1. Consider all $k-p$ models that augment the predictors in $\\mathcal{M}_{p}$ with one additional predictor.\n",
    " 2. Choose the _best_ among these $k-p$ models, and call it $\\mathcal{M}_{p+1}$. Here best is defined as having smallest $RSS$ or highest $R^{2}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "models.forward <- regsubsets(lprice~., data = datos, nvmax = 8,method=\"forward\")\n",
    "\n",
    "## returning the set of Mp models\n",
    "summary(models.forward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "3. Select a single best model from among $\\mathcal{M}_{0}, \\ldots, \\mathcal{M}_{k}$ using a *model selection criteria* previously discussed such as cross-validated prediction error, $C_{p}(\\mathrm{AIC}), \\mathrm{BIC}$, adjusted $R^{2}$, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "## selecting a single model based on selection criteria: Adjusted R2, Cp and BIC\n",
    "res.sum <- summary(models.forward)\n",
    "data.frame(Adj.R2 = which.max(res.sum$adjr2),\n",
    "           CP = which.min(res.sum$cp),\n",
    "           BIC = which.min(res.sum$bic)\n",
    "          )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "***\n",
    "⚠️ There are a total of $1+\\sum_{p=0}^{k-1}(k-p)=1+\\frac{k(k+1)}{2}$ models to be estimated instead of $2^{k}$ as before,\n",
    "e.g. if $k=20,$ there are 211 models to be estimated! Interestingly it will work even when <font color=blue>$k>n$</font>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stepwise Selection: _Backward_ Stepwise Selection\n",
    "\n",
    "Unlike forward stepwise selection, it begins with the full least squares model containing all $k$ regressors, and then iteractively the least useful regressors one-at-a-time until there are no predictors in the model, i.e., the null model.\n",
    "\n",
    "***\n",
    "**Algorithm**: <ins>_Backward_ Stepwise Selection</ins>\n",
    "\n",
    "1. Let $\\mathcal{M}_{k}$ denote the full model, which contains _all_ $k$ predictors.\n",
    "2. For $p=k,k-1,\\ldots, 1$:\n",
    " 1. Consider all $p$ models that contain all but one of the predictors in $\\mathcal{M}_{p}$ for a total of $p-1$ predictors.\n",
    " 2. Choose the best among these $p$ models, and call it $\\mathcal{M}_{p-1}$. Here best is defined as having smallest RSS or highest $R^{2}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "models.backward <- regsubsets(lprice~., data = datos, nvmax = 8,method=\"backward\")\n",
    "\n",
    "## returning the set of Mp models\n",
    "summary(models.backward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "3. Select a single best model from among $\\mathcal{M}_{0}, \\ldots, \\mathcal{M}_{k}$ using a *model selection criteria* previously discussed such as cross-validated prediction error, $C_{p}(\\mathrm{AIC}), \\mathrm{BIC}$, adjusted $R^{2}$, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "## selecting a single model based on selection criteria: Adjusted R2, Cp and BIC\n",
    "res.sum <- summary(models.backward)\n",
    "data.frame(Adj.R2 = which.max(res.sum$adjr2),\n",
    "           CP = which.min(res.sum$cp),\n",
    "           BIC = which.min(res.sum$bic)\n",
    "          )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "***\n",
    "⚠️ Unlike the _forward_ stepwise selection algorithm, this will _only_ work when <font color=blue>$n>k$</font>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<font color=red><ins>Note</ins> Notice that the $\\bar{R}^2$, BIC and $C_p$ are calculated on the training data that have been used to fit the model. This means that, the model selection, using these metrics, is possibly subject to overfitting and may not perform as well when applied to new data.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Choosing the _Optimal_ Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "All the previous algorithms are good in selecting a model that best fit the training data set (because they all minimize the $RSS$ which is equivalent to maximize the $R^2$  for these observations) and this does not necessarily mean they minimize the $RSS$ in any validation data set. Therefore 'training set' $RSS$ and 'training set' $R^2$  cannot be used to select from among a set of models with different numbers of variables if we are meant to use this model for _prediction_. Therefore we can replace step 3 in these algorithms with a step where the __cross-validated prediction error__  is calculated using observations in the validation data set!\n",
    "\n",
    "💡 Of course you can use other _model selection criteria_ like $\\bar{R}^2$, $C_p$ (AIC), BIC, etc., _but_ if we care about the prediction power of our model, the __cross-validated prediction error__ is a more natural choice.\n",
    "\n",
    "💡 Remember that the $k$-fold Cross-validation consists of first dividing the data into $k$ subsets, also known as $k$-fold, where $k$ is generally set to 5 or 10. Each subset (10%) serves successively as test data set and the remaining subset (90%) as training data. The average cross-validation error is computed as the model prediction error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "💻 The following user-written ```get_model_formula()``` function allows to access easily the formula of the models returned by the function ```regsubsets()``` above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "## id: model id\n",
    "## object: regsubsets object\n",
    "## data: data used to fit regsubsets\n",
    "## outcome: outcome variable\n",
    "get_model_formula <- function(id, object, outcome){\n",
    "  models <- summary(object)$which[id,-1] # get models data\n",
    "  # Get model predictors\n",
    "  predictors <- names(which(models == TRUE))\n",
    "  predictors <- paste(predictors, collapse = \"+\")\n",
    "  # Build model formula\n",
    "  as.formula(paste0(outcome, \"~\", predictors))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "💻 For example the following code returns the best 6-variable model formula from the previous _best_ subset selection, i.e., $\\mathcal{M}_{6}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "get_model_formula(6, models.best, \"datos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "💻 The following user-written ```get_cv_error()``` calculates the cross-validation (CV) ```method=\"cv\"``` error for a given model ```model.formula```, using data set ```data``` and 5-folds ```number=5```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "get_cv_error <- function(model.formula, data){\n",
    "      set.seed(42)\n",
    "      train.control <- trainControl(method = \"cv\", number = 5)\n",
    "      cv <- train(model.formula, data = data, method = \"lm\",\n",
    "                  trControl = train.control)\n",
    "      cv$results$RMSE\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "💻 Finally, use the above defined user-written functions to compute the prediction error of the different best models returned by the ```regsubsets(...,method=\"exhaustive\")``` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "## compute cross-validation error\n",
    "model.ids <- 1:8\n",
    "cv.errors <-  map(model.ids, get_model_formula, models.best, \"lprice\") %>%\n",
    "  map(get_cv_error, data = datos) %>%\n",
    "  unlist()\n",
    "cv.errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "## select the model that minimize the CV error\n",
    "which.min(cv.errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "💻 It can be seen that the model with 8 variables is the best model. It has the lower prediction error. The regression coefficients of this model can be extracted as follow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "coef(models.best, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Obviously, you can find the best model using the _forward_ and _backward_ stepwise selection algorithms following the steps above by replacing the user-given names ```model.best``` to either ```model.forward``` or ```model.backwards```, but there is a quicker cleaner way using the ```library(caret)``` package:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "## set seed for reproducibility\n",
    "set.seed(42)\n",
    "## set up repeated 10-fold cross-validation\n",
    "train.control <- trainControl(method = \"cv\", number = 10)\n",
    "# Train the model\n",
    "step.model <- train(lprice ~., data = datos,\n",
    "                    method = \"leapBackward\", \n",
    "                    tuneGrid = data.frame(nvmax = 1:8),\n",
    "                    trControl = train.control\n",
    "                    )\n",
    "step.model$results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "💻 If we are interested in using a _forward_ stepwise selection algorithm we simply need to change the method in the code snippet above to ```method = \"leapForward\"```."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "💻 The following set of commands identify the _best_ model in terms of the calculated $RMSE$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "## retrieving the best model in terms of 10-fold CV RMSE\n",
    "step.model$bestTune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "## looking at the specification of the best model\n",
    "summary(step.model$finalModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "## looking at the estimated coefficient of the best model\n",
    "coef(step.model$finalModel, 8)"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.0.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
