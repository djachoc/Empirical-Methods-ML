{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subset Selection\n",
    "\n",
    "This approach involves identifying a subset of the $p$ features (predictors) out of the $k$ that we believe to be related to the response. We then fit a model using OLS on the reduced set of variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import patsy\n",
    "import itertools\n",
    "import time\n",
    "import statsmodels.api as sm\n",
    "\n",
    "hprice2 = pd.read_stata('http://fmwww.bc.edu/ec-p/data/wooldridge/hprice2.dta')\n",
    "f = 'lprice ~ lnox + lproptax + crime + rooms + dist + radial + stratio + lowstat'\n",
    "y1, X1 = patsy.dmatrices(f, data=hprice2, return_type='dataframe')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In what follows, we will pre-processed the data by demeaning both the outcome and features so all models can be fitted without an intercept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y1.sub(y1.mean())\n",
    "X = X1.sub(X1.mean()).drop('Intercept',axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Code) Source:\n",
    "1. [http://www.science.smith.edu/~jcrouser/SDS293/labs/lab8-py.html](http://www.science.smith.edu/~jcrouser/SDS293/labs/lab8-py.html)\n",
    "2. [http://www.science.smith.edu/~jcrouser/SDS293/labs/lab9-py.html](http://www.science.smith.edu/~jcrouser/SDS293/labs/lab9-py.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _Best_ Subset Selection\n",
    "\n",
    "To perform best subset selection, we fit a separate OLS regression best subset for each possible combination of the $k$ predictors. That is, we fit _all_ $k$ models selection that contain exactly one predictor, all $\\left(\\begin{array}{l}{k} \\\\ {2}\\end{array}\\right)=k(k-1) / 2$ models that contain exactly two predictors, and so forth. We then look at all of the resulting models, with the goal of identifying the one that is best."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "**Algorithm**: <ins>_Best_ Subset Selection</ins>\n",
    "\n",
    "1. Let $\\mathcal{M}_{0}$ denote the null model, which contains no predictors. This model simply predicts the sample mean for each observation.\n",
    "2. For $p=1,2, \\ldots k$:\n",
    " 1. Fit all $\\left(\\begin{array}{l}{k} \\\\ {p}\\end{array}\\right)$ models that contain exactly $p$ predictors.\n",
    " 2. Pick the best among these $\\left(\\begin{array}{l}{k} \\\\ {p}\\end{array}\\right)$ models, and call it $\\mathcal{M}_{p}$. Here best is defined as having the smallest $RSS$, or equivalently largest $R^{2}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processSubset(feature_set):\n",
    "    # Fit model on feature_set and calculate RSS\n",
    "    model = sm.OLS(y,X[list(feature_set)])\n",
    "    regr = model.fit()\n",
    "    RSS = regr.ssr\n",
    "    return {\"model\":regr, \"RSS\":RSS}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getBest(p):\n",
    "    \n",
    "    tic = time.time()\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for combo in itertools.combinations(X.columns, p):\n",
    "        results.append(processSubset(combo))\n",
    "    \n",
    "    # Wrap everything up in a nice dataframe\n",
    "    models = pd.DataFrame(results)\n",
    "    \n",
    "    # Choose the model with the lowest RSS\n",
    "    best_model = models.loc[models['RSS'].argmin()]\n",
    "    \n",
    "    toc = time.time()\n",
    "    print(\"Processed\", models.shape[0], \"models on\", p, \"predictors in\", (toc-tic), \"seconds.\")\n",
    "    \n",
    "    # Return the best model, along with some other useful information about the model\n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This returns a ```DataFrame``` containing the best model that we generated, along with some extra information about the model. Now we want to call that function for each number of predictors  $p$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Could take quite awhile to complete...\n",
    "\n",
    "models_best = pd.DataFrame(columns=[\"RSS\", \"model\"])\n",
    "\n",
    "tic = time.time()\n",
    "\n",
    "for i in range(1,9):\n",
    "    models_best.loc[i] = getBest(i)\n",
    "\n",
    "toc = time.time()\n",
    "print(\"Total elapsed time:\", (toc-tic), \"seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.rcParams.update({'font.size': 18, 'lines.markersize': 10})\n",
    "\n",
    "# Set up a 2x2 grid so we can look at 4 plots at once\n",
    "plt.subplot(2, 2, 1)\n",
    "\n",
    "# We will now plot a red dot to indicate the model with the largest adjusted R^2 statistic.\n",
    "# The argmax() function can be used to identify the location of the maximum point of a vector\n",
    "plt.plot(models_best[\"RSS\"])\n",
    "plt.xlabel('# Predictors')\n",
    "plt.ylabel('RSS')\n",
    "\n",
    "# We will now plot a red dot to indicate the model with the largest adjusted R^2 statistic.\n",
    "# The argmax() function can be used to identify the location of the maximum point of a vector\n",
    "\n",
    "rsquared_adj = models_best.apply(lambda row: row[1].rsquared_adj, axis=1)\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(rsquared_adj)\n",
    "plt.plot(rsquared_adj.argmax()+1, rsquared_adj.max(), \"or\")\n",
    "plt.xlabel('# Predictors')\n",
    "plt.ylabel('Adjusted rsquared')\n",
    "\n",
    "# We'll do the same for AIC and BIC, this time looking for the models with the SMALLEST statistic\n",
    "aic = models_best.apply(lambda row: row[1].aic, axis=1)\n",
    "\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.plot(aic)\n",
    "plt.plot(aic.argmin()+1, aic.min(), \"or\")\n",
    "plt.xlabel('# Predictors')\n",
    "plt.ylabel('AIC')\n",
    "\n",
    "bic = models_best.apply(lambda row: row[1].bic, axis=1)\n",
    "\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.plot(bic)\n",
    "plt.plot(bic.argmin()+1, bic.min(), \"or\")\n",
    "plt.xlabel('# Predictors')\n",
    "plt.ylabel('BIC')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "‚ö†Ô∏è The number of possible models that must be considered grows rapidly as $k$ increases, e.g., if $k$=10, then there are approximately 1,000 possible models to be considered, and if $k=20$, there are over one million possibilities (1,048,576)!\n",
    "\n",
    "## Stepwise Selection: _Forward_ Stepwise Selection\n",
    "\n",
    "For computational reason, the previous '_best_ subset selection' cannot be applied with very large $k$ (remember it consinders all $2^k$  possible models). The algorithm begins with a model containing no predictors, and then adds predictors to the model, one-at-a-time, untill all the predictors are in the model.\n",
    "\n",
    "***\n",
    "**Algorithm**: <ins>_Forward_ Stepwise Selection</ins>\n",
    "\n",
    "1. Let $\\mathcal{M}_{0}$ denote the null model, which contains _no_ predictors.\n",
    "2. For $p=0,1,2,\\ldots, k-1$:\n",
    " 1. Consider all $k-p$ models that augment the predictors in $\\mathcal{M}_{p}$ with one additional predictor.\n",
    " 2. Choose the _best_ among these $k-p$ models, and call it $\\mathcal{M}_{p+1}$. Here best is defined as having smallest $RSS$ or highest $R^{2}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(predictors):\n",
    "\n",
    "    # Pull out predictors we still need to process\n",
    "    remaining_predictors = [p for p in X.columns if p not in predictors]\n",
    "    \n",
    "    tic = time.time()\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for p in remaining_predictors:\n",
    "        results.append(processSubset(predictors+[p]))\n",
    "    \n",
    "    # Wrap everything up in a nice dataframe\n",
    "    models = pd.DataFrame(results)\n",
    "    \n",
    "    # Choose the model with the lowest RSS\n",
    "    best_model = models.loc[models['RSS'].argmin()]\n",
    "    \n",
    "    toc = time.time()\n",
    "    print(\"Processed \", models.shape[0], \"models on\", len(predictors)+1, \"predictors in\", (toc-tic), \"seconds.\")\n",
    "    \n",
    "    # Return the best model, along with some other useful information about the model\n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Select a single best model from among $\\mathcal{M}_{0}, \\ldots, \\mathcal{M}_{k}$ using a *model selection criteria* previously discussed such as cross-validated prediction error, $C_{p}(\\mathrm{AIC}), \\mathrm{BIC}$, adjusted $R^{2}$, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_fwd = pd.DataFrame(columns=[\"RSS\", \"model\"])\n",
    "\n",
    "tic = time.time()\n",
    "predictors = []\n",
    "\n",
    "for i in range(1,len(X.columns)+1):    \n",
    "    models_fwd.loc[i] = forward(predictors)\n",
    "    predictors = models_fwd.loc[i][\"model\"].model.exog_names\n",
    "\n",
    "toc = time.time()\n",
    "print(\"Total elapsed time:\", (toc-tic), \"seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "plt.rcParams.update({'font.size': 18, 'lines.markersize': 10})\n",
    "\n",
    "# Set up a 2x2 grid so we can look at 4 plots at once\n",
    "plt.subplot(2, 2, 1)\n",
    "\n",
    "# We will now plot a red dot to indicate the model with the largest adjusted R^2 statistic.\n",
    "# The argmax() function can be used to identify the location of the maximum point of a vector\n",
    "plt.plot(models_fwd[\"RSS\"])\n",
    "plt.xlabel('# Predictors')\n",
    "plt.ylabel('RSS')\n",
    "\n",
    "# We will now plot a red dot to indicate the model with the largest adjusted R^2 statistic.\n",
    "# The argmax() function can be used to identify the location of the maximum point of a vector\n",
    "\n",
    "rsquared_adj = models_fwd.apply(lambda row: row[1].rsquared_adj, axis=1)\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(rsquared_adj)\n",
    "plt.plot(rsquared_adj.argmax()+1, rsquared_adj.max(), \"or\")\n",
    "plt.xlabel('# Predictors')\n",
    "plt.ylabel('Adjusted rsquared')\n",
    "\n",
    "# We'll do the same for AIC and BIC, this time looking for the models with the SMALLEST statistic\n",
    "aic = models_fwd.apply(lambda row: row[1].aic, axis=1)\n",
    "\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.plot(aic)\n",
    "plt.plot(aic.argmin()+1, aic.min(), \"or\")\n",
    "plt.xlabel('# Predictors')\n",
    "plt.ylabel('AIC')\n",
    "\n",
    "bic = models_fwd.apply(lambda row: row[1].bic, axis=1)\n",
    "\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.plot(bic)\n",
    "plt.plot(bic.argmin()+1, bic.min(), \"or\")\n",
    "plt.xlabel('# Predictors')\n",
    "plt.ylabel('BIC')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "‚ö†Ô∏è There are a total of $1+\\sum_{p=0}^{k-1}(k-p)=1+\\frac{k(k+1)}{2}$ models to be estimated instead of $2^{k}$ as before,\n",
    "e.g. if $k=20,$ there are 211 models to be estimated! Interestingly it will work even when <font color=blue>$k>n$</font>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stepwise Selection: _Backward_ Stepwise Selection\n",
    "\n",
    "Unlike forward stepwise selection, it begins with the full least squares model containing all $k$ regressors, and then iteractively the least useful regressors one-at-a-time until there are no predictors in the model, i.e., the null model.\n",
    "\n",
    "***\n",
    "**Algorithm**: <ins>_Backward_ Stepwise Selection</ins>\n",
    "\n",
    "1. Let $\\mathcal{M}_{k}$ denote the full model, which contains _all_ $k$ predictors.\n",
    "2. For $p=k,k-1,\\ldots, 1$:\n",
    " 1. Consider all $p$ models that contain all but one of the predictors in $\\mathcal{M}_{p}$ for a total of $p-1$ predictors.\n",
    " 2. Choose the best among these $p$ models, and call it $\\mathcal{M}_{p-1}$. Here best is defined as having smallest RSS or highest $R^{2}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward(predictors):\n",
    "    \n",
    "    tic = time.time()\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for combo in itertools.combinations(predictors, len(predictors)-1):\n",
    "        results.append(processSubset(combo))\n",
    "    \n",
    "    # Wrap everything up in a nice dataframe\n",
    "    models = pd.DataFrame(results)\n",
    "    \n",
    "    # Choose the model with the lowest RSS\n",
    "    best_model = models.loc[models['RSS'].argmin()]\n",
    "    \n",
    "    toc = time.time()\n",
    "    print(\"Processed \", models.shape[0], \"models on\", len(predictors)-1, \"predictors in\", (toc-tic), \"seconds.\")\n",
    "    \n",
    "    # Return the best model, along with some other useful information about the model\n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_bwd = pd.DataFrame(columns=[\"RSS\", \"model\"], index = range(1,len(X.columns)))\n",
    "\n",
    "tic = time.time()\n",
    "predictors = X.columns\n",
    "\n",
    "while(len(predictors) > 1):  \n",
    "    models_bwd.loc[len(predictors)-1] = backward(predictors)\n",
    "    predictors = models_bwd.loc[len(predictors)-1][\"model\"].model.exog_names\n",
    "\n",
    "toc = time.time()\n",
    "print(\"Total elapsed time:\", (toc-tic), \"seconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Select a single best model from among $\\mathcal{M}_{0}, \\ldots, \\mathcal{M}_{k}$ using a *model selection criteria* previously discussed such as cross-validated prediction error, $C_{p}(\\mathrm{AIC}), \\mathrm{BIC}$, adjusted $R^{2}$, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "plt.rcParams.update({'font.size': 18, 'lines.markersize': 10})\n",
    "\n",
    "# Set up a 2x2 grid so we can look at 4 plots at once\n",
    "plt.subplot(2, 2, 1)\n",
    "\n",
    "# We will now plot a red dot to indicate the model with the largest adjusted R^2 statistic.\n",
    "# The argmax() function can be used to identify the location of the maximum point of a vector\n",
    "plt.plot(models_bwd[\"RSS\"])\n",
    "plt.xlabel('# Predictors')\n",
    "plt.ylabel('RSS')\n",
    "\n",
    "# We will now plot a red dot to indicate the model with the largest adjusted R^2 statistic.\n",
    "# The argmax() function can be used to identify the location of the maximum point of a vector\n",
    "\n",
    "rsquared_adj = models_bwd.apply(lambda row: row[1].rsquared_adj, axis=1)\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(rsquared_adj)\n",
    "plt.plot(rsquared_adj.argmax()+1, rsquared_adj.max(), \"or\")\n",
    "plt.xlabel('# Predictors')\n",
    "plt.ylabel('Adjusted rsquared')\n",
    "\n",
    "# We'll do the same for AIC and BIC, this time looking for the models with the SMALLEST statistic\n",
    "aic = models_bwd.apply(lambda row: row[1].aic, axis=1)\n",
    "\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.plot(aic)\n",
    "plt.plot(aic.argmin()+1, aic.min(), \"or\")\n",
    "plt.xlabel('# Predictors')\n",
    "plt.ylabel('AIC')\n",
    "\n",
    "bic = models_bwd.apply(lambda row: row[1].bic, axis=1)\n",
    "\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.plot(bic)\n",
    "plt.plot(bic.argmin()+1, bic.min(), \"or\")\n",
    "plt.xlabel('# Predictors')\n",
    "plt.ylabel('BIC')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "***\n",
    "‚ö†Ô∏è Unlike the _forward_ stepwise selection algorithm, this will _only_ work when <font color=blue>$n>k$</font>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<font color=red><ins>Note</ins> Notice that the $\\bar{R}^2$, BIC and $C_p$ are calculated on the training data that have been used to fit the model. This means that, the model selection, using these metrics, is possibly subject to overfitting and may not perform as well when applied to new data.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing the _Optimal_ Model\n",
    "\n",
    "All the previous algorithms are good in selecting a model that best fit the training data set (because they all minimize the $RSS$ which is equivalent to maximize the $R^2$  for these observations) and this does not necessarily means they minimize the $RSS$ in any validation data set. Therefore 'training set' $RSS$ and 'training set' $R^2$  cannot be used to select from among a set of models with different numbers of variables if we are meant to use this model for _prediction_. Therefore we can replace step 3 in these algorithms with a step where the __cross-validated prediction error__  is calculated using observations in the validation data set!\n",
    "\n",
    "üí° Of course you can use other _model selection criteria_ like $\\bar{R}^2$, $C_p$ (AIC), BIC, etc., _but_ if we care about the prediction power of our model, the __cross-validated prediction error__ is a more natural choice.\n",
    "\n",
    "üí° Remember that the $k$-fold Cross-validation consists of first dividing the data into $k$ subsets, also known as $k$-fold, where $k$ is generally set to 5 or 10. Each subset (10%) serves successively as test data set and the remaining subset (90%) as training data. The average cross-validation error is computed as the model prediction error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processSubset(feature_set, X_train, y_train, X_test, y_test):\n",
    "    # Fit model on feature_set and calculate RSS\n",
    "    model = sm.OLS(y_train,X_train[list(feature_set)])\n",
    "    regr = model.fit()\n",
    "    RSS = ((regr.predict(X_test[list(feature_set)]) - y_test.iloc[:,0])**2).sum()\n",
    "    return {\"model\":regr, \"RSS\":RSS}\n",
    "\n",
    "def forward(predictors, X_train, y_train, X_test, y_test):\n",
    "    results = []\n",
    "\n",
    "    # Pull out predictors we still need to process\n",
    "    remaining_predictors = [p for p in X_train.columns if p not in predictors]\n",
    "    \n",
    "    for p in remaining_predictors:\n",
    "        results.append(processSubset(predictors+[p], X_train, y_train, X_test, y_test))\n",
    "    \n",
    "    # Wrap everything up in a nice dataframe\n",
    "    models = pd.DataFrame(results)\n",
    "    \n",
    "    # Choose the model with the lowest RSS\n",
    "    best_model = models.loc[models['RSS'].argmin()]\n",
    "        \n",
    "    # Return the best model, along with some other useful information about the model\n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 10        # number of folds\n",
    "np.random.seed(seed=1)\n",
    "folds = np.random.choice(k, size = len(y), replace = True)\n",
    "\n",
    "# Create a DataFrame to store the results of our upcoming calculations\n",
    "cv_errors = pd.DataFrame(columns=range(1,k+1), index=range(1,9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_cv = pd.DataFrame(columns=[\"RSS\", \"model\"])\n",
    "\n",
    "# Outer loop iterates over all folds\n",
    "for j in range(1,k+1):\n",
    "\n",
    "    # Reset predictors\n",
    "    predictors = []\n",
    "    \n",
    "    # Inner loop iterates over each size i\n",
    "    for i in range(1,len(X.columns)+1):    \n",
    "    \n",
    "        # The perform forward selection on the full dataset minus the jth fold, test on jth fold\n",
    "        models_cv.loc[i] = forward(predictors,  X[folds != (j-1)], y[folds != (j-1)], X[folds == (j-1)], y[folds == (j-1)])\n",
    "        \n",
    "        \n",
    "        # Save the cross-validated error for this fold\n",
    "        cv_errors[j][i] = models_cv.loc[i][\"RSS\"]\n",
    "\n",
    "        # Extract the predictors\n",
    "        predictors = models_cv.loc[i][\"model\"].model.exog_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_mean = cv_errors.apply(np.mean, axis=1)\n",
    "\n",
    "plt.plot(cv_mean)\n",
    "plt.xlabel('# Predictors')\n",
    "plt.ylabel('CV Error')\n",
    "plt.plot(cv_mean.argmin()+1, cv_mean.min(), \"or\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now extract the best model and estimate its parameters using the entire data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sm.OLS(y1,sm.add_constant(X1[models_cv.loc[7, \"model\"].model.exog_names])).fit().summary())\n",
    "print(sm.OLS(y,X[models_cv.loc[7, \"model\"].model.exog_names]).fit().summary())"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
